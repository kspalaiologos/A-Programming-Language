
\chapter{The Language}

\section{Introduction}

\par Applied mathematics is concerned with the design and analysis of algorithms or \textit{programs}. The systematic treatment of complex algorithms requires a suitable \textit{programming language} for their description, and such a programming language should be concise, precise, consistent over a wide area of application, mnemonic, and economical of symbols; it should exhibit clearly the constraints on the sequence in which operations are performed; and it should permit the description of a process to be independent of the particular representation chosen for the data.

\par Existing languages prove unsuitable for a variety of reasons. Computer coding specifies sequence constraints adequately and is also comprehensive, since the logical functions provided by the branch instructions can, in principle, be employed to synthesize any finite algorithm. However, the set of basic operations provided is not, in general, directly suited to the execution of commonly needed processes, and the numeric symbols used for variables have little mnemonic value. Moreover, the description provided by computer coding depends directly on the particular representation chosen for the data, and it therefore cannot serve as a description of the algorithm per se.

\par Ordinary English lacks both precision and conciseness. The widely used Goldstine-von Neumann (1947) flowcharting provides the conciseness necessary to an over-all view of the processes, only at the cost of suppressing essential detail. The so-called pseudo-English used as a basis for certain automatic programming systems suffers from the same defect. Moreover, the potential mnemonic advantage in substituting familiar English words and phrases for less familiar but more compact mathematical symbols fails to materialize because of the obvious but unwonted precision required in their use.

\par Most of the concepts and operations needed in a programming language have already been defined and developed in one or another branch of mathematics. Therefore, much use can and will be made of existing notations. However, since most notations are specialized to a narrow field of discourse, a consistent unification must be provided. For example, separate and conflicting notations have been developed for the treatment of sets, logical variables, vectors, matrices, and trees, all of which may, in the broad universe of discourse of data processing, occur in a single algorithm.

\section{Programs}

\par A \textit{program statement} is the specification of some quantity or quantities in terms of some finite operation upon specified operands. Specification is symbolized by an arrow directed toward the specified quantity. thus ``$y$ is specified by $\sin x$'' is a statement denoted by

$$
      y\ ←\ \sin \ x.
$$

\par A set of statements together with a specified order of execution constitutes a \textit{program}. The program is \textit{finite} if the number of executions is finite. The \textit{results} of the program are some subset of the quantities specified by the program. The \textit{sequence} or order of execution will be defined by the order of listing and otherwise by arrows connecting any statement to its successor. A cyclic sequence of statements is called a \textit{loop}.

\par (TODO: FIGURES 1.1 AND 1.2)

\par Thus Program 1.1 is a program of two statements defining the result $v$ as the (approximate) area of a circle of radius $x$, whereas Program 1.2 is an infinite program in which the quantity $z$ is specified as $(2y)^n$ on the $n$th execution of the two statement loop. Statements will be numbered on the left for reference.

\par A number of similar programs may be subsumed under a single more general program as follows. At certain \textit{branch points} in the program a finite number of alternative statements are specified as possible successors. One of these successors is chosen according to criteria determined in the statement or statements preceding the branch point. These criteria are usually stated as a \textit{comparison} or test of a specified relation between a specified pair of quantities. A branch is denoted by a set of arrows leading to each of the alternative successors, with each arrow labeled by the comparison condition under which the corresponding successor is chosen. The quantities compared are separated by a colon in the statement at the branch point, and a labeled branch is followed if and only if the relation indicated by the label holds when substituted for the colon. The conditions on the branches of a properly defined program must be disjoint and exhaustive.

\par Program 1.3 illustrates the use of a branch point. Statement \verb|α|$5$ is a comparison which determines the branch to statements \verb|β|$1$, \verb|δ|$1$, or \verb|γ|$1$, according as $z > n$, $z = n$, or $z < n$. The program represents a crude by effective process for determining $x = n^\frac{2}{3}$ for any positive cube n.

\par (TODO: FIGURE 1.3)

\par Program 1.4 shows the preceding program reorganized into a compact linear array and introduces two further conventions on the labeling of branch points. The listed successor of a branch statement is selected if none of the labeled conditions is met. Thus statement 6 follows statement 5 if neither of the arrows (to exit or to statement 8) are followed, i.e. if $z < n$. Moreover, any unlabeled arrow is always followed; e.g., statement 7 is invariably followed by statement 3, never by statement 8.

\par A program begins at a point indicated by an \textit{entry arrow} (step 1) and ends at a point indicated by an \textit{exit arrow} (step 5). There are two useful consequences of confining a program to the form of a linear array: the statements may be referred to by a unique serial index (statement number), and unnecessarily complex organization of the program manifests itself in crossing branch lines. The importance of the latter characteristic in developing clear and comprehensible programs is not sufficiently appreciated.

\par (TODO: FIGURES 1.4 AND 1.5)

\par A process which is repeated a number of times is said to be \textit{iterated}, and a process (such as in Program 1.4) which includes one or more iterated subprocesses is said to be \textit{iterative}. Program 1.5 shows an iterative process for the matrix multiplication

$$
      C\ ←\ AB
$$

\noindent defined in the usual way as

\begin{equation*}
  \begin{split}
    C^i_j = \sum^{ν(\mat{A})}_{k=1} A^k_i \times B^k_j
  \end{split}
\quad\quad
  \begin{split}
    i &= 1,2, ..., μ(\mat{A}),\\
    j &= 1,2, ..., ν(\mat{B}),
  \end{split}
\end{equation*}

\noindent where the dimensions of an $m \times n$ matrix $\mat{X}$ (of $m$ rows and $n$ columns) is denoted by $μ(\mat{X}) \times ν(\mat{X})$.

\par \textbf{Program 1.5}. Steps 1-3 initialize the indices, and the loop 5-7 continues to add successive products to the partial sum until $k$ reaches zero. When this occurs, the process continues through step 8 to decrement $j$ and to repeat the entire summation for the new value of $j$, providing that it is not zero. If $j$ is zero, the branch to step 10 decrements $i$ and the entire process over $j$ and $k$ is repeated from $j = ν(\mat{B})$, providing that $i$ is not zero. If $i$ is zero, the process is complete, as indicated by the exit arrow.

\par In all examples used in this chapter, emphasis will be placed on clarity of description of the process, and considerations of efficient execution by a computer or class of computers will be subordinated. These considerations can often be introduced later by relatively routine modifications of the program. For example, since the execution of a computer operation involving an indexed variable is normally more costly than the corresponding operation upon a nonindexed variable, the substitution of a variable $s$ for the variable $\mat{C}^i_j$ specified by statement 5 of Program 1.5 would accelerate the execution of the loop. The variable $s$ would be initialized to zero before each entry to the loop and would be used to specify $\mat{C}^i_j$ at each termination.

\par The practice of first setting an index to its maximum value and then decrementing it (e.g., the index $k$ in Program 1.5) permits the termination comparison to be made with zero. Since zero often occurs in comparisons, it is convenient to omit it. Thus, if a variable stands alone at a branch point, comparison with zero is implied. Moreover, since a comparison on an index frequently occurs immediately after it is modified, a branch at the point of modification will denote branching upon comparison of the indicated index with zero, the comparison occurring \textit{after} modification. Designing programs to execute decisions immediately after modification of the controlling variable results in efficient execution as well as notational elegance, since the variable must be present in a central register for both operations.

\par Since the sequence of execution of statements is indicated by connecting arrows as well as by the order of listing, the latter can be chosen arbitrarily. This is illustrated by the functionally identical Programs 1.3 and 1.4. Certain principles of ordering may yield advantages such as clarity or simplicity of the pattern of connections. Even though the advantages of a particular organizing principle are not particularly marked, the uniformity resulting from its consistent application will itself be a boon. The scheme here adopted is called the \textit{method of leading decisions}: the decision on each parameter is placed as early in the program as practicable, normally just before the operations indexed by the parameter. This arrangement groups at the head of each iterative segment the initialization, modification, and the termination test of the controlling parameter. Moreover, it tends to avoid program flaws occasioned by unusual values of the argument.

\par (TODO: FIGURE 1.6)

\par For example, Program 1.6 (which is a reorganization of Program 1.5) behaves properly for matrices of dimension zero, whereas Program 1.5 treats every matrix as if it were of dimension one or greater.

\par Although the labeled arrow representation of program branches provides a complete and graphic description, it is deficient in the following respects: (1) a routine translation to another language (such as computer code) would require the tracing of arrows, and (2) it does not permit programmed modification of the branches.

\par The following alternative form of a branch statement will therefore be used as well:

$$
x\ :\ y,\ \vect{r}\ →\ \vect{s}.
$$

\par This denotes a branch to statement number $\vect{s}_i$ of the program if the relation $x\vect{r}_i y$ holds. The parameters $\vect{r}$ and $\vect{s}$ may themselves be defined and redefined in other parts of the program. The \textit{null element} $∘$ will be used to denote the relation which complements the remaining relations $\vect{r}_i$; in particular, $(∘)\ →\ (s)$, or simply $→\ s$, will denote an unconditional branch to statement $s$. Program 1.7 shows the use of these conventions in a reformulation of Program 1.6. More generally, two or more otherwise independent programs may interact through a statement in one program specifying a branch in a second. The statement number occurring in the branch must then be augmented by the name of the program in which the branch is effected. Thus the statement $(∘)\ →$ Program 2.24 executed in Program 1 causes a branch to step 24 to occur in Program 2.

\par (TODO: FIGURE 1.7)

\par One statement in a program can be modified by another statement which changes certain of its parameters, usually indices. More general changes in statements can be effected by considering the program itself as a vector $\vect{p}$ whose components are the individual, serially numbered statements. All the operations to be defined on general vectors can then be applied to the statements themselves. For example, the $j$th statement can be respecified by the $i$th through the occurrence of the statement $\vect{p}_j\ ←\ \vect{p}_i$.

\par The interchange of two quantities $y$ and $x$ (that is, $x$ specifies $y$ and the \textit{original} value of $y$ specifies $x$) will be denoted by the statement $y\ \leftrightarrow\ x$.

\section{Structure of the language}

\subsection*{Conventions}

\par The Summary of Notation at the end of the book summarizes the notation developed in this chapter. Although intended primarily for reference, it supplements the text in several ways. It frequently provides a more concise alternative definition of an operation discussed in the text, and it also contains important but easily grasped extensions not treated explicitly in the text. By grouping the operations into related classes it displays their family relationships.

\par A concise programming language must incorporate families of operations whose members are related in a systematic manner. Each family will be denoted by a specific operation symbol, and the particular member of the family will be designated by an associated \textit{controlling parameter} (scalar, vector, matrix, or tree) which immediately precedes the main operation symbol. The operand is placed immediately after the main operation symbol. For example, the operation $k\ ↑\ \vect{x}$ (left rotation of $\vect{x}$ by $k$ places) may be viewed as the $k$th member of the set of rotation operators denoted by the symbol $↑$.

\par Operations involving a single operand and no controlling parameter (such as $\lfloor x \rfloor$, or $\lceil x \rceil$) will be denoted by a pair of operation symbols which enclose the operand. Operations involving two operands and a controlling parameter (such as the mask operation $/\vect{a},\ \vect{u},\ \vect{b}/$) will be denoted by a pair of operation symbols enclosing the entire set of variables, and the controlling parameter will appear between the two operands. In these cases the operation symbols themselves serve as grouping symbols.

\par In interpreting a compound operation such as $k ↑ (j ↓ \vect{x})$ it is important to recognize that the operation symbol and its associated controlling parameter together represent an indivisible operation and must not be separated. It would, for example, be incorrect to assume that $j ↑ (k ↓ \vect{x})$ were equivalent to $k ↑ (j ↓ \vect{x})$, although it can be shown that the complete operations $j ↓$ and $k ↑$ do commute, that is $k ↑ (j ↓ \vect{x}) = j ↓ (k ↑ \vect{x})$.

\par The need for parentheses will be reduced by assuming that compound statements are, except for intervening parentheses, executed from right to left. Thus $k ↑ j ↓ \vect{x}$ is equivalent to $k ↑ (j ↓ \vect{x})$, not to $(k ↑ j) ↓ \vect{x}$.

\par Structured operands such as vectors and matrices, together with a systematic component-by-component generalization of elementary operations, provide an important subordination of detail in the description of algorithms. The use of structured operands will be facilitated by \textit{selection operations} for extracting a specified portion of an operand, \textit{reduction operations} for extending an operation (such as logical or arithmetic multiplication) over all components, and \textit{permutation operations} for reordering components. Operations defined on vectors are extended to matrices: the extended operation is called a \textit{row} operation if the underlying vector operation is applied to each row of the matrix and a \textit{column} operation if it is applied to each column. A column operation is denoted by doubling the symbol employed for the corresponding row (and vector) operation.

\par A distinct typeface will be used for each class of operand as detailed in Table 1.8. Special quantities (such as the prefix vectors $\mathbf{α}^i$ defined in Sec. 1.7) will be denoted by Greek letters in the appropriate typeface. For mnemonic reasons, an operation closely related to such a special quantity will be denoted by the same Greek letter. For example, $α/\vect{u}$ denotes the maximum prefix (Sec. 1.10) of the logical vector $\vect{u}$. Where a Greek letter is indistinguishable from a Roman, sanserif characters will be used, e.g. $\mathsf{\mat{E}}$ and $\mathsf{\mat{I}}$ for the capitals of epsilon and iota.

\par (TODO: TABLE 1.8)

\par \textbf{Table 1.8} Typographic conventions for classes of operands

\subsection*{Literals and variables}

\par The power of any mathematical notation rests largely on the use of symbols to represent general quantities which, in given instances, are further specified by other quantities. Thus Program 1.4 represents a general process which determines $x = n^\frac{2}{3}$ for any suitable value of $n$. In a specific case, say $n = 27$, the quantity $x$ is specified as the number 9.

\par Each operand occurring in a meaningful process must be specified ultimately in terms of commonly accepted concepts. The symbols representing such accepted concepts will be called \textit{literals}. Examples of literals are the integers, the characters of the various alphabets, punctuation marks, and miscellaneous symbols such as \$ and \%. The literals occurring in Program 1.4 are 0, 1, 2.

\par It is important to distinguish clearly between general symbols and literals. In ordinary algebra this presents little difficulty, since the only literals occurring are the integers and the decimal point, and each general symbol employed includes an alphabetic character. In describing more general processes, however, alphabetic literals (such as proper names) also appear. Moreover, in a computer program, numeric symbols (register addresses) are used to represent the variables.

\par In general, then, alphabetic literals, alphabetic variables, numeric literals, and numeric variables may all appear in a complex process and must be clearly differentiated. The symbols used for literals will be Roman letters (enclosed in quotes when appearing in text) and standard numerals. The symbols used for variables will be italic letters, italic numerals, and boldface letters as detailed in Table 1.8. Miscellaneous signs and symbols when used as literals will be enclosed in quotes in both programs and text.

\par It is sometimes desirable (e.g., for mnemonic reasons) to denote a variable by a string of alphabetic or other symbols rather than by a single symbol. The monolithic interpretation of such a string will be indicated by the \textit{tie} used in musical notation, thus:
(TODO ties) $inv$, $\mathbf{inv}$, $\mathbf{INV}$ may denote the variable ``inventory'', a vector of inventory values, and a matrix of inventory values, respectively.

\par In the set of alphabetic characters, the \textit{space} plays a special role. For other sets a similar role is usually played by some one element, and this element is given the special name of \textit{null element}. In the set of numeric digits, the \textit{zero} plays a dual role as both null element and numeric quantity. The null element will be denoted by the degree symbol $∘$.

\par In any determinate process, each operand must be specified ultimately in terms of literals. In Program 1.4, for example, the quantity $k$ is specified in terms of known arithmetic operations (multiplication and division) involving the literals 1 and 2. The quantity $n$, on the other hand, is not determined within the process and must presumably be specified within some larger process which includes Program 1.4. Such a quantity is called an \textit{argument} of the process.

\subsection*{Domain and range}

\par The class of arguments and the class of results of a given operator are called its \textit{domain} and \textit{range}, respectively. Thus the domain and range of the magnitude operation ($|x|$) are the real numbers and the nonnegative real numbers, respectively.

\par A variable is classified according to the range of values it may assume: it is \textit{logical}, \textit{integral}, or \textit{numerical}, according as the range is the set of logical variables (that is, 0 and 1), the set of integers, or the set of real numbers. Each of the foregoing classes is clearly a subclass of each class following it, and any operation defined on a class clearly applies to any of its subclasses. A variable which is nonnumeric will be called \textit{arbitrary}. In the Summary of Notation, the range and domain of each of the operators defined is specified in terms of the foregoing classes according to the conventions shown in Sec. S.1.

\section{Elementary operations}

\par The elementary operations employed include the ordinary arithmetic operations, the elementary operations of the logical calculus, and the residue and related operations arising in elementary number theory. In defining operations in the text, the symbol $\leftrightarrow$ will be used to denote equivalence of the pair of statements between which it occurs.

\subsection*{Arithmetic operations}

The ordinary arithmetic operations will be denoted by the ordinary symbols $+$, $-$, $\times$, and $÷$ and defined as usual except that the domain and range of multiplication will be extended slightly as follows. If one of the factors is a logical variable (0 or 1), the second may be arbitrary and the product then assumes the value of the second factor or zero according as the value of the first factor (the logical variable) is 1 or 0. Thus if the arbitrary factor is the literal ``q'', then

\begin{align*}
           & 0 \times \text{q} = \text{q} \times 0 = 0 \\
\text{and} & 1 \times \text{q} = \text{q} \times 1 = \text{q}.
\end{align*}

\par According to the usual custom in ordinary algebra, the multiplication symbol may be elided.

\subsection*{Logical operations}

\par The elementary logical operations \textit{and}, \textit{or}, and \textit{not} will be denoted by $\wedge$, $\vee$ and an overbar and are defined in the usual way as follows:

\begin{alignat*}{2}
 w ← u \wedge v  & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 1 \quad\text{and}\quad v = 1, \\
 w ← u \vee v    & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 1 \quad\text{or} \quad v = 1, \\
 w ← \overbar{u} & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 0.
\end{alignat*}

\par If $x$ and $y$ are numerical quantities, then the expression $x < y$ implies that the quantity $x$ stands in the relation ``less than'' to the quantity $y$. More generally, if $α$ and $β$ are arbitrary entities and $\mathcal{R}$ is any relation defined on them, the \textit{relational statement} $(α \mathcal{R} β)$ is a logical variable which is true (equal to 1) if and only if $α$ stands in the relation $\mathcal{R}$ to $β$. For example, if $x$ is any real number, then the function

$$
  (x > 0) - (x < 0)
$$

\par (commonly called the \textit{sign function} or $\text{sgn}\ x$) assumes the values 1, 0, or $-1$ according as $x$ is strictly positive, 0, or strictly negative. Moreover, the magnitude function $|x|$ may be defined as $|x| = x \times \text{sgn}\ x = x \times ((x > 0) - (x < 0))$.

\par The relational statement is a useful generalization of the Kronecker delta, that is $δ_j^i = (i = j)$. Moreover, it provides a convenient expression for a number of familiar logical operations. The \textit{exclusive or}, for example, may be denoted by $(u \neq v)$, and its negation (i.e., the equivalence function) may be denoted by $(u = v)$.

\subsection*{Residues and congruence}

\par For each set of integers $n$, $j$, and $b$, with $b > 0$, there exists a unique pair of integers $q$ and $r$ such that

$$
  n = bq + r,\quad j \leq r < j + b.
$$

\par The quantity $r$ is called the \textit{$j$-residue of n modulo b} and is denoted by $b |_j n$. For example, $3 |_0 9 = 0$, $3 |_1 9 = 3$, and $3 |_0 10 = 1$. Moreover, if $n \geq 0$, then $b |_0 n$ is the remainder obtained in dividing $n$ by $b$ and $q$ is the integral part of the quotient. A number $n$ is said to be of \textit{even parity} if its 0-residue modulo 2 is zero and of \textit{odd parity} if $2 |_0 n = 1$.

\par If two numbers $n$ and $m$ have the same $j$-residue modulo $b$, they differ by an integral multiple of $b$ and therefore have the same $k$-residue module $b$ for any $k$. If $b |_j n = b |_j m$, then $m$ and $n$ are said to be \textit{congruent mod $b$}. Congruency is transitive and reflexive and is denoted by

$$
  m \equiv n (\mod b).
$$

\par In classical treatments, such as <acronym title="Wright, H.N. (1939), First Course in Theory of Numbers, Wiley, New York.">Wright (1939)</acronym>, only the 0-residue is considered. The use of 1-origin indexing (cf. Sec. 1.5) accounts for the interest of the 1-residue.

\par A number represented in a positional notation (e.g., in a base ten or a base two number system) must, in practice, employ only a finite number of digits. It is therefore often desirable to approximate a number $x$ by an integer. For this purpose two functions are defined:

\begin{enumerate}
  \item the \textit{floor of x} (or integral part of $x$), denoted by $⌊x⌋$ and defined as the largest integer not exceeding $x$,
  \item the \textit{ceiling of x}, denoted by $⌈x⌉$ and defined as the smallest integer not exceeded by $x$.
\end{enumerate}

\noindent Thus

\begin{alignat*}{2}
  ⌈3.14⌉ = 4, & ⌊3.14⌋ = 3, & ⌊-3.14⌋ = -4, \\
  ⌈3.00⌉ = 3, & ⌊3.00⌋ = 3, & ⌊-3.00⌋ = -3.
\end{alignat*}

\par Clearly $⌈x⌉ = -⌊-x⌋$ and $⌊x⌋ \leq x \leq ⌈x⌉$. Moreover, $n = b⌊n ÷ b⌋ + b |_0 n$ for all integers $n$. Hence the integral quotient $⌊n ÷ b⌋$ is equivalent to the quantity $q$ occurring in the definition of the $j$-residue for the case $j = 0$.

\section{Structured operands}

\subsection*{Elementary operations}

\par Any operation defined on a single operand can be generalized to apply to each member of an array of related operands. Similarly, any binary operation (defined on two operands) can be generalized to apply to pairs of corresponding elements of two arrays. Since algorithms commonly incorporate processes which are repeated on each member of an array of operands, such generalization permits effective subordination of detail in their description. For example, the accounting process defined on the data of an individual bank account treats a number of distinct operands within the account, such as account number, name, and balance. Moreover, the over-all process is defined on a large number of similar accounts, all represented in a common format. Such structured arrays of variables will be called \textit{structured operands}, and extensive use will be made of three types, called \textit{vector}, \textit{matrix}, and \textit{tree}. As indicated in Sec. S.1 of the Summary of Notation, a structured operand is further classified as \textit{logical}, \textit{integral}, \textit{numerical}, or \textit{arbitrary}, according to the type of elements in contains.

\par A \textit{vector} $\vect{x}$ is the ordered array of elements $(\vect{x}_1, \vect{x}_2, \vect{x}_3, ..., \vect{x}_{ν(\vect{x})})$. The variable $\vect{x}_i$ is called the $i$th \textit{component} of the vector $\vect{x}$, and the number of components, denoted by $ν(\vect{x})$ (or simply by $ν$ when the determining vector is clear from context), is called the \textit{dimension} of $\vect{x}$. Vectors and their components will be represented in lower case boldface italics. A numerical vector $\vect{x}$ may be multiplied by a numerical quantity $k$ to produce the \textit{scalar multiple} $k \times \vect{x}$ (or $k\vect{x}$) defined as the vector $\vect{z}$ such that $\vect{z}_i = k \times \vect{x}_i$.

\par All elementary operations defined on individual variables are extended consistently to vectors as component-by-component operations. For example,

\begin{align*}
  \vect{z} = \vect{x} + \vect{y}      & \leftrightarrow \vect{z}_i = \vect{x}_i + \vect{y}_i, \\
  \vect{z} = \vect{x} \times \vect{y} & \leftrightarrow \vect{z}_i = \vect{x}_i \times \vect{y}_i, \\
  \vect{z} = \vect{x} ÷ \vect{y}      & \leftrightarrow \vect{z}_i = \vect{x}_i ÷ \vect{y}_i, \\
  \vect{z} = ⌈\vect{x}⌉               & \leftrightarrow \vect{z}_i = ⌈\vect{x}_i⌉, \\
  \vect{w} = \vect{u} \wedge \vect{v} & \leftrightarrow \vect{w}_i = \vect{u}_i \wedge \vect{v}_i, \\
  \vect{w} = (\vect{x} < \vect{y})    & \leftrightarrow \vect{w}_i = (\vect{x}_i < \vect{y}_i).
\end{align*}

\par Thus if $\vect{x} = (1, 0, 1, 1)$ and $\vect{y} = (0, 1, 1, 0)$ then $\vect{x} + \vect{y} = (1, 1, 2, 1)$, $\vect{x} \wedge \vect{y} = (0, 0, 1, 0)$, and $(\vect{x} < \vect{y}) = (0, 1, 0, 0)$.

\subsection*{Matrices}

\par A matrix $\mat{M}$ is the ordered two-dimensional array of variables

$$
  \begin{pmatrix}
    \mat{M}_1^1, & \mat{M}_2^1, & ..., & \mat{M}_{ν(\mat{M})}^1 \\
    \mat{M}_1^2, & \mat{M}_2^2, & ..., & \mat{M}_{ν(\mat{M})}^2 \\
    . & . & . & . \\
    \mat{M}_1^{μ(\mat{M})}, & \mat{M}_2^{μ(\mat{M})}, & ..., & \mat{M}_{ν(\mat{M})}^{μ(\mat{M})}
  \end{pmatrix}
$$

\par The vector $(\mat{M}_1^i, \mat{M}_2^i, ..., \mat{M}_ν^i)$ is called the $i$th \textit{row vector} of $\mat{M}$ and is denoted by $\mat{M}^i$. Its dimension $ν(\mat{M})$ is called the \textit{row dimension} of the matrix. The vector $(\mat{M}_j^1, \mat{M}_j^2, ..., \mat{M}_j^μ)$ is called the $j$th \textit{column vector} of $\mat{M}$ and is denoted by $\mat{M}_j$. Its dimension $μ(\mat{M})$ is called the \textit{column dimension} of the matrix.

\par The variable $\mat{M}_j^i$ is called the $(i,j)$th \textit{component} or \textit{element} of the matrix. A matrix and its elements will be represented by upper case boldface italics. Operations defined on each element of a matrix are generalized component by component to the entire matrix. Thus, if $\odot$ is any binary operator,

$$
  \mat{P} = \mat{M}
    \odot \mat{N} \leftrightarrow \mat{M}_j^i
    \odot \mat{N}_j^i.
$$

\subsection*{Index systems}

\par The subscript appended to a vector to designate a single component is called an \textit{index}, and the indices are normally chosen as a set of successive integers beginning at 1, that is, $\vect{x} = (\vect{x}_1, \vect{x}_2, ... \vect{x}_ν)$. It is, however, convenient to admit more general \textit{$j$-origin indexing} in which the set of successive integers employed as indices in any structured operand begin with a specified integer $j$.

\par The two systems of greatest interest are the common 1-origin system, which will be employed almost exclusively in this chapter, and the 0-origin system. The latter system is particularly convenient whenever the index itself must be represented in a positional number system and will therefore be employed exclusively in the treatment of computer organization in Chapter 2.

\section{Rotation}

\par The \textit{left rotation} of a vector $\vect{x}$ is denoted by $k ↑ \vect{x}$ and specifies the vector obtained by cyclical left shift of the components of $\vect{x}$ by $k$ places. Thus if $\vect{a} = (1, 2, 3, 4, 5, 6)$, and $\vect{b} = (\text{c}, \text{a}, \text{n}, \text{d}, \text{y})$, then $2 ↑ \vect{a} = (3, 4, 5, 6, 1, 2)$ and $3 ↑ \vect{b} = 8 ↑ \vect{b} = (\text{d}, \text{y}, \text{c}, \text{a}, \text{n})$. Formally,%^{<a href="#note1a">[a]</a>}

$$
  \vect{z} = k ↑ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j,\quad
    \text{where}\; j = ν|_1(i + k).
$$

\noindent \textit{Right rotation} is denoted by $k ↓ \vect{x}$ and is defined analogously. Thus

$$
  \vect{z} = k ↓ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j,\quad
    \text{where}\; j = ν|_1(i - k).
$$

\noindent If $k = 1$, it may be elided. Thus $↑ \vect{b} = (\text{a}, \text{n}, \text{d}, \text{y}, \text{c})$.

\par Left rotation is extended to matrices in two ways as follows:

\begin{alignat*}{2}
 \mat{A} ← \vect{j} ↑ \mat{B} & \leftrightarrow & \mat{A}^i = \vect{j}_i ↑ \mat{B}^i \\
 \mat{C} ← \vect{k} \Uparrow \mat{B} & \leftrightarrow & \mat{C}_i = \vect{k}_j ↑ \mat{B}_j
\end{alignat*}

\par The first operation is an extension of the basic vector rotation to each row of the matrix and is therefore called \textit{row rotation}. The second operation is the corresponding column operation and is therefore denoted by the doubled operation symbol $\Uparrow$. For example, if

$$
  \vect{k} = (0, 1, 2),
$$

\noindent and

$$
  \mat{B} = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}
$$

\noindent then

$$
  \vect{k} ↑ \mat{B} = \begin{pmatrix} a & b & c \\ e & f & d \\ i & g & h \end{pmatrix}
  \quad \text{and} \quad
  \vect{k} \Uparrow \mat{B} = \begin{pmatrix} a & e & i \\ d & h & c \\ g & b & f \end{pmatrix}
$$

\par Right rotation is extended analogously.

\section{Special vectors}

\par Certain special vectors warrant special symbols. In each of the following definitions, the parameter $n$ will be used to specify the dimension. The \textit{interval vector} $\textbf{ι}^j(n)$ is defined as the vector of integers beginning with $j$. Thus $\textbf{ι}^0(4)=(0, 1, 2, 3)$, $\textbf{ι}^1(4)=(1, 2, 3, 4)$, and $\textbf{ι}^{-7}(5)= (-7, -6, -5, -4, -3)$. Four types of logical vectors are defined as follows. The $j$th \textit{unit vector} $\textbf{ϵ}^j(n)$ has a one in the $j$th position, that is, $(\textbf{ϵ}^j(n))_k = (k = j)$. The \textit{full vector} $\textbf{ϵ}(n)$ consists of all ones. The vector consisting of all zeros is denoted both by $0$ and by $\overbar{\textbf{ϵ}}(n)$. The \textit{prefix vector of weight j} is denoted by $\mathbf{α}^j(n)$ and possesses ones in the first $k$ positions, where $k$ is the lesser of $j$ and $n$. The \textit{suffix vector} $\textbf{ω}^j(n)$ is defined analogously. Thus $\textbf{ϵ}^2(3) = (0, 1, 0)$, $\textbf{ϵ}(4) = (1, 1, 1, 1)$, $\mathbf{α}^3(5) = (1, 1, 1, 0, 0)$, $\textbf{ω}^3(5) = (0, 0, 1, 1, 1)$, and $\mathbf{α}^7(5) = \mathbf{α}^5(5) = (1, 1, 1, 1, 1)$. Moreover, $\textbf{ω}^j(n) = j ↑ \mathbf{α}^j(n)$, and $\mathbf{α}^j(n) = j ↓ \textbf{ω}^j(n)$.

\par A logical vector of the form $\mathbf{α}^h(n) \wedge \textbf{ω}^j(n)$ is called an \textit{infix vector}. An infix vector can also be specified in the form $j ↓ \mathbf{α}^k(n)$, which displays its weight and location more directly.

\par An operation such as $\vect{x} \wedge \vect{y}$ is defined only for \textit{compatible} vectors $\vect{x}$ and $\vect{y}$, that is, for vectors of like dimension. Since this compatibility requirement can be assumed to specify implicitly the dimension of one of the operands, elision of the parameter $n$ may be permitted in the notation for the special vectors. Thus, if $y = (3, 4, 5, 6, 7)$, the expression $\textbf{ϵ} \times \vect{y}$ and $\textbf{ϵ}^j \times \vect{y}$ imply that the dimensions of $\textbf{ϵ}$ and $\textbf{ϵ}^j$ are both 5. Moreover, elision of $j$ will be permitted for the interval vector $\textbf{ι}^j(n)$ (or $\textbf{ι}^j$), and for the residue operator $|_j$ when $j$ is the index origin in use.

\par It is, of course, necessary to specify the index origin in use at any given time. For example, the unit vector $\textbf{ϵ}^3(5)$ is $(0, 0, 1, 0, 0)$ in a 1-origin system and $(0, 0, 0, 1, 0)$ in a 0-origin system, even though the definition (that is, $(\textbf{ϵ}^j(n))_k = (k = j)$) remains unchanged. The prefix and suffix vectors are, of course, independent of the index origin. Unless otherwise specified, 1-origin indexing will be assumed.

\par The vector $\textbf{ϵ}(0)$ is a vector of dimension zero and will be called the \textit{null vector}. It should not be confused with the special null element $∘$.

\section{Reduction}

\par An operation (such as summation) which is applied to all components of a vector to produce a result of a simpler structure is called a \textit{reduction}. The $\odot$-reduction of a vector $\vect{x}$ is denoted by $\odot/\vect{x}$ and defined as

% Unbalanced parens, but seems intentional
$$
  \vect{z} ←
    \odot/\vect{x} \leftrightarrow \vect{z} =
      (\cdots ((\vect{x}_1 \odot \vect{x}_2) \odot \vect{x}_3) \odot \cdots)
    \odot \vect{x}_{\textit{ν}}),
$$

\noindent where $\odot$ is any binary operator with a suitable domain. Thus $+/\vect{x}$ is the sum, $\times/\vect{x}$ is the product, and $\vee/\vect{x}$ is the logical sum of the components of a vector $\vect{x}$. For example, $\times/\textbf{ι}^1(5) = 1 \times 2 \times 3 \times 4 \times 5$, $\times/\textbf{ι}^1(n) = n!$, and $+/\textbf{ι}^1(n) = n(n + 1)/2$.

\par As a further example, De Morgan's law may be expressed as $\wedge/\vect{u} = \overline{\vee/\overbar{\vect{u}}}$, where $\vect{u}$ is a logical vector of dimension two. Moreover, a simple inductive argument (Exercise 1.10) shows that the foregoing expression is the valid generalization of De Morgan's law for a logical vector $\vect{u}$ of arbitrary dimension.

\par A relation $\mathcal{R}$ incorporated into a relational statement $(x\mathcal{R}y)$ becomes, in effect, an operator on the variables $x$ and $y$. Consequently, the reduction $\mathcal{R}/\vect{x}$ can be defined in a manner analogous to that of $(\odot/\vect{x})$, that is,

$$
  \mathcal{R}/\vect{x} = (\cdots ((\vect{x}_1 \mathcal{R} \vect{x}_2) \mathcal{R} \vect{x}_3) \mathcal{R} \cdots) \mathcal{R} \vect{x}_{\textit{ν}}).
$$

\noindent The parentheses now imply relational statements as well as grouping. The relational reductions of practical interest are $\neq/\vect{u}$, and $=/\vect{u}$, the \textit{exclusive-or} and the \textit{equivalence} reduction, respectively.

\par The inductive argument of Exercise 1.10 shows that $\neq/\vect{u} = 2 |_0 (+/\vect{u})$. For example, if $\vect{u} = (1,0,1,1,0)$, then

\begin{align*}
  \neq/\vect{u} & = ((((1 \neq 0) \neq 1) \neq 1) \neq 0) \\
                & = (((1 \neq 1) \neq 1) \neq 0) \\
                & = ((0 \neq 1) \neq 0) \\
                & = (1 \neq 0) = 1,
\end{align*}

\noindent and $2 |_0 (+/\vect{u}) = 2 |_0 3 = 1$. Similarly, $=/\vect{u} = \overline{2 |_0 (+/\overbar{\vect{u}})}$, and as a consequence,

$$
  \neq/\vect{u} = \overline{=/\overbar{\vect{u}}},
$$

\noindent a useful companion to De Morgan's law.

\par To complete the system it is essential to define the value of $\odot/\textbf{ϵ}(0)$, the reduction of the null vector of dimension zero, as the identity element of the operator or relation $\odot$. Thus $+/\textbf{ϵ}(0) = \vee/\textbf{ϵ}(0) = 0$, and $\times/\textbf{ϵ}(0) = \wedge/\textbf{ϵ}(0) = 1$.

\par A reduction operation is extended to matrices in two ways. A \textit{row reduction} of a matrix $\mat{X}$ by an operator $\odot$ is denoted by

$$
  \vect{y} ← \odot/\mat{X}
$$

\noindent and specifies a vector $\vect{y}$ of dimension $μ(\mat{X})$ such that $\vect{y}_i = \odot/\mat{X}^i$. A \textit{column reduction} of $\mat{X}$ is denoted by $\vect{z} ← \odot/\!/\mat{X}$ and specifies a vector $\vect{z}$ of dimension $\textit{ν}(\mat{X})$ such that $\vect{z}_j = \odot/\!/\mat{X}_j$.

\par For example, if

$$
  \mat{U} = \begin{pmatrix} 1&0&1&0 \\ 0&0&1&1 \\ 1&1&1&0 \end{pmatrix}
$$

\noindent then $+/\mat{U} = (2, 2, 3)$, $+/\!/\mat{U} = (2, 1, 3, 1)$, $\wedge/\!/\mat{U} = (0, 0, 1, 0)$, $\neq/\mat{U} = (0, 0, 1)$, $=/\!/\mat{U} = (0, 1, 1, 1)$, and $+/(=/\!/\mat{U}) = 3$.

\section{Selection}

\subsection*{Compression}

\par The effective use of structured operands depends not only on generalized operations but also on the ability to specify and select certain elements or groups of elements. The selection of single elements can be indicated by indices, as in the expressions $\vect{v}_i$, $\mat{M}^i$, $\mat{M}_j$, and $\mat{M}_i^j$. Since selection is a binary operation (i.e., to select or not to select), more general selection is conveniently specified by a logical vector, each unit component indicating selection of the corresponding component of the operand.

\par The selection operation defined on an arbitrary vector $\vect{a}$ and a compatible (i.e., equal in dimension) logical vector $\vect{u}$ is denoted by $\vect{c} ← \vect{u}/\vect{a}$ and is defined as follows: the vector $\vect{c}$ is obtained from $\vect{a}$ by suppressing from $\vect{a}$ each component $\vect{a}_i$ for which $\vect{u}_i = 0$. The vector $\vect{u}$ is said to \textit{compress} the vector $\vect{a}$. Clearly $ν(\vect{c}) = +/\vect{u}$. For example, if $\vect{u} = (1, 0, 0, 0, 1, 1)$ and $\vect{a} = (\text{M}, \text{o}, \text{n}, \text{d}, \text{a}, \text{y})$, then $\vect{u}/\vect{a} =$ (\text{M}, \text{a}, \text{y}). Moreover, if $n$ is even and $\vect{v} = (2\textbf{ϵ}) |_0 \textbf{ι}^1(n) = (1, 0, 1, 0, 1, ...)$, then $\vect{v}/\textbf{ι}^1(n) = (1, 3, 5, ..., n-1)$ and $+/(\vect{v}/\textbf{ι}^1(n)) = (n/2)^2$.

\par \textit{Row compression} of a matrix, denoted by $\vect{u}/\mat{A}$, compresses each row vector $\mat{A}^i$ to form a matrix of dimension $μ(\mat{A}) \times +/\vect{u}$. \textit{Column compression}, denoted by $\vect{u}/\!/\mat{A}$, compresses each column vector $\mat{A}_j$ to form a matrix of dimension $+/\vect{u} \times ν(\mat{A})$. Compatibility conditions are $ν(\vect{u}) = ν(\mat{A})$ for row compression, and $ν(\vect{u}) = μ(\mat{A})$ for column compression. For example, if $\mat{A}$ is an arbitrary $3 \times 4$ matrix, $\vect{u} = (0, 1, 0, 1)$ and $\vect{v} = (1, 0, 1)$; then

$$
  \vect{u}/\mat{A} = \begin{pmatrix}
    \mat{A}_2^1 & \mat{A}_4^1 \\
    \mat{A}_2^3 & \mat{A}_4^3 \\
    \mat{A}_2^2 & \mat{A}_4^2
  \end{pmatrix}, \quad
  \vect{v}/\!/\mat{A} = \begin{pmatrix}
    \mat{A}_1^1 & \mat{A}_2^1 & \mat{A}_3^1 & \mat{A}_4^1 \\
    \mat{A}_1^3 & \mat{A}_2^3 & \mat{A}_3^3 & \mat{A}_4^3
  \end{pmatrix}
$$

\noindent and

$$
  \vect{u}/\vect{v}/\!/\mat{A} = \vect{v}/\!/\vect{u}/\mat{A} =
    \begin{pmatrix}
      \mat{A}_2^1 & \mat{A}_4^1 \\
      \mat{A}_2^3 & \mat{A}_4^3
    \end{pmatrix}
$$

\par It is clear that \textit{row} compression \textit{suppresses columns} corresponding to zeros of the logical vector and that \textit{column} compression \textit{suppresses rows}. This illustrates the type of confusion in nomenclature which is avoided by the convention adopted in Sec. 1.3: an operation is called a \textit{row operation} if the underlying operation from which it is generalized is applied to the row vectors of the matrix, and a \textit{column operation} if it is applied to columns.

\par \textbf{Example 1.1}. A bank makes a quarterly review of accounts to produce the following four lists:
\begin{enumerate}
  \item the name, account number, and balance for each account with a balance less than two dollars.
  \item the name, account number, and balance for each account with a negative balance exceeding one hundred dollars.
  \item the name and account number of each account with a balance exceeding one thousand dollars.
  \item all unassigned account numbers.
\end{enumerate}

\noindent The ledger may be described by a matrix

$$
  \mat{L}
    = (\mat{L}_1, \mat{L}_2, \mat{L}_3)
    = \begin{pmatrix} \mat{L}^1 \\.\\.\\.\\ \mat{L}^m \end{pmatrix}
$$

\noindent with column vectors $\mat{L}_1, \mat{L}_2$, and $\mat{L}_3$, representing names, account numbers, and balances, respectively, and with row vectors $\mat{L}^1$, $\mat{L}^2$, ..., $\mat{L}^m$, representing individual accounts. An unassigned account number is identified by the word ``none'' in the name position. The four output lists will be denoted by the matrices $\mat{P}$, $\mat{Q}$, $\mat{R}$, and $\mat{S}$, respectively. They can be produced by Program 1.9.

% (END EXAMPLE 1.1)

\par (TODO: PROGRAM 1.9)

\par \textbf{Program 1.9} Selection on bank ledger $\mat{L}$ (Example 1.1)

\par \textbf{Program 1.9}. Since $\mat{L}_3$ is the vector of balances, and $2\textbf{ϵ}$ is a compatible vector each of whose components equals two, the relational statement $(\mat{L}_3 < 2\textbf{ϵ})$ defines a logical vector having unit components corresponding to those accounts to be included in the list $\mat{P}$. Consequently, the column compression of step 1 selects the appropriate rows of $\mat{L}$ to define $\mat{P}$. Step 2 is similar, but step 3 incorporates an additional row compression by the compatible prefix vector $\mathbf{α}^2 = (1,1,0)$ to select columns one and two of $\mat{L}$. Step 4 represents the comparison of the name (in column $\mat{L}_1$) with the literal ``none'', the selection of each row which shows agreement, and the suppression of all columns but the second. The expression ``none $\textbf{ϵ}$'' occurring in step 4 illustrates the use of the extended definition of multiplication.

% (END PROGRAM 1.9 DESCRIPTION)

\subsection*{Mesh, mask, and expansion}

\par A logical vector $\vect{u}$ and the two vectors $\vect{a} = \overbar{\vect{u}}/\vect{c}$ and $\vect{b} = \vect{u}/\vect{c}$, obtained by compressing a vector $\vect{c}$, collectively determine the vector $\vect{c}$. The operation which specifies $\vect{c}$ as a function of $\vect{a}$, $\vect{b}$, and $\vect{u}$ is called a \textit{mesh} and is defined as follows: If $\vect{a}$ and $\vect{b}$ are arbitrary vectors and if $\vect{u}$ is a logical vector such that $+/\overbar{\vect{u}} = \textit{ν}(\vect{a})$ and $+/\vect{u} = \textit{ν}(\vect{b})$, then the \textit{mesh of} $\vect{a}$ \textit{and} $\vect{b}$ \textit{on} $\vect{u}$ is denoted by $\backslash\vect{a}, \vect{u}, \vect{b}\backslash$ and is defined as the vector $\vect{c}$ such that $\overbar{\vect{u}}/\vect{c} = \vect{a}$ and $\vect{u}/\vect{c} = \vect{b}$. The mesh operation is equivalent to choosing successive components of $\vect{c}$ from $\vect{a}$ or $\vect{b}$ according as the successive components of $\vect{u}$ are $0$ or $1$. If, for example, $\vect{a} = (\text{s}, \text{e}, \text{k})$, $\vect{b} = (\text{t}, \text{a})$, and $\vect{u} = (0, 1, 0, 1, 0)$, then $\backslash\vect{a}, \vect{u}, \vect{b}\backslash = (\text{s}, \text{t}, \text{e}, \text{a}, \text{k})$. As a further example, Program 1.10a (which describes the merging of the vectors $\vect{a}$ and $\vect{b}$, with the first and every third component thereafter chosen from $\vect{a})$ can be described alternatively as shown in Program 1.10b. Since $\textbf{ι}^1 = (1, 2, 3, 4, 5, 6, ...)$, then $(3\textbf{ϵ}) |_0 \textbf{ι}^1 = (1, 2, 0, 1, 2, 0, ...)$, and consequently the vector $\vect{u}$ specified by step 1 is of the form $\vect{u} = (0, 1, 1, 0, 1, 1, 0, ...)$.

\par (TODO: PROGRAM 1.10)

\par \textbf{Program 1.10} Interfiling program

\par Mesh operations on matrices are defined analogously, row mesh and column mesh being denoted by single and double reverse virgules, respectively.

\par The \textit{catenation} of vectors $\vect{x}, \vect{y}, ..., \vect{z}$ is denoted by $\vect{x} \oplus \vect{y} \oplus \cdots \oplus \vect{z}$ and is defined by the relation

$$
  \vect{x} \oplus \vect{y} \oplus \cdots \oplus \vect{z}
    = (\vect{x}_1, \vect{x}_2, ...,
       \vect{x}_{ν(\vect{x})}, \vect{y}_1, \vect{y}_2, ...,
       \vect{z}_{ν(\vect{z})}).
$$

\noindent Catenation is clearly associative and for two vectors $\vect{x}$ and $\vect{y}$ it is a special case of the mesh $\backslash\vect{x}, \vect{u}, \vect{y}\backslash$ in which $\vect{u}$ is a suffix vector.

\par In numerical vectors (for which addition of two vectors is defined), the effect of the general mesh operation can be produced as the sum of two meshes, each involving one zero vector. Specifically,

\begin{alignat*}{2}
  \backslash\vect{x}, \vect{u}, \vect{y}\backslash
    & = \backslash\vect{x}, \vect{u}, 0\backslash
    & + \backslash0, \vect{u}, \vect{y}\backslash
  \\& = \backslash0, \overbar{\vect{u}}, \vect{x}\backslash
    & + \backslash0, \vect{u}, \vect{y}\backslash.
\end{alignat*}

\noindent The operation $\backslash0, \vect{u}, \vect{y}\backslash$ proves very useful in numerical work and will be called \textit{expansion} of the vector $\vect{y}$, denoted by $\vect{u}\backslash\vect{y}$. Compression of $\vect{u}\backslash\vect{y}$ by $\vect{u}$ and by $\overbar{\vect{u}}$ clearly yields $\vect{y}$ and 0, respectively. Moreover, any numerical vector $\vect{x}$ can be \textit{decomposed} by a compatible vector $\vect{u}$ according to the relation

$$
  \vect{x}
    = \overbar{\vect{u}}\backslash\overbar{\vect{u}}/\vect{x}
    + \vect{u}\backslash\vect{u}/\vect{x}.
$$

\noindent The two terms are vectors of the same dimension which have no nonzero components in common. Thus if $\vect{u} = (1, 0, 1, 0, 1)$, the decomposition of $\vect{x}$ appears as

$$
  \vect{x} = (0, \vect{x}_2, 0, \vect{x}_4, 0) + (\vect{x}_1, 0, \vect{x}_3, 0, \vect{x}_5).
$$

\par Row expansion and column expansion of matrices are defined and denoted analogously. The decomposition relations become

\begin{align*}
  & \mat{X}
    = \overbar{\vect{u}}\backslash\overbar{\vect{u}}/\mat{X}
    + \vect{u}\backslash\vect{u}/\mat{X},\\
\text{and}
  & \mat{X}
    = \overbar{\vect{u}}\backslash\backslash\overbar{\vect{u}}/\!/\mat{X}
    + \vect{u}\backslash\backslash\vect{u}/\!/\mat{X}.
\end{align*}

\par The \textit{mask} operation is defined formally as follows:

$$
  \vect{c} ← /\vect{a}, \vect{u}, \vect{b}/
  \leftrightarrow 
  \overbar{\vect{u}}/\vect{c} = \overbar{\vect{u}}/\vect{a},
  \quad\text{and}\quad
  \vect{u}/\vect{c} = \vect{u}/\vect{b}.
$$

\noindent The vectors $\vect{c}$, $\vect{a}$, $\vect{u}$, and $\vect{b}$ are clearly of a common dimension and $\vect{c}_i = \vect{a}_i$ or $\vect{b}_i$ according as $\vect{u}_i = 0$ or $\vect{u}_i = 1$. Moreover, the compress, expand, mask, and mesh operations on vectors are related as follows:

\begin{align*}
  /\vect{a}, \vect{u}, \vect{b}/
    & = \backslash\overbar{\vect{u}}/\vect{a}, \vect{u}, \vect{u}/\vect{b}\backslash, \\
  \backslash\vect{a}, \vect{u}, \vect{b}\backslash
    & = /\overbar{\vect{u}}\backslash\vect{a}, \vect{u}, \vect{u}\backslash\vect{b}/. \\
\end{align*} 

\noindent Analogous relations hold for the row mask and row mesh and for the column mask and column mesh.

\par Certain selection operations are controlled by logical matrices rather than by logical vectors. The \textit{row compression} $\mat{U}/\mat{A}$ selects elements of $\mat{A}$ corresponding to the nonzero elements of $\mat{U}$. Since the nonzero elements of $\mat{U}$ may occur in an arbitrary pattern, the result must be construed as a vector rather than a matrix. More precisely, $\mat{U}/\mat{A}$ denotes the catenation of the vectors $\mat{U}^i/\mat{A}^i$ obtained by row-by-row compression of $\mat{A}$ by $\mat{U}$. The \textit{column compression} $\mat{U}/\!/\mat{A}$ denotes the catenation of the vectors $\mat{U}_j/\mat{A}_j$. If, for example

$$
  \mat{U} = \begin{pmatrix}
    0&1&0&1&1 \\
    1&1&0&0&0 \\
    0&1&1&0&0
  \end{pmatrix}
$$
\noindent then
$$ \mat{U}/\mat{A} = (\mat{A}_2^1, \mat{A}_4^1, \mat{A}_5^1, \mat{A}_1^2, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3), $$
\noindent and
$$ \mat{U}/\!/\mat{A} = (\mat{A}_1^2, \mat{A}_2^1, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3, \mat{A}_4^1, \mat{A}_5^1). $$

\par Compression by the full matrix $\mathsf{\mat{E}}$ (defined by $\overbar{\mathsf{\mat{E}}} = 0$) produces either a \textit{row list} ($\mathsf{\mat{E}}/\mat{A}$) or a \textit{column list} ($\mathsf{\mat{E}}/\!/\mat{A}$) of the matrix $\mat{A}$. Moreover, a numerical matrix $\mat{X}$ can be represented jointly by the logical matrix $\mat{U}$ and the row list $\mat{U}/\mat{X}$ (or the column list $\mat{U}/\!/\mat{X}$), where $\mat{U} = (\mat{X} \neq 0)$. If the matrix $\mat{X}$ is sparse (i.e., the components are predominantly zero), this provides a compact representation which may reduce the computer storage required for $\mat{X}$.

\par The compression operations controlled by matrices also generate a group of corresponding mesh and mask operations as shown in Sec. S.9.

\section{Selection vectors}

\par The logical vector $\vect{u}$ involved in selection operations may itself arise in various ways. It may be a prefix vector $\mathbf{α}^j$, a suffix $\textbf{ω}^j$, or an infix $(i ↓ \mathbf{α}^j)$; the corresponding compressed vectors $\mathbf{α}^j/\vect{x}, \textbf{ω}^j/\vect{x}$, and $(i ↓ \mathbf{α}^j)/\vect{x}$ are called a \textit{prefix}, \textit{suffix}, and \textit{infix} of $\vect{x}$, respectively.

\par Certain selection vectors arise as functions of other vectors, e.g., the vector $(\vect{x} \geq$ 0) can be used to select all nonnegative components of $\vect{x}$, and $(\vect{b} \neq *\textbf{ϵ})$ serves to select all components of $\vect{b}$ which are not equal to the literal ``*''. Two further types are important: the selection of the longest unbroken prefix (or suffix) of a given logical vector, and the selection of the set of distinct components occurring in a vector. The first is useful in left (or right) justification or in a corresponding compression intended to eliminate leading or trailing ``filler components'' of a vector (such as left zeros in a number or right spaces in a short name).

\par For any logical vector $u$, the \textit{maximum prefix} of $\vect{u}$ is denoted by $α/\vect{u}$ and defined as follows:

$$
  \vect{v} ← α/\vect{u} \leftrightarrow \vect{v} = \mathbf{α}^j
$$

\noindent where $j$ is the maximum value for which $\wedge/(\mathbf{α}^j/\vect{u}) = 1$. The maximum suffix is denoted by $ω/\vect{u}$ and is defined analogously. If, for example, $\vect{u} = (1, 1, 1, 0, 1, 1, 0, 0, 1, 1)$, then $α/\vect{u} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0)$, $ω/\vect{u} = (0, 0, 0, 0, 0, 0, 0, 0, 1, 1)$, $+/α/\vect{u} = 3$, and $+/ω/\vect{u} = 2$.

\par The leading zeros of a numerical vector $x$ can clearly be removed either by compression:

$$
  \vect{y} ← \overline{(α/(\vect{x} = 0))}/\vect{x}
$$

\noindent or by left justification (normalization):

$$
  \vect{z} ← (+/α/(\vect{x} = 0)) ↑ \vect{x}.
$$

\par The extension of the maximum prefix operation to the rows of a logical matrix $\mat{U}$ is denoted by $α/\mat{U}$ and defined as the compatible logical matrix $\mat{V}$, such that $\mat{V}^i = α/\mat{U}^i$. The corresponding maximum column prefix operation is denoted by $α/\!/\mat{U}$. Right justification of a numerical matrix $\mat{X}$ is achieved by the rotation $\vect{k} ↓ \mat{X}$, where $\vect{k} = +/ω/(\mat{X} = 0)$, and \textit{top justification} is achieved by the rotation $(+/\!/α/\!/(\mat{X} = 0)) \Uparrow \mat{X}$ (see Sec. S.6.)

\par A vector whose components are all distinct will be called an \textit{ordered set}. The \textit{forward set selector} on $\vect{b}$ is a logical vector denoted by $σ/\vect{b}$ and defined as follows: the statement $\vect{v} ← σ/\vect{b}$ implies that $\vect{v}_j = 1$ if and only if $\vect{b}_j$ differs from all preceding components of $\vect{b}$. Hence $\vect{v}/\vect{b}$ is a set which contains all distinct components of $\vect{b}$, and $+/\vect{v}/\textbf{ι}$ is a minimum. For example, if $\vect{c} = (\text{C}, \text{a}, \text{n}, \text{a}, \text{d}, \text{a})$, then $(σ/\vect{c})/\vect{c} = (\text{C}, \text{a}, \text{n}, \text{d})$ is a list of the distinct letters in $\vect{c}$ in order of occurrence. Clearly $(σ/\vect{b})/\vect{b} = \vect{b}$ if and only if $\vect{b}$ is a set.

\par The backward set selector $τ/\vect{b}$ is defined analogously (e.g., $(τ/\vect{c})/\vect{c} = (\text{C}, \text{n}, \text{d}, \text{a})$). Forward and backward set selection are extended to matrices by both rows ($σ/\mat{B}$, and $τ/\mat{B}$) and columns ($σ/\!/\mat{B}$, and $τ/\!/\mat{B}$) in the established manner.

\section{The generalized matrix product}

\par The ordinary matrix product of matrices $\mat{X}$ and $\mat{Y}$ is commonly denoted by $\vect{XY}$ and defined as follows:

\begin{equation*}
  \begin{split}
    \mat{Z} ← \vect{X}\vect{Y} \leftrightarrow \mat{Z}_j^i
      = \sum_{k=1}^{ν(\mat{X})} \mat{X}_k^i \times \mat{Y}_j^k,
  \end{split}
\quad\quad
  \begin{split}
    i &= 1, 2, ..., μ(\mat{X}) \\
    j &= 1, 2, ..., ν(\mat{Y}).
  \end{split}
\end{equation*}

\noindent It can be defined alternatively as follows:

$$
  (\vect{X}\vect{Y})_j^i = +/(\mat{X}^i \times \mat{Y}_j).
$$

\par This formulation emphasizes the fact that matrix multiplication incorporates two elementary operations ($+$, $\times$) and suggests that they be displayed explicitly. The ordinary matrix product will therefore be written as $\mat{X} {+ \atop \times} \mat{Y}$.

\par More generally, if $\odot_1$ and $\odot_2$ are any two operators (whose domains include the relevant operands), then the \textit{generalized matrix product} $\mat{X} {\odot_1 \atop \odot_2} \mat{Y}$ is defined as follows:

\begin{equation*}
  \begin{split}
    (\mat{X} {\odot_1 \atop \odot_2} \mat{Y})_j^i
      = \odot_1/(\mat{X}^i \odot_2 \mat{Y}_j),
  \end{split}
\quad\quad
  \begin{split}
    i &= 1, 2, ..., μ(\mat{X}) \\
    j &= 1, 2, ..., ν(\mat{Y}).
  \end{split}
\end{equation*}

\noindent For example, if

$$
  \mat{A} = \begin{pmatrix}
    1&3&2&0 \\
    2&1&0&1 \\
    4&0&0&2
  \end{pmatrix} \quad\text{and}\quad \mat{B} = \begin{pmatrix}
    4&1 \\
    0&3 \\
    0&2 \\
    2&0
  \end{pmatrix}
$$

\noindent then

$$
  \mat{A} {+ \atop \times} \mat{B} = \begin{pmatrix}
     4&14 \\
    10& 5 \\
    20& 4
  \end{pmatrix}, \quad
  \mat{A} {\wedge \atop =} \mat{B} = \begin{pmatrix}
    0&1 \\
    0&0 \\
    1&0
  \end{pmatrix}
$$

$$
  \mat{A} {\vee \atop \neq} \mat{B} = \begin{pmatrix}
    1&0 \\
    1&1 \\
    0&1
  \end{pmatrix}, \quad\text{and}\quad
  (\mat{A} \neq 0) {+ \atop /} \mat{B} = \begin{pmatrix} 
    4&6 \\
    6&4 \\
    6&1
  \end{pmatrix}
$$

\par The generalized matrix product and the selection operations together provide an elegant formulation in several established areas of mathematics. A few examples will be chosen from two such areas, symbolic logic and matrix algebra.

\par In symbolic logic, De Morgan's laws ($\wedge/\vect{u} = \overline{\vee/\overbar{\vect{u}}}$ and $=/\vect{u} = \overline{\neq/\overbar{\vect{u}}}$) can be applied directly to show that

$$
  \mat{U} {\neq \atop \wedge} \mat{V} = \overline{\overbar{\mat{U}} {= \atop \vee} \overbar{\mat{V}}}.
$$

\par In matrix algebra, the notion of partitioning a matrix into submatrices of contiguous rows and columns can be generalized to an arbitrary partitioning specified by a logical vector $\vect{u}$. The following easily verifiable identities are typical of the useful relations which result:

\begin{align*}
  \mat{X} {+ \atop \times} \mat{Y} &= (\overbar{\vect{u}}/\mat{X}) {+ \atop \times} (\overbar{\vect{u}}/\!/\mat{Y}) + (\vect{u}/\mat{X}) {+ \atop \times} (\vect{u}/\!/\mat{Y}), \\
  \vect{u}/(\mat{X} {+ \atop \times} \mat{Y}) &= \mat{X} {+ \atop \times} (\vect{u}/\mat{Y}), \\
  \vect{u}/\!/(\mat{X} {+ \atop \times} \mat{Y}) &= (\vect{u}/\!/\mat{X}) {+ \atop \times} \mat{Y}. \\
\end{align*}

\par The first identity depends on the commutativity and associativity of the operator $+$ and can clearly be generalized to other associative commutative operators, such as $\wedge$, $\vee$, and $\neq$.

\par The generalized matrix product applies directly (as does the ordinary matrix product $\mat{X} {+ \atop \times} \mat{Y}$) to vectors considered as row (that is, $1 \times n$) or as column matrices. Thus:

\begin{alignat*}{2}
  \vect{z} ← \mat{X}  {\odot_1 \atop \odot_2} \vect{y} & \leftrightarrow \vect{z}_{i} &= \odot_1/(\mat{X}^{i} \odot_2 \vect{y}), \\
  \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \mat{X}  & \leftrightarrow \vect{z}_{j} &= \odot_1/(\vect{y} \odot_2 \mat{X}_{j}), \\
  \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \vect{x} & \leftrightarrow \vect{z}     &= \odot_1/(\vect{y} \odot_2 \vect{x}).
\end{alignat*}

\par The question of whether a vector enters a given operation as a row vector or as a column vector is normally settled by the requirement of conformability, and no special indication is required. Thus $\vect{y}$ enters as a column vector in the first of the preceding group of definitions and as a row vector in the last two. The question remains, however, in the case of the two vector operands, which may be considered with the pre-operand either as a row (as in the scalar product $\vect{y} {+ \atop \times} \vect{x}$) or as a column. The latter case produces a matrix $\mat{Z}$ and will be denoted by

$$
  \mat{Z} ← \vect{y} {\circ \atop \odot_2} \vect{x},
$$

\par where $\mat{Z}_j^i = \vect{y}_i \odot_2 \vect{x}_j$, $μ(\mat{Z}) = ν(\vect{y})$, and $ν(\mat{Z}) = ν(\vect{x})$.%^{<a href="#note1b">[b]</a>}
For example, if each of the vectors indicated is of dimension three, then

$$
  \textbf{ϵ} {\circ \atop \times} \vect{y} = \begin{pmatrix}
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3 \\
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3 \\
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3
  \end{pmatrix}; \quad
  \vect{y} {\circ \atop \times} \textbf{ϵ} = \begin{pmatrix}
    \vect{y}_1 & \vect{y}_1 & \vect{y}_1 \\
    \vect{y}_2 & \vect{y}_2 & \vect{y}_2 \\
    \vect{y}_3 & \vect{y}_3 & \vect{y}_3
  \end{pmatrix}
$$
$$
  \mathbf{α}^2(3) {\circ \atop \wedge} \mathbf{α}^2(3) = \begin{pmatrix}
    1&1&0 \\
    1&1&0 \\
    0&0&0
  \end{pmatrix}
$$

\section{Transpositions}

\par Since the generalized matrix product is defined on columns of the post-operand and rows of the pre-operand, convenient description of corresponding operations on the rows of the post-operand and columns of the pre-operand demands the ability to \textit{transpose} a matrix $\mat{B}$, that is, to specify a matrix $\mat{C}$ such that $\mat{C}_i^j = \mat{B}_j^i$. In ordinary matrix algebra this type of transposition suffices, but in more general work transpositions about either diagonal and about the horizontal and the vertical are also useful. Each of these transpositions of a matrix $\mat{B}$ is denoted by a superior arrow whose inclination indicates the axis of the transposition. Thus:

\begin{equation*}
  \begin{split}
    \mat{C} & ← \mathop{\mat{B}}\limits^\nwarrow    \quad \mat{C}_i^j = \mat{B}_j^i \\
    \mat{C} & ← \mathop{\mat{B}}\limits^\nearrow    \quad \mat{C}_i^j = \mat{B}_{ν+1-j}^{μ+1-i} \\
    \mat{C} & ← \mathop{\mat{B}}\limits^\rightarrow \quad \mat{C}_i^j = \mat{B}_j^{μ+1-i} \\
    \mat{C} & ← \mathop{\mat{B}}\limits^\uparrow    \quad \mat{C}_i^j = \mat{B}_{ν+1-j}^i \\
  \end{split}
\quad\quad
  \begin{split}
    i &= 1, 2, ..., μ(\mat{B})\\
    j &= 1, 2, ..., ν(\mat{B})
  \end{split}
\end{equation*}

\noindent For a vector $\vect{x}$, either $\mathop{\vect{x}}\limits^\rightarrow$ or $\mathop{\vect{x}}\limits^\uparrow$ will denote reversal of the order of the components. For ordinary matrix transposition (that is, $\mathop{\mat{B}}\limits^\nwarrow$), the commonly used notation $\tilde{\mat{B}}$ will also be employed.

\par Since transpositions can effect any one or more of three independent alternatives (i.e., interchange of row and column indices or reversal of order of row or of column indices), repeated transpositions can produce eight distinct configurations. There are therefore seven distinct transformations possible; all can be generated by any pair of transpositions having nonperpendicular axes.%^{<a href="#note1c">[c]</a>}

\section{Special logical matrices}

\par Certain of the special logical vectors introduced in Sec. 1.7 have useful analogs in logical matrices. Dimensions will again be indicated in parentheses (with the column dimension first) and may be elided whenever the dimension is determined by context. If not otherwise specified, a matrix is assumed to be square.

\par Cases of obvious interest are the \textit{full matrix} $\mathsf{\mat{E}}(m \times n)$, defined by $\overbar{\mathsf{\mat{E}}}(m \times n) = 0$, and the \textit{identity matrix} $\mathsf{\mat{I}}(m \times n)$, defined by $\mathsf{\mat{I}}_{j}^{i} = (i = j)$. More generally, \textit{superdiagonal} matrices $^{k}\mathsf{\mat{I}}(m \times n)$ are defined such that $^{k}\mathsf{\mat{I}}_{j}^{i}(m \times n) = (j = i + k)$, for $k \geq 0$. Clearly $^{0}\mathsf{\mat{I}} = \mathsf{\mat{I}}$. Moreover, for square matrices, $^{h}\mathsf{\mat{I}} {+ \atop \times} ^{k}\mathsf{\mat{I}} = ^{(h + k)}\mathsf{\mat{I}}$.

\par Four \textit{triangular} matrices will be defined, the geometrical symbols employed for each indicating the (right-angled isosceles) triangular area of the $m \times n$ rectangular matrix which is occupied by \textit{ones}. Thus

% TODO square symbols; bracket
\begin{equation*}
  \begin{split}
    \mat{C} ← quadnw(m \times n) & \leftrightarrow \mat{C}_j^i \\
    \mat{C} ← quadne(m \times n) & \leftrightarrow \mat{C}_{ν+1-j}^i \\
    \mat{C} ← quadsw(m \times n) & \leftrightarrow \mat{C}_j^{μ+1-i} \\
    \mat{C} ← quadse(m \times n) & \leftrightarrow \mat{C}_{ν+1-j}^{μ+1-i}
  \end{split}
\quad
  \begin{split}
    = (i+j \le \min(m,n))
  \end{split}
\quad
  \begin{split}
    \text{for} i &= 1, 2, ..., m \\
    \text{and} j &= 1, 2, ..., n
  \end{split}
\end{equation*}

\par The use of the matrices $\mathsf{\mat{E}}$ and $\mathsf{\mat{I}}$ will be illustrated briefly. The relation $\vect{u} {\neq \atop \wedge} \vect{v} = 2 |_0 (\vect{u} {+ \atop \times} \vect{v})$ can be extended to logical matrices as follows:

$$
  \mat{U} {\neq \atop \wedge} \mat{V}
    = (2\mathsf{\mat{E}}) |_0 (\mat{U} {+ \atop \times} \mat{V});
$$

\noindent the trace of a square numerical matrix $\mat{X}$ may be expressed as $t = +/\mathsf{\mat{I}}/\mat{X}$. The triangular matrices are employed in the succeeding section.

\section{Polynomials and positional number systems}

\par Any positional representation of a number $n$ in a base $b$ number system can be considered as a numerical vector $\vect{x}$ whose \textit{base $b$ value} is the quantity $n = \vect{w} {+ \atop \times} \vect{x}$, where the \textit{weighting vector} $\vect{w}$ is defined by $\vect{w} = (b^{ν(\vect{x})-1}, b^{ν(\vect{x})-2}, ... b^2, b^1, 1)$. More generally, $\vect{x}$ may represent a number in a mixed-radix system in which the successive radices (from high to low order) are the successive components of a \textit{radix vector} $\vect{y}$.

\par The \textit{base $\vect{y}$ value of $\vect{x}$} is a scalar denoted by $\vect{y} ⊥ \vect{x}$ and defined as the scalar product $\vect{y} ⊥ \vect{x} = \vect{w} {+ \atop \times} \vect{x}$, where $\vect{w} = quadne {\times \atop /} \vect{y}$ is the weighting vector. For example, if $\vect{y} = (7, 24, 60, 60)$ is the radix vector for the common temporal system of units, and if $\vect{x} = (0, 2, 1, 18)$ represents elapsed time in days, hours, minutes, and seconds, then

$$
  t = \vect{w} {+ \atop \times} \vect{x} = (86400, 3600, 60, 1) {+ \atop \times} (0, 2, 1, 18) = 7278
$$

\noindent is the elapsed time in seconds, and the weighting vector $\vect{w}$ is obtained as the product

$$
  quadne {\times \atop /} \vect{y} =
  = \begin{pmatrix}
    0 & 1 & 1 & 1 \\
    0 & 0 & 1 & 1 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0
  \end{pmatrix}
  {\times \atop /}
  \begin{pmatrix} 7 \\ 24 \\ 60 \\ 60 \end{pmatrix} % TODO align left
  = \begin{pmatrix} % TODO align right
    \times/(24, 60, 60) \\
    \times/(60, 60)     \\
    \times/(60)         \\
    \times/\textbf{ϵ}(0)
  \end{pmatrix}
  = \begin{pmatrix} 86400 \\ 3600 \\ 60 \\ 1 \end{pmatrix} % TODO align left
$$

\par If $b$ is any integer, then the value of $\vect{x}$ in the fixed base $b$ is denoted by $(b\textbf{ϵ}) ⊥ \vect{x}$. For example, $(2\textbf{ϵ}) ⊥ \mathbf{α}^2(5) = 24$. More generally, if $y$ is any real number, then $(y\textbf{ϵ}) ⊥ \vect{x}$ is clearly a polynomial in $y$ with coefficients $\vect{x}_1, \vect{x}_2, ... \vect{x}_ν$, that is,

$$
  (y\textbf{ϵ}) ⊥ \vect{x} = \vect{x}_1 y^{ν(\vect{x})-1} +
                           ... + \vect{x}_{ν-1} y + \vect{x}_ν .
$$

\par Writing the definition of $\vect{y} ⊥ \vect{x}$ in the form

$$
  \vect{y} ⊥ \vect{x}
    = (quadne {\times \atop /} \vect{y}) {+ \atop \times} \vect{x}
$$

\noindent exhibits the fact that the operation $⊥$ is of the double operator type. Its use in the generalized matrix product therefore requires no secondary scan operator. This will be indicated by a null placed over the symbol $⊥$. Thus

$$
  \mat{Z} ← \mat{X} {\circ \atop ⊥} \mat{Y} \leftrightarrow
  \mat{Z}_j^i = \mat{X}^i ⊥ \mat{Y}_j.
$$

\par For example, $(y\textbf{ϵ}) {\circ \atop ⊥} \mat{X}$ represents a set of polynomials in $y$ with coefficients $\mat{X}_1, \mat{X}_2, ..., \mat{X}_ν$, and $\mat{Y} {\circ \atop ⊥} \vect{x}$ represents a set of evaluations of the vector $\vect{x}$ in a set of bases $\mat{Y}^1, \mat{Y}^2, ..., \mat{Y}^μ$.

\section{Set operations}

\par In conventional treatments, such as <acronym title="Jacobson, N. (1951), Lectures in Abstract Algebra, vol. 1, Van Nostrand, New York.">Jacobson (1951)</acronym> or <acronym title="Birkhoff, G., and S. MacLane (1941), A Survey of Modern Algebra, Macmillian, New York.">Birkhoff and MacLane (1941)</acronym>, a \textit{set} is defined as an unordered collection of distinct elements. A calculus of sets is then based on such elementary relations as set membership and on such elementary operations as \textit{set intersection} and \textit{set union}, none of which imply or depend on an ordering among members of a set. In the present context it is more fruitful to develop a calculus of \textit{ordered sets}.

\par A vector whose components are all distinct has been called (Sec. 1.10) an \textit{ordered set} and (since no other types are to be considered) will hereafter be called a \textit{set}. In order to provide a closed system, all of the ``set operations'' will, in fact, be defined on vectors. However, the operations will, in the special case of sets, be analogous to classical set operations. The following vectors, the first four of which are sets, will be used for illustration throughout.

\begin{align*}
  & \vect{t} = (\text{t}, \text{e}, \text{a}) \\
  & \vect{a} = (\text{a}, \text{t}, \text{e}) \\
  & \vect{s} = (\text{s}, \text{a}, \text{t}, \text{e}, \text{d}) \\
  & \vect{d} = (\text{d}, \text{u}, \text{s}, \text{k}) \\
  & \vect{n} = (\text{n}, \text{o}, \text{n}, \text{s}, \text{e}, \text{t}) \\
  & \vect{r} = (\text{r}, \text{e}, \text{d}, \text{u}, \text{n}, \text{d}, \text{a}, \text{n}, \text{t}) \\
\end{align*}

\par A variable $z$ is a \textit{member} of a vector $\vect{x}$ if $z = \vect{x}_i$ for some $i$. Membership is denoted by $z ϵ \vect{x}$. A vector $\vect{x}$ \textit{includes} a vector $\vect{y}$ (denoted by either $\vect{x} \supseteq \vect{y}$ or $\vect{y} \subseteq \vect{x})$ if each element $\vect{y}_i$ is a member of $\vect{x}$. If both $\vect{x} \supseteq \vect{y}$ and $\vect{x} \subseteq \vect{y}$, then $\vect{x}$ and $\vect{y}$ are said to be \textit{similar}. Similarity of $\vect{x}$ and $\vect{y}$ is denoted by $\vect{x} ≡ \vect{y}$. For example, $\vect{t} \subseteq \vect{s}, \vect{t} \subseteq \vect{r}, \vect{t} \subseteq \vect{a}, \vect{a} \subseteq \vect{t}, \vect{t} ≡ \vect{a}$, and $\vect{t} ≢ \vect{r}$. If $\vect{x} \subseteq \vect{y}$ and $\vect{x} ≢ \vect{y}$, then $\vect{x}$ is \textit{strictly} included in $\vect{y}$. Strict inclusion is denoted by $\vect{x} ⊂ \vect{y}$.

\par The \textit{characteristic vector} of $\vect{x}$ on $\vect{y}$ is a logical vector denoted by $\textbf{ϵ}_{\vect{y}}^{\vect{x}}$, and defined as follows:

$$
  \vect{u} = \textbf{ϵ}_{\vect{y}}^{\vect{x}}
    \leftrightarrow ν(\vect{u}) = ν(\vect{y}),
  \;\text{and}\;
  \vect{u}_j = (\vect{y}_j ϵ \vect{x}).
$$

\noindent For example, $\textbf{ϵ}_{\vect{s}}^{\vect{t}} = (\text{0}, \text{1}, \text{1}, \text{1}, \text{0})$, $\textbf{ϵ}_{\vect{t}}^{\vect{s}} = (\text{1}, \text{1}, \text{1})$, $\textbf{ϵ}_{\vect{s}}^{\vect{d}} = (\text{1}, \text{0}, \text{0}, \text{0}, \text{1})$, $\textbf{ϵ}_{\vect{d}}^{\vect{s}} = (\text{1}, \text{0}, \text{1}, \text{0})$, and $\textbf{ϵ}_{\vect{n}}^{\vect{r}} = (\text{1}, \text{0}, \text{1}, \text{0}, \text{1}, \text{1})$.

\par The intersection of $\vect{y}$ with $\vect{x}$ is denoted by $\vect{y} ∩ \vect{x}$, and defined as follows:

$$
  \vect{y} ∩ \vect{x} = \textbf{ϵ}_{\vect{y}}^{\vect{x}}/\vect{y}.
$$

% TODO saner set notation
\noindent For example, $\vect{s} ∩ \vect{d} = \text{(s, d)}$, $\vect{d} ∩ \vect{s} = \text{(d, s)}$, $\vect{s} ∩ \vect{r} = \text{(a, t, e, d)}$, and $\vect{r} ∩ \vect{s} = \text{(e, d, d, a, t)}$. Clearly, $\vect{x} ∩ \vect{y} ≡ \vect{y} ∩ \vect{x}$, although $\vect{x} ∩ \vect{y}$ is not, in general, equal to $\vect{y} ∩ \vect{x}$, since the components may occur in a different order and may be repeated a different number of times. The vector $\vect{x} ∩ \vect{y}$ is said to be \textit{ordered} on $\vect{x}$. Thus $\vect{a}$ is ordered on $\vect{s}$. If $\vect{x}$ and $\vect{y}$ contain no common elements (that is, $(\vect{x} ∩ \vect{y}) = \textbf{ϵ}(0))$, they are said to be \textit{disjoint}.

\par The \textit{set difference} of $\vect{y}$ and $\vect{x}$ is denoted by $\vect{y} ∆ \vect{x}$ and is defined as follows:

$$
  \vect{y} ∆ \vect{x} = \overbar{\textbf{ϵ}}_{\vect{y}}^{\vect{x}}/\vect{y}.
$$

\noindent Hence $\vect{y} ∆ \vect{x}$ is obtained from $\vect{y}$ by suppressing those components which belong to $\vect{x}$. For example, $\overbar{\textbf{ϵ}}_{\vect{s}}^{\vect{t}} = \text{(1, 0, 0, 0, 1)}$ and $\vect{s} ∆ \vect{t} = \text{(s, d)}$. Moreover, $\overbar{\textbf{ϵ}}_{\vect{t}}^{\vect{s}} = \text{(0, 0, 0)} and \vect{t} ∆ \vect{s} = \textbf{ϵ}(0)$.

\par The \textit{union} of $\vect{y}$ and $\vect{x}$ is denoted by $\vect{y} ∪ \vect{x}$ and defined as follows:%^{<a href="#note1d">[d]</a>}
$\vect{y} ∪ \vect{x} = \vect{y} \oplus (\vect{x} ∆ \vect{y})$. For example, $\vect{s} ∪ \vect{d} = \text{(s, a, t, e, d, u, k)}$, $\vect{d} ∪ \vect{s} = \text{(d, u, s, k, a, t, e)}$, $\vect{s} ∪ \vect{a} = \vect{s} ∪ \vect{t} = \vect{s}$, and $\vect{n} ∪ \vect{t} = \text{(n, o, n, s, e, t, a)}$. In general, $\vect{x} ∪ \vect{y} ≡ \vect{y} ∪ \vect{x}$, and $\vect{x} ≡ (\vect{x} ∩ \vect{y}) ∪ (\vect{x} ∆ \vect{y})$. If $\vect{x}$ and $\vect{y}$ are disjoint, their union is equivalent to their catenation, that is, $\vect{x} ∩ \vect{y} = \textbf{ϵ}(0)$ implies that $\vect{x} ∪ \vect{y} = \vect{x} \oplus \vect{y}$.

\par In the foregoing development, the concepts of inclusion and similarity are equivalent to the concepts of inclusion and equality in the conventional treatment of (unordered) sets. The remaining definitions of intersection, difference, and union differ from the usual formulation in that the result of any of these operations on a pair of ordered sets is again an \textit{ordered} set. With respect to \textit{similarity}, these operations satisfy the same identities as do the analogous conventional set operations on unordered sets with respect to equality.

\par The forward selection $σ/\vect{b}$ and the backward selection $τ/\vect{b}$ defined in Sec. 1.10 can both be used to reduce any vector $\vect{b}$ to a similar set, that is,

$$
  (σ/\vect{b})/\vect{b} ≡ (τ/\vect{b})/\vect{b} ≡ \vect{b}.
$$

\noindent Moreover, if $\vect{f} = (σ/\vect{x})/\vect{b}, \vect{g} = (σ/\vect{y})/\vect{y}$, and $\vect{h} = (σ/\vect{z})/\vect{z}$, then $\vect{x} = \vect{y} ∩ \vect{z}$ implies that $\vect{f} = \vect{g} ∩ \vect{h}$, and $\vect{x} = \vect{y} ∪ \vect{z}$ implies that $\vect{f} = \vect{g} ∪ \vect{h}$.

\par The unit vector $\textbf{ϵ}^j(n)$ will be recognized as a special case of the characteristic vector $\textbf{ϵ}_{\vect{y}}^{\vect{x}}$ in which $\vect{x}$ consists of the single component $j$, and $\vect{y} = \textbf{ι}^h(n)$, where $h$ is the index origin in use. In fact, the notation $\textbf{ϵ}^j_{i^h}$ can be used to make explicit the index origin $h$ assumed for $\textbf{ϵ}^j$.

\par If $\vect{z}$ is any vector of dimension two such that $\vect{z}_1 ϵ \vect{x}$ and $\vect{z}_2 ϵ \vect{y}$, then $\vect{z}$ is said to belong to the \textit{Cartesian product} of $\vect{x}$ and $\vect{y}$. Thus if $\vect{x} = \text{(a, b, c)}$ and $\vect{y} = \text{(0, 1)}$, the rows of the matrix

$$
  \mat{A} = \begin{pmatrix}
    a & 0 \\
    a & 1 \\
    b & 0 \\
    b & 1 \\
    c & 0 \\
    c & 1
  \end{pmatrix}
$$

\noindent are a complete list of the vectors $\vect{z}$ belonging to the product set of $\vect{x}$ and $\vect{y}$. The matrix $\mat{A}$ will be called the Cartesian product of $\vect{x}$ and $\vect{y}$ and will be denoted $\vect{x} \otimes \vect{y}$.

\par The foregoing definition by example will be formalized in a more general way that admits the Cartesian product of several vectors (that is, $\vect{u} \otimes \vect{v} \otimes ... \otimes \vect{y}$) which need not be sets, and which specifies a unique ordering of the rows of the resulting matrix. Consider a family of vectors $\vect{x}^1, \vect{x}^2, ..., \vect{x}^s$ of dimensions $\vect{d}_1, \vect{d}_2, ..., \vect{d}_s$. Then

$$
  \mat{A}
    ← \vect{x}^1 \otimes \vect{x}^2 \otimes ... \otimes \vect{x}^s
  \leftrightarrow \mat{A}^{1 + \vect{d} ⊥ (\vect{k} - \textbf{ϵ})}
    = (\vect{x}^1_{\vect{k}_1}, \vect{x}^2_{\vect{k}_2}, ..., \vect{x}^s_{\vect{k}_s}),
$$

\noindent for all vectors $\vect{k}$ such that $1 \leq \vect{k}_i \leq \vect{d}_i$. Clearly, $ν(\mat{A}) = s$, and $μ(\mat{A}) = \times/\vect{d}$. As illustrated by Table 1.11, the rows of the Cartesian product $\mat{A}$ are not distinct if any one of the vectors $\vect{x}^i$ is not a set.

% TODO figure positioning
\begin{equation*}
  \begin{split}
    \vect{x}^1 & = (a, b, a) \\
    \vect{x}^2 & = (\#, *)   \\
    \vect{x}^3 & = (0, 1)    \\
    \vect{d}   & = (3, 2, 2)
  \end{split}
\quad\quad
  \begin{split}
    \mat{A} = \begin{pmatrix}
      a & \# & 0 \\
      a & \# & 1 \\
      a &  * & 0 \\
      a &  * & 1 \\
      b & \# & 0 \\
      b & \# & 1 \\
      b &  * & 0 \\
      b &  * & 1 \\
      a & \# & 0 \\
      a & \# & 1 \\
      a &  * & 0 \\
      a &  * & 1 \\
    \end{pmatrix}
  \end{split}
\end{equation*}

\par \textbf{Table 1.11} The Cartesian product $\mat{A} = \vect{x}^1 \otimes \vect{x}^2 \otimes \vect{x}^3$

\par If the vectors $\vect{x}^i$ are all of the the same dimension, they may be considered as the columns of a matrix $\mat{X}$, that is, $\mat{X}_i = \vect{x}^i$. The product $\vect{x}^1 \otimes \vect{x}^2 \otimes ... \otimes \vect{x}^s = \mat{X}_1 \otimes \mat{X}_2 \otimes ... \otimes \mat{X}_s$ may then be defined by $\otimes/\mat{X}$, or alternatively by $\otimes/\!/\mat{Y}$, where $\mat{Y}$ is the transpose of $\mat{X}$. For example, if

$$
  \mat{X} = \textbf{ι}^0(2) {\circ \atop \wedge} \textbf{ϵ}(3)
          = \begin{pmatrix} 0 & 0 & 0 \\ 1 & 1 & 1 \end{pmatrix}
$$

\par then $\otimes/\mat{X}$ is the matrix of arguments of the truth table for three variables.

\section{Ranking}

\par The \textit{rank} or \textit{index} of an element $c ϵ \vect{b}$ is called the $\vect{b}$ \textit{index of c} and is defined as the smallest value of $i$ such that $c = \vect{b}_i$. To establish a closed system, the $\vect{b}$ index of any element $\vect{a} \notin \vect{b}$ will be defined as the null characer $∘$. The $\vect{b}$ index of any element $c$ will be denoted by $\vect{b} ι c$; if necessary, the index origin in use will be indicated by a subscript appended to the operator $ι$. Thus, if $\vect{b} = \text{(a, p, e)}$, $\vect{b} ι_0 p = 1$, and $\vect{b} ι_1 p = 2$.

\par The $\vect{b}$ index of a vector $\vect{c}$ is defined as follows:

$$
  \vect{k} ← \vect{b} ι \vect{c}
  \leftrightarrow
  \vect{k}_i = \vect{b} ι \vect{c}_i.
$$

\noindent The extension to matrices may be either row by row or (as indicated by a doubled operator symbol $ιι$) column by column, as follows:

\begin{align*}
  \mat{J} ← \mat{B}  ι \mat{C} & \leftrightarrow \mat{J}^i ← \mat{B}^i ι \mat{C}^i, \\
  \mat{K} ← \mat{B} ιι \mat{C} & \leftrightarrow \mat{K}_j ← \mat{B}_j ι \mat{C}_j.
\end{align*}

\par Use of the ranking operator in a matrix product requires no secondary scan and is therefore indicated by a superior null symbol. Moreover, since the result must be limited to a two-dimensional array (matrix), either the pre- or post-operand is required to be a vector. Hence

\begin{align*}
  \mat{J} ← \mat{B}  {\circ \atop \iota} \vect{c} & \leftrightarrow \mat{J}^i ← \mat{B}^i ι \vect{c}^i, \\
  \mat{K} ← \vect{b} {\circ \atop \iota} \mat{C}  & \leftrightarrow \mat{K}_j ← \vect{b}_j ι \mat{C}_j.
\end{align*}

\par The first of these ranks the components of $\vect{c}$ with respect to each of a set of vectors $\mat{B}^1, \mat{B}^2, ..., \mat{B}^μ$, whereas the second ranks each of the vectors $\mat{C}_1, \mat{C}_2, ..., \mat{C}_ν$ with respect to the fixed vector $\vect{b}$.

\par The use of the ranking operation can be illustrated as follows. Consider the vector $\vect{b} = \text{(a, b, c, d, e)}$ and the set of all $3^5$ three-letter sequences (vectors) formed from its components. If the set is ordered lexically, and if $\vect{x}$ is the $j$th member of the set (counting from zero), then

$$
  j = (ν(\vect{b})\textbf{ϵ}) ⊥ (\vect{b} ι_0 \vect{x}).
$$

\par For example, if $\vect{x} = \text{(c, a, b)}$, then $(\vect{b} ι_0 \vect{x}) = (2, 0, 1)$, and $j = 51$.

\section{Mapping and permutation}

\subsection*{Reordering operations}

\par The selection operations employed thus far do not permit convenient reorderings of the components. This is provided by the \textit{mapping} operation defined as follows:%^{<a href="#note1e">[e]</a>}

$$
  \vect{c} ← \vect{a}_{\vect{k}} \leftrightarrow
  \vect{c}_i = \vect{a}_{\vect{k}_i}
$$

\noindent For example, if $\vect{a} = \text{(a, b, ..., z)}$ and $\vect{k} = (6, 5, 4)$, then $\vect{c} = \text{(f, e, d)}$.

\par The foregoing definition is meaningful only if the components of $\vect{k}$ each lie in the range of the indices of $\vect{a}$, and it will be extended by defining $\vect{a}_j$ as the null element $∘$ if $j$ does not belong to the index set of $\vect{a}$. Formally,

\begin{equation*}
  \vect{c} ← \vect{a}_{\vect{m}} \leftrightarrow \vect{c}_i =
  \begin{cases}
    \vect{a}_{\vect{m}_i} & \text{if } \vect{m}_i ϵ \textbf{ι}^1(ν(\vect{a})) \\
    ∘                & \text{if } \vect{m}_i \notin \textbf{ι}^1(ν(\vect{a})).
  \end{cases}
\end{equation*}

\par The ability to specify an arbitrary index origin for the vector $\vect{a}$ being mapped is provided by the following alternative notation for mapping:

\begin{equation*}
  \vect{c} ← \vect{m} \int_j \vect{a} \leftrightarrow \vect{c}_i =
  \begin{cases}
    \vect{a}_{\vect{m}_i} & \text{if } \vect{m}_i ϵ \textbf{ι}^j(ν(\vect{a})) \\
    ∘                & \text{if } \vect{m}_i \notin \textbf{ι}^j(ν(\vect{a})),
  \end{cases}
\end{equation*}

\noindent where $j$-origin indexing is assumed for the vector $\vect{a}$. For example, if $\vect{a}$ is the alphabet and $\vect{m} = (5, ∘, ∘, 4, 27, ∘, 3)$, then $\vect{c} = \vect{m} \int_0 \vect{a} = (f, ∘, ∘, \text{e}, ∘, ∘, \text{d})$, and $(\vect{c} \neq ∘\textbf{ϵ})/\vect{c} = \text{(f, e, d)}$. Moreover, $\vect{m} \int_2 \vect{a} = (\text{d}, ∘, ∘, \text{c}, \text{z}, ∘, \text{b})$. Elision of $j$ is permitted.

\par If $\vect{a} ⊆ \vect{b}$, and $\vect{m} = \vect{b} ι_j \vect{a}$, then clearly $\vect{m} \int_j \vect{b} = \vect{a}$. If $\vect{a} \nsubseteq \vect{b}$, then $\vect{m} \int_j \vect{b}$ contains (in addition to certain nulls) those components common to $\vect{b}$ and $\vect{a}$, arranged in the order in which they occur in $\vect{a}$. In other words,

$$
  (\vect{m} \neq ∘\textbf{ϵ})/(\vect{m} \int_j \vect{b}) = \vect{a} ∩ \vect{b}.
$$

\noindent Consequently, if $\vect{p}$, $\vect{q}$, ..., $\vect{t}$ are vectors, each contained in $\vect{b}$, then each can be represented jointly by the vector $\vect{b}$ and a mapping vector. If, for example, $\vect{b}$ is a glossary and $\vect{p}$, $\vect{q}$, etc. are texts, the total storage required for $\vect{b}$ and the mapping vectors might be considerably less than for the entire set of texts.

\par Mapping may be shown to be associative, that is, $\vect{m}^1 \int_i (\vect{m}^2 \int_j \vect{a}) = (\vect{m}^1 \int_i \vect{m}^2) \int_j \vect{a}$. Mapping is not, in general, commutative.

\par Mapping is extended to matrices as follows:

\begin{align*}
  \mat{A} ← \mat{M}     \int_h \mat{B} & \leftrightarrow \mat{A}^i = \mat{M}^i \int_h \mat{B}^i, \\
  \mat{C} ← \mat{M} \int\int_h \mat{B} & \leftrightarrow \mat{C}_j = \mat{M}_j \int_h \mat{B}_j.
\end{align*}

\noindent Row and column mappings are associative. A row mapping $^1\mat{M}$ and a column mapping $^2\mat{M}$ do not, in general, commute, but do if all rows of $^1\mat{M}$ agree (that is, $^1\mat{M} = \textbf{ϵ} {\circ \atop \times} \vect{p}$), and if all columns of $^2\mat{M}$ agree (that is, $^2\mat{M} = \vect{q} {\circ \atop \times} \textbf{ϵ}$). The generalized matrix product is defined for the cases $\mat{M} {\circ \atop \int} \mat{A}$, and $\mat{M} {\circ \atop \int} \vect{a}$.

\par The alternative notation (that is, $\vect{c} = \vect{a}_{\vect{m}}$), which does not incorporate specification of the index origin, is particularly convenient for matrices and is extended as follows:

\begin{align*}
  \mat{A} ← \mat{B}^{\vect{m}} & \leftrightarrow \mat{A}^i = \mat{B}^{\vect{m}_i}, \\
  \mat{A} ← \mat{B}_{\vect{m}} & \leftrightarrow \mat{A}_i = \mat{B}_{\vect{m}_i}.
\end{align*}

\subsection*{Permutations}

\par A vector $\vect{k}$ of dimension $n$ is called a $j$-origin \textit{permutation vector} if $\vect{k} ≡ \textbf{ι}^j(n)$. A permutation vector used to map any set of the same dimension produces a reordering of the set without either repetition or suppression of elements, that is, $\vect{k} \int_j \vect{a} ≡ \vect{a}$ for any set $\vect{a}$ of dimension $ν(\vect{k})$. For example, if $\vect{a} = (\text{f}, 4, *, 6, \text{z})$, and $\vect{k} = (4, 2, 5, 1, 3)$, then $\vect{k} \int_1 \vect{a} = (6, 4, \text{z}, \text{f}, *)$.

\par If $\vect{p}$ is an $h$-origin permutation vector and $\vect{q}$ is any $j$-origin permutation vector of the same dimension, then $\vect{q} \int_j \vect{p}$ is an $h$-origin permutation vector.

\par Since

$$
  \textbf{ι}^j(ν(\vect{a})) \int_j \vect{a} = \vect{a},
$$

\par the interval vector $\textbf{ι}^j(n)$ will also be called the \textit{j-origin identity permutation vector}. If $\vect{p}$ and $\vect{q}$ are two $j$-origin permutation vectors of the same dimension $n$ and if $\vect{q} \int_j \vect{p} = \textbf{ι}^j(n)$, then $\vect{p} \int_j \vect{q} = \textbf{ι}^j(n)$ also and $\vect{p}$ and $\vect{q}$ are said to be \textit{inverse} permutations. If $\vect{p}$ is any $j$-origin permutation vector, then $\vect{q} = \vect{p} ι_j \textbf{ι}^j$ is inverse to $\vect{p}$.

\par The rotation operation $k ↑ \vect{x}$ is a special case of permutation.

\subsection*{Function mapping}

\par A function $f$ which defines for each element $\vect{b}_i$ of a set $\vect{b}$ a unique correspondent $\vect{a}_k$ in a set $\vect{a}$ is called a \textit{mapping from} $\vect{b}$ to $\vect{a}$. If $f(\vect{b}_i) = \vect{a}_k$, the element $\vect{b}_i$ is said to \textit{map into} the element $\vect{a}_k$. If the elements $f(\vect{b}_i)$ exhaust the set $\vect{a}$, the function $f$ is set to map $\vect{b}$ onto $\vect{a}$. If $\vect{b}$ maps onto $\vect{a}$ and the elements $f(\vect{b}_i)$ are all distinct, the mapping is said to be one-to-one or \textit{biunique}. In this case, $ν(\vect{a}) = ν(\vect{b})$, and there exists an inverse mapping from $\vect{a}$ to $\vect{b}$ with the same correspondences.

\par A program for performing the mapping $f$ from $\vect{b}$ to $\vect{a}$ must therefore determine for any given element $b ϵ \vect{b}$, the correspondent $a ϵ \vect{a}$, such that $a = f(b)$. Because of the convenience of operating upon integers (e.g., upon register addresses or other numeric symbols) in the automatic execution of programs, the mapping is frequently performed in three successive phases, determining in turn the following quantities:

\begin{enumerate}
  \item the index $i = \vect{b} ι b$,
  \item the index $k$ such that $\vect{a}_{k} = f(\vect{b}_{i})$,
  \item the element $\vect{a}_{k}$.
\end{enumerate}

\par The three phases are shown in detail in Program 1.12a. The ranking is performed (steps 1-3) by scanning the set $\vect{b}$ in order and comparing each element with the argument $b$. The second phase is a permutation of the integers $1$, $2$, ..., $ν(\vect{b})$, which may be described by a permutation vector $\vect{j}$, such that $\vect{j}_i = k$. The selection of $\vect{j}_i$ (step 4) then defines $k$, which, in turn, determines the selection of $\vect{a}_k$ on step 5.

\par \textbf{Example 1.2}. If

\begin{align*}
  \vect{b} &= (\text{apple}, \text{booty}, \text{dust}, \text{eye}, \text{night}), \\
  \vect{a} &= (\text{Apfel}, \text{Auge}, \text{Beute}, \text{Nacht}, \text{Staub})
\end{align*}

\noindent are, respectively, a set of English words and a set of German correspondents (both in alphabetical order), and if the function required is the mapping of a given English word $b$ into its German equivalent $a$ according to the dictionary correspondences:

\begin{tabular}{llllll}
  English: & apple & booty & dust & eye & night \\
  German:  & Apfel & Beute & Staub & Auge & Nacht
\end{tabular}

\par then $\vect{j} = (1, 3, 5, 2, 4)$. If $b = \text{``night''}$, then $i = 5$, $\vect{j}_i = 4$, and $a = \vect{a}_4 = \text{Nacht}$.

% (END EXAMPLE 1.2)

\par If $\vect{k}$ is a permutation vector inverse to $\vect{j}$, then Program 1.12b describes a mapping inverse to that of Program 1.12a. If $\vect{j} = (1, 3, 5, 2, 4)$, then $\vect{k} = (1, 4, 2, 5, 3)$. The inverse mapping can also be described in terms of $\vect{j}$, as is done in Program 1.12c. The selection of the $i$th component of the permutation vector is then necessarily replaced by a scan of its components. Programs 1.12d and 1.12e show alternative formulations of Program 1.12a.

\par (TODO: PROGRAM 1.12)

\par \textbf{Program 1.12} Mapping defined by a permutation vector $j$

\subsection*{Ordering vector}

\par If $\vect{x}$ is a numeric vector and $\vect{k}$ is a $j$-origin permutation vector such that the components of $\vect{y} = \vect{k} \int_j \vect{x}$ are in ascending order, then $\vect{k}$ is said to \textit{order} $\vect{x}$. The vector $\vect{k}$ can be determined by an ordering operation defined as follows:

$$
  \vect{k} ← θ_j/\vect{x}
$$

\noindent implies that $\vect{k}$ is a $j$-origin permutation vector, and that if $\vect{y} = \vect{k} \int_j \vect{x}$, then either $\vect{y}_i < \vect{y}_{i+1}$ or $\vect{y}_i = \vect{y}_{i+1}$ and $\vect{k}_i < \vect{k}_{i+1}$. The resulting vector $\vect{k}$ is unique and preserves the original relative order among equal components. For example, if $\vect{x} = (7, 3, 5, 3)$, then $θ_1/\vect{x} = (2, 4, 3, 1)$.

\par The ordering operation is extended to arbitrary vectors by treating all nonnumeric quantities as equal and as greater than any numeric quantity. For example, if $\vect{a} = (7, ∘, 3,∘, 5, 3)$, then $θ_1/\vect{a} = (3, 6, 5, 1, 2, 4)$, and if $\vect{b}$ is any vector with no numerical components, then $θ_j/\vect{b} = \textbf{ι}^j(ν(\vect{b}))$.

\par Ordering of a vector $\vect{a}$ with respect to a vector $\vect{b}$ is achieved by ordering the $\vect{b}$-index of $\vect{a}$. For example, if $\vect{a} = \text{(e, a, s, t, 4, 7, t, h)}$, and $\vect{b}$ is the alphabet, then $\vect{m} = \vect{b} ι_1 \vect{a} = (5, 1, 19, 20, ∘, ∘, 20, 8)$ and $θ_1/\vect{m} = (2, 1, 8, 3, 4, 7, 5, 6)$.

\par The ordering operation is extended to matrices by the usual convention. If $\mat{K} = θ_j/\!/\mat{A}$, then each column of the matrix $\mat{B} = \mat{K} \int\int_j \mat{A}$ is in ascending order.

\section{Maximization}

\par In determining the maximum $m$ over components of a numerical vector $x$, it is often necessary to determine the indices of the maximum components as well. The maximization operator is therefore defined so as to determine a logical vector $\vect{v}$ such that $\vect{v}/\vect{x} = m\textbf{ϵ}$.

\par Maximization over the entire vector $\vect{x}$ is denoted by $\textbf{ϵ}⌈\vect{x}$, and is defined as follows: if $\vect{v} = \textbf{ϵ}⌈\vect{x}$, then there exists a quantity $m$ such that $\vect{v}/\vect{x} = m\textbf{ϵ}$ and such that all components of $\overbar{\vect{v}}/\vect{x}$ are strictly less than $m$. The maximization is assumed by a single component of $\vect{x}$ if and only if $+/\vect{v} = 1$. The actual value of the maximum is given by the first (or any) component of $\vect{v}/\vect{x}$. Moreover, the $j$-origin indices of the maximum components are the components of the vector $\vect{v}/\textbf{ι}^j$.

\par More generally, the maximization operation $\vect{v} ← \vect{u}⌈\vect{x}$ will be defined so as to determine the maximum over the subvector $\vect{u}/\vect{x}$ only, but to express the result $\vect{v}$ with respect to the entire vector $\vect{x}$. More precisely,

$$
  \vect{v} ← \vect{u}⌈\vect{x} \leftrightarrow
  \vect{v} = \vect{u}\backslash(\textbf{ϵ}⌈(\vect{u}/\vect{x})).
$$

\noindent The operation may be visualized as follows---a horizontal plane punched at points corresponding to the zeros of $\vect{u}$ is lowered over a plot of the components of $\vect{x}$, and the positions at which the plane first touches them are the positions of the unit components of $\vect{v}$. For example, maximization over the negative components of $\vect{x}$ is denoted by

$$
  \vect{v} ← (\vect{x} < 0)⌈\vect{x}
$$

\noindent and if $\vect{x} =$ (2, -3, 7, -5, 4, -3, 6), then $(\vect{x} <$ 0) $=$ (0, 1, 0, 1, 0, 1, 0), $\vect{v} =$ (0, 1, 0, 0, 0, 1, 0), $\vect{v}/\vect{x} =$ (-3, -3), $(\vect{v}/\vect{x})_1 =$ -3, and $\vect{v}/\textbf{ι}^1 =$ (2, 6). Minimization is defined analogously and is denoted by $\vect{u}⌊\vect{x}$.

\par The extension of maximization and minimization to arbitrary vectors is the same as for the ordering operation, i.e., all nonnumeric quantities are treated as equal and as exceeding all numeric quantities. The extensions to matrices are denoted and defined as follows:

\begin{align*}
  \mat{V} ← \mat{U} ⌈ \mat{X}                & \leftrightarrow \mat{V}^i = \mat{U}^i ⌈ \mat{X}^i, \\
  \mat{V} ← \mat{U} ⌈⌈ \mat{X}               & \leftrightarrow \mat{V}_j = \mat{U}_j ⌈ \mat{X}_j, \\
  \mat{V} ← \mat{U} {\circ \atop ⌈} \vect{x} & \leftrightarrow \mat{V}^i = \mat{U}^i ⌈ \vect{x},  \\
  \mat{V} ← \vect{u} {\circ \atop ⌈} \mat{X} & \leftrightarrow \mat{V}_j = \vect{u}  ⌈ \mat{X}_j.
\end{align*}

\par As in the case of the ordering operation, maximization in a vector $\vect{a}$ with respect to order in a set $\vect{b}$ is achieved by maximizing over the $\vect{b}$-index of $\vect{a}$. Thus if

$$
  \setcounter{MaxMatrixCols}{13}
  \mat{H} = \begin{pmatrix}
    \text{d} & \text{c} & \text{h} & \text{d} & \text{h} & \text{s} & \text{h} & \text{d} & \text{c} & \text{h} & \text{c} & \text{h} & \text{d} \\
    \text{a} & 6 & \text{k} & \text{q} & 4 & 3 & 5 & \text{k} & 8 & 2 & \text{j} & 9 & 2
  \end{pmatrix}
$$

\noindent represents a hand of thirteen playing cards, and if

\begin{align*}
  \mat{B} &= \begin{pmatrix}
    \text{c} & \text{d} & \text{h} & \text{s} & ∘ & ∘ & ∘ & ∘ & ∘ & ∘ & ∘ & ∘ & ∘ \\
    2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & \text{j} & \text{q} & \text{k} & \text{a}
  \end{pmatrix}, \\
\text{then}\quad \mat{B} ι_0 \mat{H} &= \begin{pmatrix}
    1 & 0 & 2 & 1 & 2 & 3 & 2 & 1 & 0 & 2 & 0 & 2 & 1 \\
    12 & 4 & 11 & 10 & 2 & 1 & 3 & 11 & 6 & 0 & 9 & 7 & 0
  \end{pmatrix}, \\
  (4, 13) {\circ \atop ⊥} (\mat{B} ι_0 \mat{H}) &= (25, 4, 37, 23, 28, 40, 29, 24, 6, 26, 9, 33, 13), \\
\text{and}\quad &
  (\textbf{ϵ}⌈((4, 13) {\circ \atop ⊥} (\mat{B} ι_0 \mat{H})))/\mat{H} = (s, 3)
\end{align*}

\par is the highest ranking card in the hand.

\section{Inverse functions}

\par To every biunique%^{<a href="#note1f">[f]</a>}
function $f$ there corresponds an \textit{inverse} function $g$ such that $g(f(x)) = x$ for each argument $x$ in the domain of the function $f$. It is common practice either to introduce a distinct symbolism for the inverse function, as for the inverse functions of logarithm ($\log_b x$) and exponentiation ($b^3$), or to use a superscript $-1$, as in $\sin^{-1}x$ or $f^{-1}(x)$.

\par The first alternative doubles the number of distinct operator symbols required and obscures the relation between pairs of inverse functions; the second raises other difficulties. The solution adopted here is that of \textit{implicit} specification; i.e. a statement is permitted to specify not only a variable but also any function of that variable. Functions may therefore appear on both sides of the specification arrow in a statement. For example,

$$
  (2\textbf{ϵ}) ⊥ x ← z
$$

\noindent specifies the variable $x$ as the vector whose base two value is the number $z$.

\par Certain ambiguities remain in the foregoing statement. First, the dimension of $x$ is not specified. For example, if $z = 12$, $x = (1, 1, 0, 0)$ is an admissible solution, but so are $(0, 1, 1, 0, 0)$ and $(0, 0, 0, 1, 1, 0, 0)$. This could be clarified by compatibility with a specified dimension of $\textbf{ϵ}$. Thus the statement

$$
  (2\textbf{ϵ}(5)) ⊥ x ← z
$$

\noindent specifies $x$ unambiguously as $(0, 1, 1, 0, 0)$. More generally, however, any previously specified auxiliary variables will be listed to the right of the main statement, with a semicolon serving as a separation symbol. The current example could therefore be written as

\begin{align*}
  ν(x) ← 5 \\
  (2\textbf{ϵ}) ⊥ x ← z; ν(x)
\end{align*}

\par The second ambiguity concerns the permissible range of the individual components of $x$. For example, the base two value of $x = (5, 2)$ is also twelve. For certain functions it is therefore necessary to adopt some obvious conventions concerning the range of the result. The assumption implicit in the preceding paragraph is that each component of $x$ is limited to the range of the residues modulo the corresponding radix. This convention will be adopted. Hence the pair of statements

\begin{align*}
  y ← (7, 24, 60, 60) \\
  y ⊥ x ← 7278; y
\end{align*}

\noindent determines $x$ unambiguously as the vector $(0, 2, 1, 18)$.

\par it is also convenient, though not essential, to use selection operations on the left of a statement. Thus the statement

$$
  \vect{u}/\vect{b} ← \vect{a}
$$

\noindent is understood to respecify only the selected components of $\vect{b}$ and to leave all others unchanged. It is therefore equivalent to the statement

$$
  \vect{b} ← \backslash\overbar{\vect{u}}/\vect{b}, \vect{u}, \vect{a}\backslash.
$$

\noindent Similarly,

$$
  \vect{u}/\vect{b} ← \vect{u}/\vect{a}
$$

\noindent is equivalent to

$$
  \vect{b} ← /\vect{b}, \vect{u}, \vect{a}/.
$$

\section{Levels of structure}

\par Vectors and matrices are arrays which exhibit one level and two levels of structure, respectively. Although in certain fields, such as tensor analysis, it is convenient to define more general arrays whose \textit{rank} specifies the number of levels of structure (i.e., zero for a scalar, one for a vector of scalars, two for a vector of vectors (matrix), three for a vector of matrices, etc.), the notation will here be limited to the two levels provided by the matrix.%^{<a href="#note1g">[g]</a>}
The present section will, however, indicate methods for removing this limitation.

% TODO doubled \perp
\par The only essential particularization to two levels occurs in the provision of single and double symbols (e.g. ``$/$'' and ``$/\!/$'', ``$⊥$'' and ``$⊥⊥$'') for row and column operations, respectively, and in the use of superscripts and subscripts for denoting rows and columns, respectively. In applications requiring multiple levels, the former can be generalized by adjoining to the single symbol an index which specifies the coordinate (e.g. ``$/_1$'' and ``$/_2$'', for row and for column compression, and, in general, ``$/_j$''.) The latter can be generalized by using a vector index subscript possessing one component index for each coordinate.

\par The generalized notation can be made compatible with the present notation for vectors and matrices by adopting the name \textit{tensor} and a symbol class (such as capital letters) for the general array of arbitrary rank.

\section{Subroutines}

\par Detail can be subordinated in a more general manner by the use of subroutines. The name of one program appearing as a single statement in a second program implies execution of the named program at that point; the named program is called a \textit{subroutine} of the second program. If, for example, ``Cos'' is the name of a program which specifies $z$ as the cosine of the angle between the vectors $\vect{x}$ and $\vect{y}$, then Program 1.13a uses the program ``Cos'' as a subroutine to determine $r$ as the cosine of the angle between the vectors $\vect{p}$ and $\vect{q}$.

\par (TODO: PROGRAM 1.13)

\par \textbf{Program 1.13} Modes of subroutine reference

\par It is sometimes convenient to include the names of the arguments or results or both in the name of the subroutine as dummy variables. Thus if ``$\text{Cos}(\vect{x}, \vect{y})$'' is the name of a subroutine which determines $z$ as the cosine of the angle between $\vect{x}$ and $\vect{y}$, then Program 1.13b uses $\text{Cos}(x, y)$ as a subroutine to determine $r$ as the cosine of the angle between $\vect{p}$ and $\vect{q}$. Similarly, the program ``$z ← \text{Cos}(x, y)$'' can be used as in Program 1.13c to produce the same result.

\section{Files}

\par Many devices used for the storage of information impose certain restrictions upon its insertion or withdrawal. The items recorded on a magnetic tape, for example, may be read from the tape much more quickly in the order in which they appear physically on the tape than in some other prescribed order.

\par Certain storage devices are also self-indexing in the sense that the item selected in the next read from the device will be determined by the current state or position of the device. The next item read from a magnetic tape, for example, is determined by the position in which the tape was left by the last preceding read operation.

\par To allow the convenient description of algorithms constrained by the characteristics of storage devices, the following notation will be adopted. A \textit{file} is a representation of a vector $\vect{x}$ arranged as follows:

$$
  \vect{p}_1, \vect{x}_1, \vect{p}_2, \vect{x}_2, ...,
  \vect{x}_{ν(\vect{x})},
  \vect{p}_{ν(\vect{x}) + 1}, ∘,
  \vect{p}_{ν(\vect{x}) + 2}, ∘, ...,
  \vect{p}_{ν(\vect{p})}.
$$

\noindent The null elements denote ``unused'' portion of the file not employed in representing $\vect{x}$. Each \textit{partition} $\vect{p}_j$ determines a \textit{position} (position $j$) in the file. If a file $\tree{Φ}$ is in position $j$, then a \textit{forward read}, denoted by

$$
  x, p ← {}_0\tree{Φ},
$$

\noindent specifies $x$ by the component $\vect{x}_j$, the auxiliary variable $p$ by the succeeding partition $\vect{p}_{j+1}$, and stops the file in the position $j + 1$.

\par The position of a file $\tree{Φ}$ will be denoted by $π(\tree{Φ})$. Thus the statement $j ← π(\tree{Φ})$ specifies $j$ as the position of $\tree{Φ}$, whereas $π(\tree{Φ}) ← j$ \textit{positions} the file to $j$. In particular, $π(\tree{Φ}) ← 1$ denotes the \textit{rewinding} of the file and $π(\tree{Φ}) ← ν$ denotes \textit{winding}, i.e. positioning to the extreme end of the file. Any file for which the general positioning operation $π(\tree{Φ}) ← j$ is to be avoided as impossible or inefficient is called a \textit{serial} or \textit{serial-access} file.

\par Each terminal partition (that is, $\vect{p}_1$ and $\vect{p}_{ν(\vect{p})}$) assumes a single fixed value denoted by $λ$. Each nonterminal partition $\vect{p}_j$ may assume one of several values denoted by $\mathbf{λ}_1$, $\mathbf{λ}_2$, ..., $\mathbf{λ}_{ν(\mathbf{λ})}$, the partitions with larger indices normally demarking larger subgroups of components within the file. Thus if $\vect{x}$ were the row list of a matrix, the last component might be followed by the partition $\mathbf{λ}_3$, the last component of each of the preceding rows by $\mathbf{λ}_2$, and the remaining components by $\mathbf{λ}_1$. The auxiliary variable $p$ specified by the partition symbol during the read of a file is normally used to control a subsequent branch.

\par A file may be produced by a sequence of \textit{forward record} statements:

$$
  {}_0\tree{Φ} ← \vect{x}_i, p \text{ for } i ϵ \textbf{ι}^1(ν(\vect{x})),
$$

\noindent where $p$ is the partition symbol recorded after the component $\vect{x}_i$. As in reading, each forward record operation increments the position of the file by one. A file which is only recorded during a process is called an \textit{output file} of the process; a file which is only read is called an \textit{input file}.

\par Different files occurring in a process will be distinguished by righthand subscripts and superscripts, the latter being usually employed to denote major classes of files, such as input and output.

\par \textbf{Example 1.3}. A set of $m$ input files $\tree{Φ}_i^1$, $i ϵ \textbf{ι}^1(m)$, each terminated by a partition $λ_2$, is to be copied to a single output file $\tree{Φ}_1^2$ as follows. Successive items (components) are chosen in turn from files $\tree{Φ}_1^1$, $\tree{Φ}_2^1$, ..., $\tree{Φ}_m^1$, $\tree{Φ}_1^1$, $\tree{Φ}_2^1$, ..., always omitting from the sequence any exhausted file. A partition $λ_2$ is to be recorded with the last item recorded on $\tree{Φ}_1^2$, and all files are to be rewound. The process is described by Program 1.14.

% (END EXAMPLE 1.3)

\par (TODO: PROGRAM 1.14)

\par \textbf{Program 1.14}. Step 8 cycles $k$ through the values 1 to $m$, and step 9 allows the read on steps 10 to occur only if $\vect{u}_k = 0$. The logical vector $\vect{u}$ is of dimension $m$ and designates the set of exhausted files. Its $k$th component is set to unity by step 11 when file $k$ is exhausted, as indicated by the occurrence of the partition $\mathbf{λ}_2$. Each read is normally followed by step 13, which records on the output file the item read. However, when the last file becomes exhausted, step 14 is executed instead to record the last item, together with the final partition $\mathbf{λ}_2$.

\par Steps 1-6 initialize the parameters $\vect{u}$ and $k$ and rewind all files. After the last item is recorded by step 14, the file rewinds are repeated before the final termination on step 7.

% (END PROGRAM 1.14 DESCRIPTION)

\par It is sometimes convenient to suppress explicit reference to the partition symbol read from a file by using a statement of the form

% TODO: vertically center arrows and join with |s
$$
  \underleftarrow{\mathbf{λ}_1}\left|
  x ← {}_0\tree{Φ}
  \right|\underrightarrow{\mathbf{λ}_2}
$$

\noindent where the indicated branches depend on the value of the partition $\vect{p}_{j+1}$ which terminates the read. Thus the left or the right branch is taken according to whether $\vect{p}_{j+1} = \vect{λ}_1$ or $\vect{p}_{j+1} = \vect{λ}_2$. Certain files (such as the IBM 7090 tape files) permit only such ``immediate'' branching and do not permit the partition symbol to be stored for use in later operations, as was done in Program 1.14.

\par In recording, the lowest level partition $\vect{λ}_1$ may be elided. Thus statement 13 of Program 1.14 may be written as

$$
  \tree{Φ}_1^2 ← b.
$$

\par A file may be read or recorded backward as well as forward. A backward read is denoted by

$$
  x, p ← {}_1\tree{Φ},
$$

\noindent and if $\tree{Φ}$ is initially in position $j + 1$, then $x = \vect{x}_j$, $p = \vect{p}_j$, and the final position becomes $j$. Backward recording is defined analogously. The zero prescript may be omitted from the symbol ${}_0\tree{Φ}$ for both forward reading and recording.

\par The conventions used for matrices can be applied in an obvious way to an array of files $\tree{Φ}_j^i$. For example, the statement

$$
  π(\tree{Φ}^i) ← \textbf{ϵ}
$$

\noindent denotes the rewinding of the \textit{row of files} $\tree{Φ}_j^i$, $j ϵ \textbf{ι}^1(ν(\tree{Φ}))$; the statement

$$
  π(\tree{Φ}_j) ← \textbf{ϵ}
$$

\noindent denotes the rewinding of the \textit{column of files} $\tree{Φ}_j^i$, $i ϵ \textbf{ι}^1(μ(\tree{Φ}))$; and the statement

$$
  \vect{u}/\tree{Φ}^i ← \vect{u}/\vect{x}, \vect{u}/\vect{p}
$$

\noindent denotes the recording of the vector components $\vect{x}_j$ on the file $\tree{Φ}_j^i$ together with partition $\vect{p}_j$ for all $j$ such that $\vect{u}_j = 1$.

\par As for vectors and matrices, $j$-origin indexing may be used and will apply to the indexing of the file positions and the partition vector $\mathbf{λ}$ as well as to the array indices. However, the prescripts (denoting direction of read and record) are independent of index origin. 0-origin indexing is used in the following example.

\par \textbf{Example 1.4}. Files $\tree{Φ}_0^0$ and $\tree{Φ}_1^0$ contain the vectors $\vect{x}$ and $\vect{y}$, respectively, each of dimension $n$. In the first phase, the components are to be merged in the order $\vect{x}_0$, $\vect{y}_0$, $\vect{x}_1$, $\vect{y}_1$, ... $\vect{x}_{ν-1}$, $\vect{y}_{ν-1}$, and the first $n$ components of the resulting vector are to be recorded on file $\tree{Φ}_0^1$, and the last $n$ on file $\tree{Φ}_1^1$. In other words, the vectors $\vect{x}^1 = \mathbf{α}^n/\vect{z}$ and $\vect{y}^1 = \textbf{ω}^n/\vect{z}$ are to be recorded on $\tree{Φ}_0^1$ and $\tree{Φ}_1^1$, respectively, where $\vect{z} = \backslash\vect{x}, \vect{u}, \vect{y}\backslash$, and $\vect{u} = (0, 1, 0, 1, ..., 0, 1)$. In the next phase, the roles of input and output files are reversed, and the same process is performed on $\vect{x}^1$ and $\vect{y}^1$, that is, $\vect{x}^2 = \mathbf{α}^n/(\backslash\vect{x}^1, \vect{u}, \vect{y}^1\backslash)$ and $\vect{y}^2 = \textbf{ω}^n/(\backslash\vect{x}^1, \vect{u}, \vect{y}^1\backslash)$ are recorded on $\tree{Φ}_0^0$ and $\tree{Φ}_1^0$, respectively. The process is to be continued through $m$ phases.

% (END EXAMPLE 1.4)

\par (TODO: PROGRAM 1.15)

\par \textbf{Program 1.15}. The program for Example 1.4 begins with the rewind of the entire $2 \times 2$ array of files. To obviate further rewinding, the second (and each subsequent even-numbered) execution is performed by reading and recording all files in the backward direction. Step 6 performs the essential read and record operation under control of the logical vector $\vect{u}$, whose components $\vect{u}_0$, $\vect{u}_1$, $\vect{u}_2$ determine, respectively, the subscript of the file to be read, the subscript of the file to be recorded, and the direction of read and record. The file superscripts (determining which classes serve as input and output in the current repetition) are also determined by $\vect{u}_2$, the input being $\vect{u}_2$ and the output $\overbar{\vect{u}}_2$. The loop 6-8 copies $n$ items, alternating the input files through the negation of $\vect{u}_0$ on step 7. When the loop terminates, $\vect{u}_1$ is negated to interchange the outputs, and the loop is repeated unless $\vect{u}_1 = \vect{u}_2$. Equality occurs and causes a branch to step 3 if and only if all $2n$ items of the current phase have already been copied.

\par Step 3 decrements $m$ and is followed by the negation of $\vect{u}$ on step 4. The component $\vect{u}_2$ must, of course, be negated to reverse direction, but the need to negate $\vect{u}_0$ and $\vect{u}_1$ is not so evident. It arises because the copying order was prescribed for the forward direction, beginning always with the operation

$$
  {}_0\tree{Φ}_0{}^p ← {}_0\tree{Φ}_0{}^{\overbar{p}}
$$

\noindent An equivalent backward copy must therefore begin with the operation

$$
  {}_1\tree{Φ}_1{}^p ← {}_1\tree{Φ}_1{}^{\overbar{p}}
$$

% (END PROGRAM 1.15 DESCRIPTION)

\par Not all computer files have the very general capabilities indicated by the present notation. Some files, for example, can be read and recorded in the forward direction only and, except for rewind, cannot be positioned directly. Positioning to an arbitrary position $k$ must then be performed by a rewind and a succession of $(k - 1)$ reads. In some files, recording can be performed in the forward direction only, and the positions are defined only by the recorded data. Consequently, recording in position $k$ makes unreliable the data in all subsequent positions, and recording must always proceed through all successive positions until terminated.

\section{Ordered trees}

\subsection*{Directed graphs}

\par For many processes it is convenient to use a structured operand with the treelike structure suggested by Fig. 1.16. It is helpful to begin with a more general structure (such as Fig. 1.17) in which a unidirectional association may be specified between any pair of its components.

\par (TODO: FIGURE 1.16)

\par \textbf{Figure 1.16} A general triply root tree with $λ(\mat{T}) = 16$, $\mathbf{ν}(\mat{T}) = (3, 3, 4, 3, 2)$, $ν(\mat{T}) = 5$, $\mathbf{μ}(\mat{T}) = (3, 7, 8, 5, 3)$, and $μ(\mat{T}) = 26$

% (END FIGURE 1.16 DESCRIPTION)

\par A \textit{directed graph} comprises a vector $\vect{n}$ and an arbitrary set of unilateral associations specified between pairs of its components. The vector $\vect{n}$ is called a \textit{node vector} and its components are also called \textit{nodes}. The associations are conveniently specified by a (logical) \textit{connection matrix} $\mat{U}$ of dimensions $ν(\vect{n}) \times μ(\vect{n})$ with the following convention: there is an association, called a \textit{branch}, from node $i$ to node $j$ if and only if $\mat{U}_j^i = 1$.

\par A directed graph admits of a simple graphical interpretation, as illustrated by Fig. 1.17. The nodes might, for example, represent places, and the lines, connecting streets. A two-way street is then represented by a pair of oppositely directed lines, as shown between nodes 3 and 4.

\par (TODO: FIGURE 1.17)

\par \textbf{Figure 1.17} A graphical representation of the graph $(\vect{n}, \mat{U})$.

% (END FIGURE 1.17 DESCRIPTION)

\par If $\vect{k}$ is any mapping vector such that

$$
  \mat{U}_{\vect{k}_i}^{\vect{k}_{i-1}} = 1
    \quad \text{for} i = 2, 3, ..., ν(\vect{k}),
$$

\par then the vector $\vect{p} = \vect{k} \int \vect{n}$ is called \textit{a path vector} of the graph $(\vect{n}, \mat{U})$. The dimension of a path vector is also called its \textit{length}. Nodes $\vect{k}_1$ and $\vect{k}_ν$ are called the \textit{initial} and \textit{final} nodes, respectively; both are also called \textit{terminal} nodes. If $\vect{j}$ is any infix of $\vect{k}$, then $\vect{q} = \vect{j} \int \vect{n}$ is also a path. It is called a subpath of $\vect{p}$ and is said to be \textit{contained} in $\vect{p}$. If $ν(\vect{q}) < ν(\vect{p})$, then $\vect{q}$ is a \textit{proper} subpath of $\vect{p}$. If $\vect{k}_1 = \vect{k}_ν$ and $\vect{p} = \vect{k} \int \vect{n}$ is a path of a length exceeding one, $\vect{p}$ is called a \textit{circuit}. For example, if $\vect{k} = (6, 1, 7, 7, 2, 6, 1, 5)$, then $\vect{p} = (\vect{n}_6, \vect{n}_1, \vect{n}_7, \vect{n}_7, \vect{n}_2, \vect{n}_6, \vect{n}_1, \vect{n}_5)$ is a path vector of the graph of Fig. 1.17, which contains the proper subpaths $(\vect{n}_7, \vect{n}_2, \vect{n}_6)$, $(\vect{n}_1, \vect{n}_7, \vect{n}_7, \vect{n}_2, \vect{n}_6, \vect{n}_1)$, and $(\vect{n}_7, \vect{n}_7)$, the last two of which are circuits. Node $j$ is said to be reachable from node $i$ if there exists a path from node $i$ to node $j$.

\subsection*{Ordered trees}

\par A graph (such as Fig. 1.16) which contains no circuits and which has at most one branch entering each node is called a \textit{tree}. Since each node is entered by at most one branch, a path existing between any two nodes in a tree is unique, and the length of path is also unique. Moreover, if any two paths have the same final node, one is a subpath of the other.

\par Since a tree contains no circuits, the length of path in a finite tree is bounded. There therefore exist \textit{maximal paths} which are proper subpaths of no longer paths. The initial and final nodes of a maximal path are called a \textit{root} and \textit{leaf} of the tree, respectively. A root is said to lie on the \textit{first level} of the tree, and, in general, a node which lies at the end of a path of length $j$ from a root, lies in the $j$th \textit{level} of the tree.

\par A tree which contains $n$ roots is said to be \textit{n-tuply rooted}. The sets of nodes reachable from each of the several roots are disjoint, for if any node is reachable by paths from each of two disjoint roots, one is a proper subpath of the other and is therefore not maximal. Similarly, any node of a tree defines a \textit{subtree} of which it is the root, consisting of itself and all nodes reachable from it, with the same associations as the parent tree.

\par If for each level $j$, a simple ordering is assigned to each of the disjoint sets of nodes reachable from each node of the preceding level, and if the roots are also simply ordered, the tree is said to be \textit{ordered}. Attention will henceforth be restricted to ordered trees, which will be denoted by uppercase boldface roman characters. The \textit{height} of a tree $\mat{T}$ is defined as the length of the longest path in $\mat{T}$ and is denoted by $ν(\mat{T})$. The number of nodes on level $j$ is called the \textit{moment of level j} and is denoted by $\mathbf{μ}_j(\mat{T})$. The vector $\mathbf{μ}(\mat{T})$ is called the \textit{moment vector}. The total number of nodes in $\mat{T}$ is called the moment of $\mat{T}$ and is denoted by $μ(\mat{T})$. Clearly, $ν(\mathbf{μ}(\mat{T})) = ν(\mat{T})$, and $+/\mathbf{μ}(\mat{T}) = μ(\mat{T}) = ν(\vect{n})$. The number of roots is equal to $\mathbf{μ}_1(\mat{T})$, and the number of leaves will be denoted by $λ(\mat{T})$.

\par The number of branches leaving a node is called its \textit{branching ratio} or \textit{degree}, and the maximum degree occurring in a tree $\mat{T}$ is denoted by $δ(\mat{T})$. The \textit{dispersion vector} of a tree $\mat{T}$ is denoted by $\mathbf{ν}(\mat{T})$ and is defined as follows: $\mathbf{ν}_1(\mat{T}) = \mathbf{μ}_1(\mat{T})$, and for $j = 2, 3, ..., ν(\mat{T})$, $\mathbf{ν}_j(\mat{T})$ is equal to the maximum over the branching ratios of the nodes on level $j - 1$. For the tree of Fig. 1.16, $\mathbf{ν}(\mat{T}) = (3, 3, 4, 3, 2)$. The number of roots possessed by a tree $\mat{T}$ (that is, $\mathbf{ν}_1(\mat{T})$) is called its \textit{dispersion}. A tree possessing unity dispersion is called \textit{rooted} or \textit{singular}.

\par Each node $\vect{n}_i$ of a graph (and hence of a tree) may be identified by its index $i$. Since a tree admits of more convenient index vectors, the underlying index $i$ will henceforth be referred to as the \textit{graph index}.

\par In an ordered tree, any path of length $k$ from a root can be uniquely specified by an \textit{index vector} $\vect{i}$ of dimension $k$, where $\vect{i}_1$ specifies the particular root, and the remaining components specify the (unique) path as follows: the path node on level $j$ is the $\vect{i}_j$th element of the set of nodes on level $j$ reachable from the path node on level $j - 1$. The node at the end of the path can therefore be designated uniquely by the index vector $\vect{i}$. The degree of node $\vect{i}$ will be denoted by $δ(\vect{i}, \mat{T})$. The index vectors are shown to the left of each node in Fig. 1.16.

\par The path from a root whose terminal node is $\vect{i}$ will be denoted by $\mat{T}^{\vect{i}}$. In Fig. 1.16, for example, $\mat{T}^{\vect{i}} = (\vect{n}_2, \vect{n}_8, \vect{n}_{13}, \vect{n}_{24})$ if $\vect{i} = (2, 2, 2, 3)$. A vector $\vect{i}$ is said to be an index of $\mat{T}$ if it is the index of some node in $\mat{T}$.

\par The subtree of $\mat{T}$ rooted in node $\vect{i}$ will be denoted by $\mat{T}_{\vect{i}}$. Thus in Fig. 1.16, $\mat{P} = \mat{T}_{(2,2,2)}$ is a rooted subtree with $\mathbf{ν}(\mat{P}) = (1, 3, 2)$, and $\mathbf{μ}(\mat{P}) = (1, 3, 3)$. A path in $\mat{T}_{\vect{i}}$ is denoted by $(\mat{T}_{\vect{i}})^{\vect{j}}$. For example, if $\mat{G}$ is an ascending genealogical tree%^{<a href="#note1h">[h]</a>}
with the sword and distaff sides denoted by the indices 1 and 2, respectively, then any individual $x$ and the nearest ($n - 1$) paternal male ancestors are represented by the path vector $(\mat{G}_{\vect{i}})^{\textbf{ϵ}(n)}$, where $\vect{i}$ is the index of $x$ in $\mat{G}$.

\par \textbf{Example 1.5}. Determine the index $\vect{i}$ such that the path $\mat{T}^{\vect{i}}$ is equal to a given argument $\vect{x}$ and is the ``first'' such path in $\mat{T}$; that is, the function

$$
  (\mathbf{α}^{ν(\vect{x})}/\mathbf{ν}(\mat{T})) ⊥ \vect{i}
$$

\noindent is a minimum.

% (END EXAMPLE 1.5)

\par (TODO: PROGRAM 1.18)

\par \textbf{Program 1.18}. The index vector $\vect{i}$ specifies the path currently under test. Its last component is incremented repeatedly by step 7 until the loop 6-8 is terminated. If the path $\mat{T}^{\vect{i}}$ agrees with the corresponding prefix of the argument $\vect{x}$, termination occurs through the branch to step 9, which tests for completion before step 10 augments $\vect{i}$ by a final zero component. Step 5 then respecifies $d$ as the degree of the penultimate node of the set of $d$ paths next to be tested by the loop. Termination by a branch from step 6 to step 2 occurs if all $d$ possible paths are exhausted without finding agreement on step 8. In this event, retraction by one level occurs on step 2, and $d$ is again respecified. If $ν(\vect{i}) = 1$, the paths to be searched comprise the roots of the tree and $d$ must therefore be specified as the number of roots. This is achieved by executing step 3 and skipping step 5. Retraction to a vector $\vect{i}$ of dimension zero occurs only if all roots have been exhausted, and final termination from step 4 indicates that the tree possesses no path equal to the argument $\vect{x}$.

% (END PROGRAM 1.18 DESCRIPTION)

\par If $\vect{d}$ is a vector of dimension $ν(\vect{n})$ such that $\vect{d}_i$ is the degree of node $\vect{n}_i$ of a tree $\mat{T}$, then $\vect{d}$ is called the \textit{degree vector associated with} $\vect{n}$. In Fig. 1.16, for example,

$$
  \vect{d} = (3, 2, 4, 0, 0, 0, 2, ..., 1, 0, 0).
$$

\noindent Moreover, if $\vect{n}$ is itself the alphabet (that is, $\vect{n} = (a, b, c, ... , z)$), then the vector $\vect{n}'$ of Table 1.19a is a permutation of $\vect{n}$, and $\vect{d}'$ is the associated degree vector. Table 1.19b shows another such pair, $\vect{n}''$ and $\vect{d}''$.

\par The degree vector provides certain useful information most directly. For example, since each leaf is of degree zero, $λ(\mat{T}) = +/(\vect{d} = 0)$. Moreover, the number of roots is equal to the number of nodes less the total of the degrees, that is, $\mathbf{μ}_1(\mat{T}) = ν(\vect{d}) - +/\vect{d}$, and the maximal degree occurring in $\mat{T}$ is given by $δ(\mat{T}) = ((\textbf{ϵ}⌈\vect{d})/\vect{d})_1$. Finally, the degree vector and the node vector together can, in certain permutations (those of Table 1.19), provide a complete and compact description of the tree.

\par (TODO: TABLE 1.19)

\par \textbf{Table 1.19}. Full list matrices of the tree of Fig. 1.16
