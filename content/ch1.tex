
\chapter{The Language}

\section{Introduction}

\par Applied mathematics is concerned with the design and analysis of algorithms or \textit{programs}. The systematic treatment of complex algorithms requires a suitable \textit{programming language} for their description, and such a programming language should be concise, precise, consistent over a wide area of application, mnemonic, and economical of symbols; it should exhibit clearly the constraints on the sequence in which operations are performed; and it should permit the description of a process to be independent of the particular representation chosen for the data.

\par Existing languages prove unsuitable for a variety of reasons. Computer coding specifies sequence constraints adequately and is also comprehensive, since the logical functions provided by the branch instructions can, in principle, be employed to synthesize any finite algorithm. However, the set of basic operations provided is not, in general, directly suited to the execution of commonly needed processes, and the numeric symbols used for variables have little mnemonic value. Moreover, the description provided by computer coding depends directly on the particular representation chosen for the data, and it therefore cannot serve as a description of the algorithm per se.

\par Ordinary English lacks both precision and conciseness. The widely used Goldstine-von Neumann (1947) flowcharting provides the conciseness necessary to an over-all view of the processes, only at the cost of suppressing essential detail. The so-called pseudo-English used as a basis for certain automatic programming systems suffers from the same defect. Moreover, the potential mnemonic advantage in substituting familiar English words and phrases for less familiar but more compact mathematical symbols fails to materialize because of the obvious but unwonted precision required in their use.

\par Most of the concepts and operations needed in a programming language have already been defined and developed in one or another branch of mathematics. Therefore, much use can and will be made of existing notations. However, since most notations are specialized to a narrow field of discourse, a consistent unification must be provided. For example, separate and conflicting notations have been developed for the treatment of sets, logical variables, vectors, matrices, and trees, all of which may, in the broad universe of discourse of data processing, occur in a single algorithm.

\section{Programs}

\par A \textit{program statement} is the specification of some quantity or quantities in terms of some finite operation upon specified operands. Specification is symbolized by an arrow directed toward the specified quantity. thus ``$y$ is specified by $\sin x$'' is a statement denoted by

$$
      y\ ←\ \sin \ x.
$$

\par A set of statements together with a specified order of execution constitutes a \textit{program}. The program is \textit{finite} if the number of executions is finite. The \textit{results} of the program are some subset of the quantities specified by the program. The \textit{sequence} or order of execution will be defined by the order of listing and otherwise by arrows connecting any statement to its successor. A cyclic sequence of statements is called a \textit{loop}.

\par (TODO: FIGURES 1.1 AND 1.2)

\par Thus Program 1.1 is a program of two statements defining the result $v$ as the (approximate) area of a circle of radius $x$, whereas Program 1.2 is an infinite program in which the quantity $z$ is specified as $(2y)^n$ on the $n$th execution of the two statement loop. Statements will be numbered on the left for reference.

\par A number of similar programs may be subsumed under a single more general program as follows. At certain \textit{branch points} in the program a finite number of alternative statements are specified as possible successors. One of these successors is chosen according to criteria determined in the statement or statements preceding the branch point. These criteria are usually stated as a \textit{comparison} or test of a specified relation between a specified pair of quantities. A branch is denoted by a set of arrows leading to each of the alternative successors, with each arrow labeled by the comparison condition under which the corresponding successor is chosen. The quantities compared are separated by a colon in the statement at the branch point, and a labeled branch is followed if and only if the relation indicated by the label holds when substituted for the colon. The conditions on the branches of a properly defined program must be disjoint and exhaustive.

\par Program 1.3 illustrates the use of a branch point. Statement \verb|α|$5$ is a comparison which determines the branch to statements \verb|β|$1$, \verb|δ|$1$, or \verb|γ|$1$, according as $z > n$, $z = n$, or $z < n$. The program represents a crude by effective process for determining $x = n^\frac{2}{3}$ for any positive cube n.

\par (TODO: FIGURE 1.3)

\par Program 1.4 shows the preceding program reorganized into a compact linear array and introduces two further conventions on the labeling of branch points. The listed successor of a branch statement is selected if none of the labeled conditions is met. Thus statement 6 follows statement 5 if neither of the arrows (to exit or to statement 8) are followed, i.e. if $z < n$. Moreover, any unlabeled arrow is always followed; e.g., statement 7 is invariably followed by statement 3, never by statement 8.

\par A program begins at a point indicated by an \textit{entry arrow} (step 1) and ends at a point indicated by an \textit{exit arrow} (step 5). There are two useful consequences of confining a program to the form of a linear array: the statements may be referred to by a unique serial index (statement number), and unnecessarily complex organization of the program manifests itself in crossing branch lines. The importance of the latter characteristic in developing clear and comprehensible programs is not sufficiently appreciated.

\par (TODO: FIGURES 1.4 AND 1.5)

\par A process which is repeated a number of times is said to be \textit{iterated}, and a process (such as in Program 1.4) which includes one or more iterated subprocesses is said to be \textit{iterative}. Program 1.5 shows an iterative process for the matrix multiplication

$$
      C\ ←\ AB
$$

\noindent defined in the usual way as

\begin{equation*}
  \begin{split}
    C^i_j = \sum^{ν(\mat{A})}_{k=1} A^k_i \times B^k_j
  \end{split}
\quad\quad
  \begin{split}
    i &= 1,2, ..., μ(\mat{A}),\\
    j &= 1,2, ..., ν(\mat{B}),
  \end{split}
\end{equation*}

\noindent where the dimensions of an $m \times n$ matrix $\mat{X}$ (of $m$ rows and $n$ columns) is denoted by $μ(\mat{X}) \times ν(\mat{X})$.

\par \textbf{Program 1.5}. Steps 1-3 initialize the indices, and the loop 5-7 continues to add successive products to the partial sum until $k$ reaches zero. When this occurs, the process continues through step 8 to decrement $j$ and to repeat the entire summation for the new value of $j$, providing that it is not zero. If $j$ is zero, the branch to step 10 decrements $i$ and the entire process over $j$ and $k$ is repeated from $j = ν(\mat{B})$, providing that $i$ is not zero. If $i$ is zero, the process is complete, as indicated by the exit arrow.

\par In all examples used in this chapter, emphasis will be placed on clarity of description of the process, and considerations of efficient execution by a computer or class of computers will be subordinated. These considerations can often be introduced later by relatively routine modifications of the program. For example, since the execution of a computer operation involving an indexed variable is normally more costly than the corresponding operation upon a nonindexed variable, the substitution of a variable $s$ for the variable $\mat{C}^i_j$ specified by statement 5 of Program 1.5 would accelerate the execution of the loop. The variable $s$ would be initialized to zero before each entry to the loop and would be used to specify $\mat{C}^i_j$ at each termination.

\par The practice of first setting an index to its maximum value and then decrementing it (e.g., the index $k$ in Program 1.5) permits the termination comparison to be made with zero. Since zero often occurs in comparisons, it is convenient to omit it. Thus, if a variable stands alone at a branch point, comparison with zero is implied. Moreover, since a comparison on an index frequently occurs immediately after it is modified, a branch at the point of modification will denote branching upon comparison of the indicated index with zero, the comparison occurring \textit{after} modification. Designing programs to execute decisions immediately after modification of the controlling variable results in efficient execution as well as notational elegance, since the variable must be present in a central register for both operations.

\par Since the sequence of execution of statements is indicated by connecting arrows as well as by the order of listing, the latter can be chosen arbitrarily. This is illustrated by the functionally identical Programs 1.3 and 1.4. Certain principles of ordering may yield advantages such as clarity or simplicity of the pattern of connections. Even though the advantages of a particular organizing principle are not particularly marked, the uniformity resulting from its consistent application will itself be a boon. The scheme here adopted is called the \textit{method of leading decisions}: the decision on each parameter is placed as early in the program as practicable, normally just before the operations indexed by the parameter. This arrangement groups at the head of each iterative segment the initialization, modification, and the termination test of the controlling parameter. Moreover, it tends to avoid program flaws occasioned by unusual values of the argument.

\par (TODO: FIGURE 1.6)

\par For example, Program 1.6 (which is a reorganization of Program 1.5) behaves properly for matrices of dimension zero, whereas Program 1.5 treats every matrix as if it were of dimension one or greater.

\par Although the labeled arrow representation of program branches provides a complete and graphic description, it is deficient in the following respects: (1) a routine translation to another language (such as computer code) would require the tracing of arrows, and (2) it does not permit programmed modification of the branches.

\par The following alternative form of a branch statement will therefore be used as well:

$$
x\ :\ y,\ \vect{r}\ →\ \vect{s}.
$$

\par This denotes a branch to statement number $\vect{s}_i$ of the program if the relation $x\vect{r}_i y$ holds. The parameters $\vect{r}$ and $\vect{s}$ may themselves be defined and redefined in other parts of the program. The \textit{null element} $∘$ will be used to denote the relation which complements the remaining relations $\vect{r}_i$; in particular, $(∘)\ →\ (s)$, or simply $→\ s$, will denote an unconditional branch to statement $s$. Program 1.7 shows the use of these conventions in a reformulation of Program 1.6. More generally, two or more otherwise independent programs may interact through a statement in one program specifying a branch in a second. The statement number occurring in the branch must then be augmented by the name of the program in which the branch is effected. Thus the statement $(∘)\ →$ Program 2.24 executed in Program 1 causes a branch to step 24 to occur in Program 2.

\par (TODO: FIGURE 1.7)

\par One statement in a program can be modified by another statement which changes certain of its parameters, usually indices. More general changes in statements can be effected by considering the program itself as a vector $\vect{p}$ whose components are the individual, serially numbered statements. All the operations to be defined on general vectors can then be applied to the statements themselves. For example, the $j$th statement can be respecified by the $i$th through the occurrence of the statement $\vect{p}_j\ ←\ \vect{p}_i$.

\par The interchange of two quantities $y$ and $x$ (that is, $x$ specifies $y$ and the \textit{original} value of $y$ specifies $x$) will be denoted by the statement $y\ \leftrightarrow\ x$.

\section{Structure of the language}

\subsection*{Conventions}

\par The Summary of Notation at the end of the book summarizes the notation developed in this chapter. Although intended primarily for reference, it supplements the text in several ways. It frequently provides a more concise alternative definition of an operation discussed in the text, and it also contains important but easily grasped extensions not treated explicitly in the text. By grouping the operations into related classes it displays their family relationships.

\par A concise programming language must incorporate families of operations whose members are related in a systematic manner. Each family will be denoted by a specific operation symbol, and the particular member of the family will be designated by an associated \textit{controlling parameter} (scalar, vector, matrix, or tree) which immediately precedes the main operation symbol. The operand is placed immediately after the main operation symbol. For example, the operation $k\ ↑\ \vect{x}$ (left rotation of $\vect{x}$ by $k$ places) may be viewed as the $k$th member of the set of rotation operators denoted by the symbol $↑$.

\par Operations involving a single operand and no controlling parameter (such as $\lfloor x \rfloor$, or $\lceil x \rceil$) will be denoted by a pair of operation symbols which enclose the operand. Operations involving two operands and a controlling parameter (such as the mask operation $/\vect{a},\ \vect{u},\ \vect{b}/$) will be denoted by a pair of operation symbols enclosing the entire set of variables, and the controlling parameter will appear between the two operands. In these cases the operation symbols themselves serve as grouping symbols.

\par In interpreting a compound operation such as $k ↑ (j ↓ \vect{x})$ it is important to recognize that the operation symbol and its associated controlling parameter together represent an indivisible operation and must not be separated. It would, for example, be incorrect to assume that $j ↑ (k ↓ \vect{x})$ were equivalent to $k ↑ (j ↓ \vect{x})$, although it can be shown that the complete operations $j ↓$ and $k ↑$ do commute, that is $k ↑ (j ↓ \vect{x}) = j ↓ (k ↑ \vect{x})$.

\par The need for parentheses will be reduced by assuming that compound statements are, except for intervening parentheses, executed from right to left. Thus $k ↑ j ↓ \vect{x}$ is equivalent to $k ↑ (j ↓ \vect{x})$, not to $(k ↑ j) ↓ \vect{x}$.

\par Structured operands such as vectors and matrices, together with a systematic component-by-component generalization of elementary operations, provide an important subordination of detail in the description of algorithms. The use of structured operands will be facilitated by \textit{selection operations} for extracting a specified portion of an operand, \textit{reduction operations} for extending an operation (such as logical or arithmetic multiplication) over all components, and \textit{permutation operations} for reordering components. Operations defined on vectors are extended to matrices: the extended operation is called a \textit{row} operation if the underlying vector operation is applied to each row of the matrix and a \textit{column} operation if it is applied to each column. A column operation is denoted by doubling the symbol employed for the corresponding row (and vector) operation.

\par A distinct typeface will be used for each class of operand as detailed in Table 1.8. Special quantities (such as the prefix vectors $\mathbf{α}^i$ defined in Sec. 1.7) will be denoted by Greek letters in the appropriate typeface. For mnemonic reasons, an operation closely related to such a special quantity will be denoted by the same Greek letter. For example, $α/\vect{u}$ denotes the maximum prefix (Sec. 1.10) of the logical vector $\vect{u}$. Where a Greek letter is indistinguishable from a Roman, sanserif characters will be used, e.g. $\mathsf{\mat{E}}$ and $\mathsf{\mat{I}}$ for the capitals of epsilon and iota.

\par (TODO: TABLE 1.8)

\par \textbf{Table 1.8} Typographic conventions for classes of operands

\subsection*{Literals and variables}

\par The power of any mathematical notation rests largely on the use of symbols to represent general quantities which, in given instances, are further specified by other quantities. Thus Program 1.4 represents a general process which determines $x = n^\frac{2}{3}$ for any suitable value of $n$. In a specific case, say $n = 27$, the quantity $x$ is specified as the number 9.

\par Each operand occurring in a meaningful process must be specified ultimately in terms of commonly accepted concepts. The symbols representing such accepted concepts will be called \textit{literals}. Examples of literals are the integers, the characters of the various alphabets, punctuation marks, and miscellaneous symbols such as \$ and \%. The literals occurring in Program 1.4 are 0, 1, 2.

\par It is important to distinguish clearly between general symbols and literals. In ordinary algebra this presents little difficulty, since the only literals occurring are the integers and the decimal point, and each general symbol employed includes an alphabetic character. In describing more general processes, however, alphabetic literals (such as proper names) also appear. Moreover, in a computer program, numeric symbols (register addresses) are used to represent the variables.

\par In general, then, alphabetic literals, alphabetic variables, numeric literals, and numeric variables may all appear in a complex process and must be clearly differentiated. The symbols used for literals will be Roman letters (enclosed in quotes when appearing in text) and standard numerals. The symbols used for variables will be italic letters, italic numerals, and boldface letters as detailed in Table 1.8. Miscellaneous signs and symbols when used as literals will be enclosed in quotes in both programs and text.

\par It is sometimes desirable (e.g., for mnemonic reasons) to denote a variable by a string of alphabetic or other symbols rather than by a single symbol. The monolithic interpretation of such a string will be indicated by the \textit{tie} used in musical notation, thus:
(TODO ties) $inv$, $\mathbf{inv}$, $\mathbf{INV}$ may denote the variable ``inventory'', a vector of inventory values, and a matrix of inventory values, respectively.

\par In the set of alphabetic characters, the \textit{space} plays a special role. For other sets a similar role is usually played by some one element, and this element is given the special name of \textit{null element}. In the set of numeric digits, the \textit{zero} plays a dual role as both null element and numeric quantity. The null element will be denoted by the degree symbol $∘$.

\par In any determinate process, each operand must be specified ultimately in terms of literals. In Program 1.4, for example, the quantity $k$ is specified in terms of known arithmetic operations (multiplication and division) involving the literals 1 and 2. The quantity $n$, on the other hand, is not determined within the process and must presumably be specified within some larger process which includes Program 1.4. Such a quantity is called an \textit{argument} of the process.

\subsection*{Domain and range}

\par The class of arguments and the class of results of a given operator are called its \textit{domain} and \textit{range}, respectively. Thus the domain and range of the magnitude operation ($|x|$) are the real numbers and the nonnegative real numbers, respectively.

\par A variable is classified according to the range of values it may assume: it is \textit{logical}, \textit{integral}, or \textit{numerical}, according as the range is the set of logical variables (that is, 0 and 1), the set of integers, or the set of real numbers. Each of the foregoing classes is clearly a subclass of each class following it, and any operation defined on a class clearly applies to any of its subclasses. A variable which is nonnumeric will be called \textit{arbitrary}. In the Summary of Notation, the range and domain of each of the operators defined is specified in terms of the foregoing classes according to the conventions shown in Sec. S.1.

\section{Elementary operations}

\par The elementary operations employed include the ordinary arithmetic operations, the elementary operations of the logical calculus, and the residue and related operations arising in elementary number theory. In defining operations in the text, the symbol $\leftrightarrow$ will be used to denote equivalence of the pair of statements between which it occurs.

\subsection*{Arithmetic operations}

The ordinary arithmetic operations will be denoted by the ordinary symbols $+$, $-$, $\times$, and $÷$ and defined as usual except that the domain and range of multiplication will be extended slightly as follows. If one of the factors is a logical variable (0 or 1), the second may be arbitrary and the product then assumes the value of the second factor or zero according as the value of the first factor (the logical variable) is 1 or 0. Thus if the arbitrary factor is the literal ``q'', then

\begin{align*}
           & 0 \times \text{q} = \text{q} \times 0 = 0 \\
\text{and} & 1 \times \text{q} = \text{q} \times 1 = \text{q}.
\end{align*}

\par According to the usual custom in ordinary algebra, the multiplication symbol may be elided.

\subsection*{Logical operations}

\par The elementary logical operations \textit{and}, \textit{or}, and \textit{not} will be denoted by $\wedge$, $\vee$ and an overbar and are defined in the usual way as follows:

\begin{alignat*}{2}
 w ← u \wedge v  & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 1 \quad\text{and}\quad v = 1, \\
 w ← u \vee v    & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 1 \quad\text{or} \quad v = 1, \\
 w ← \overbar{u} & \leftrightarrow w = 1 \quad\text{if and only if}\quad & u = 0.
\end{alignat*}

\par If $x$ and $y$ are numerical quantities, then the expression $x < y$ implies that the quantity $x$ stands in the relation ``less than'' to the quantity $y$. More generally, if $α$ and $β$ are arbitrary entities and $\mathcal{R}$ is any relation defined on them, the \textit{relational statement} $(α \mathcal{R} β)$ is a logical variable which is true (equal to 1) if and only if $α$ stands in the relation $\mathcal{R}$ to $β$. For example, if $x$ is any real number, then the function

$$
  (x > 0) - (x < 0)
$$

\par (commonly called the \textit{sign function} or $\text{sgn}\ x$) assumes the values 1, 0, or $-1$ according as $x$ is strictly positive, 0, or strictly negative. Moreover, the magnitude function $|x|$ may be defined as $|x| = x \times \text{sgn}\ x = x \times ((x > 0) - (x < 0))$.

\par The relational statement is a useful generalization of the Kronecker delta, that is $δ_j^i = (i = j)$. Moreover, it provides a convenient expression for a number of familiar logical operations. The \textit{exclusive or}, for example, may be denoted by $(u \neq v)$, and its negation (i.e., the equivalence function) may be denoted by $(u = v)$.

\subsection*{Residues and congruence}

\par For each set of integers $n$, $j$, and $b$, with $b > 0$, there exists a unique pair of integers $q$ and $r$ such that

$$
  n = bq + r,\quad j \leq r < j + b.
$$

\par The quantity $r$ is called the \textit{$j$-residue of n modulo b} and is denoted by $b |_j n$. For example, $3 |_0 9 = 0$, $3 |_1 9 = 3$, and $3 |_0 10 = 1$. Moreover, if $n \geq 0$, then $b |_0 n$ is the remainder obtained in dividing $n$ by $b$ and $q$ is the integral part of the quotient. A number $n$ is said to be of \textit{even parity} if its 0-residue modulo 2 is zero and of \textit{odd parity} if $2 |_0 n = 1$.

\par If two numbers $n$ and $m$ have the same $j$-residue modulo $b$, they differ by an integral multiple of $b$ and therefore have the same $k$-residue module $b$ for any $k$. If $b |_j n = b |_j m$, then $m$ and $n$ are said to be \textit{congruent mod $b$}. Congruency is transitive and reflexive and is denoted by

$$
  m \equiv n (\mod b).
$$

\par In classical treatments, such as <acronym title="Wright, H.N. (1939), First Course in Theory of Numbers, Wiley, New York.">Wright (1939)</acronym>, only the 0-residue is considered. The use of 1-origin indexing (cf. Sec. 1.5) accounts for the interest of the 1-residue.

\par A number represented in a positional notation (e.g., in a base ten or a base two number system) must, in practice, employ only a finite number of digits. It is therefore often desirable to approximate a number $x$ by an integer. For this purpose two functions are defined:

\begin{enumerate}
  \item the \textit{floor of x} (or integral part of $x$), denoted by $⌊x⌋$ and defined as the largest integer not exceeding $x$,
  \item the \textit{ceiling of x}, denoted by $⌈x⌉$ and defined as the smallest integer not exceeded by $x$.
\end{enumerate}

\noindent Thus

\begin{alignat*}{2}
  ⌈3.14⌉ = 4, & ⌊3.14⌋ = 3, & ⌊-3.14⌋ = -4, \\
  ⌈3.00⌉ = 3, & ⌊3.00⌋ = 3, & ⌊-3.00⌋ = -3.
\end{alignat*}

\par Clearly $⌈x⌉ = -⌊-x⌋$ and $⌊x⌋ \leq x \leq ⌈x⌉$. Moreover, $n = b⌊n ÷ b⌋ + b |_0 n$ for all integers $n$. Hence the integral quotient $⌊n ÷ b⌋$ is equivalent to the quantity $q$ occurring in the definition of the $j$-residue for the case $j = 0$.

\section{Structured operands}

\subsection*{Elementary operations}

\par Any operation defined on a single operand can be generalized to apply to each member of an array of related operands. Similarly, any binary operation (defined on two operands) can be generalized to apply to pairs of corresponding elements of two arrays. Since algorithms commonly incorporate processes which are repeated on each member of an array of operands, such generalization permits effective subordination of detail in their description. For example, the accounting process defined on the data of an individual bank account treats a number of distinct operands within the account, such as account number, name, and balance. Moreover, the over-all process is defined on a large number of similar accounts, all represented in a common format. Such structured arrays of variables will be called \textit{structured operands}, and extensive use will be made of three types, called \textit{vector}, \textit{matrix}, and \textit{tree}. As indicated in Sec. S.1 of the Summary of Notation, a structured operand is further classified as \textit{logical}, \textit{integral}, \textit{numerical}, or \textit{arbitrary}, according to the type of elements in contains.

\par A \textit{vector} $\vect{x}$ is the ordered array of elements $(\vect{x}_1, \vect{x}_2, \vect{x}_3, ..., \vect{x}_{ν(\vect{x})})$. The variable $\vect{x}_i$ is called the $i$th \textit{component} of the vector $\vect{x}$, and the number of components, denoted by $ν(\vect{x})$ (or simply by $ν$ when the determining vector is clear from context), is called the \textit{dimension} of $\vect{x}$. Vectors and their components will be represented in lower case boldface italics. A numerical vector $\vect{x}$ may be multiplied by a numerical quantity $k$ to produce the \textit{scalar multiple} $k \times \vect{x}$ (or $k\vect{x}$) defined as the vector $\vect{z}$ such that $\vect{z}_i = k \times \vect{x}_i$.

\par All elementary operations defined on individual variables are extended consistently to vectors as component-by-component operations. For example,

\begin{align*}
  \vect{z} = \vect{x} + \vect{y}      & \leftrightarrow \vect{z}_i = \vect{x}_i + \vect{y}_i, \\
  \vect{z} = \vect{x} \times \vect{y} & \leftrightarrow \vect{z}_i = \vect{x}_i \times \vect{y}_i, \\
  \vect{z} = \vect{x} ÷ \vect{y}      & \leftrightarrow \vect{z}_i = \vect{x}_i ÷ \vect{y}_i, \\
  \vect{z} = ⌈\vect{x}⌉               & \leftrightarrow \vect{z}_i = ⌈\vect{x}_i⌉, \\
  \vect{w} = \vect{u} \wedge \vect{v} & \leftrightarrow \vect{w}_i = \vect{u}_i \wedge \vect{v}_i, \\
  \vect{w} = (\vect{x} < \vect{y})    & \leftrightarrow \vect{w}_i = (\vect{x}_i < \vect{y}_i).
\end{align*}

\par Thus if $\vect{x} = (1, 0, 1, 1)$ and $\vect{y} = (0, 1, 1, 0)$ then $\vect{x} + \vect{y} = (1, 1, 2, 1)$, $\vect{x} \wedge \vect{y} = (0, 0, 1, 0)$, and $(\vect{x} < \vect{y}) = (0, 1, 0, 0)$.

\subsection*{Matrices}

\par A matrix $\mat{M}$ is the ordered two-dimensional array of variables

$$
  \begin{pmatrix}
    \mat{M}_1^1, & \mat{M}_2^1, & ..., & \mat{M}_{ν(\mat{M})}^1 \\
    \mat{M}_1^2, & \mat{M}_2^2, & ..., & \mat{M}_{ν(\mat{M})}^2 \\
    . & . & . & . \\
    \mat{M}_1^{μ(\mat{M})}, & \mat{M}_2^{μ(\mat{M})}, & ..., & \mat{M}_{ν(\mat{M})}^{μ(\mat{M})}
  \end{pmatrix}
$$

\par The vector $(\mat{M}_1^i, \mat{M}_2^i, ..., \mat{M}_ν^i)$ is called the $i$th \textit{row vector} of $\mat{M}$ and is denoted by $\mat{M}^i$. Its dimension $ν(\mat{M})$ is called the \textit{row dimension} of the matrix. The vector $(\mat{M}_j^1, \mat{M}_j^2, ..., \mat{M}_j^μ)$ is called the $j$th \textit{column vector} of $\mat{M}$ and is denoted by $\mat{M}_j$. Its dimension $μ(\mat{M})$ is called the \textit{column dimension} of the matrix.

\par The variable $\mat{M}_j^i$ is called the $(i,j)$th \textit{component} or \textit{element} of the matrix. A matrix and its elements will be represented by upper case boldface italics. Operations defined on each element of a matrix are generalized component by component to the entire matrix. Thus, if $\odot$ is any binary operator,

$$
  \mat{P} = \mat{M}
    \odot \mat{N} \leftrightarrow \mat{M}_j^i
    \odot \mat{N}_j^i.
$$

\subsection*{Index systems}

\par The subscript appended to a vector to designate a single component is called an \textit{index}, and the indices are normally chosen as a set of successive integers beginning at 1, that is, $\vect{x} = (\vect{x}_1, \vect{x}_2, ... \vect{x}_ν)$. It is, however, convenient to admit more general \textit{$j$-origin indexing} in which the set of successive integers employed as indices in any structured operand begin with a specified integer $j$.

\par The two systems of greatest interest are the common 1-origin system, which will be employed almost exclusively in this chapter, and the 0-origin system. The latter system is particularly convenient whenever the index itself must be represented in a positional number system and will therefore be employed exclusively in the treatment of computer organization in Chapter 2.

\section{Rotation}

\par The \textit{left rotation} of a vector $\vect{x}$ is denoted by $k ↑ \vect{x}$ and specifies the vector obtained by cyclical left shift of the components of $\vect{x}$ by $k$ places. Thus if $\vect{a} = (1, 2, 3, 4, 5, 6)$, and $\vect{b} = (\text{c}, \text{a}, \text{n}, \text{d}, \text{y})$, then $2 ↑ \vect{a} = (3, 4, 5, 6, 1, 2)$ and $3 ↑ \vect{b} = 8 ↑ \vect{b} = (\text{d}, \text{y}, \text{c}, \text{a}, \text{n})$. Formally,%^{<a href="#note1a">[a]</a>}

$$
  \vect{z} = k ↑ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j,\quad
    \text{where}\; j = ν|_1(i + k).
$$

\noindent \textit{Right rotation} is denoted by $k ↓ \vect{x}$ and is defined analogously. Thus

$$
  \vect{z} = k ↓ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j,\quad
    \text{where}\; j = ν|_1(i - k).
$$

\noindent If $k = 1$, it may be elided. Thus $↑ \vect{b} = (\text{a}, \text{n}, \text{d}, \text{y}, \text{c})$.

\par Left rotation is extended to matrices in two ways as follows:

\begin{alignat*}{2}
 \mat{A} ← \vect{j} ↑ \mat{B} & \leftrightarrow & \mat{A}^i = \vect{j}_i ↑ \mat{B}^i \\
 \mat{C} ← \vect{k} \Uparrow \mat{B} & \leftrightarrow & \mat{C}_i = \vect{k}_j ↑ \mat{B}_j
\end{alignat*}

\par The first operation is an extension of the basic vector rotation to each row of the matrix and is therefore called \textit{row rotation}. The second operation is the corresponding column operation and is therefore denoted by the doubled operation symbol $\Uparrow$. For example, if

$$
  \vect{k} = (0, 1, 2),
$$

\noindent and

$$
  \mat{B} = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}
$$

\noindent then

$$
  \vect{k} ↑ \mat{B} = \begin{pmatrix} a & b & c \\ e & f & d \\ i & g & h \end{pmatrix}
  \quad \text{and} \quad
  \vect{k} \Uparrow \mat{B} = \begin{pmatrix} a & e & i \\ d & h & c \\ g & b & f \end{pmatrix}
$$

\par Right rotation is extended analogously.

\section{Special vectors}

\par Certain special vectors warrant special symbols. In each of the following definitions, the parameter $n$ will be used to specify the dimension. The \textit{interval vector} $\textbf{ι}^j(n)$ is defined as the vector of integers beginning with $j$. Thus $\textbf{ι}^0(4)=(0, 1, 2, 3)$, $\textbf{ι}^1(4)=(1, 2, 3, 4)$, and $\textbf{ι}^{-7}(5)= (-7, -6, -5, -4, -3)$. Four types of logical vectors are defined as follows. The $j$th \textit{unit vector} $\textbf{ϵ}^j(n)$ has a one in the $j$th position, that is, $(\textbf{ϵ}^j(n))_k = (k = j)$. The \textit{full vector} $\textbf{ϵ}(n)$ consists of all ones. The vector consisting of all zeros is denoted both by $0$ and by $\overbar{\textbf{ϵ}}(n)$. The \textit{prefix vector of weight j} is denoted by $\mathbf{α}^j(n)$ and possesses ones in the first $k$ positions, where $k$ is the lesser of $j$ and $n$. The \textit{suffix vector} $\textbf{ω}^j(n)$ is defined analogously. Thus $\textbf{ϵ}^2(3) = (0, 1, 0)$, $\textbf{ϵ}(4) = (1, 1, 1, 1)$, $\mathbf{α}^3(5) = (1, 1, 1, 0, 0)$, $\textbf{ω}^3(5) = (0, 0, 1, 1, 1)$, and $\mathbf{α}^7(5) = \mathbf{α}^5(5) = (1, 1, 1, 1, 1)$. Moreover, $\textbf{ω}^j(n) = j ↑ \mathbf{α}^j(n)$, and $\mathbf{α}^j(n) = j ↓ \textbf{ω}^j(n)$.

\par A logical vector of the form $\mathbf{α}^h(n) \wedge \textbf{ω}^j(n)$ is called an \textit{infix vector}. An infix vector can also be specified in the form $j ↓ \mathbf{α}^k(n)$, which displays its weight and location more directly.

\par An operation such as $\vect{x} \wedge \vect{y}$ is defined only for \textit{compatible} vectors $\vect{x}$ and $\vect{y}$, that is, for vectors of like dimension. Since this compatibility requirement can be assumed to specify implicitly the dimension of one of the operands, elision of the parameter $n$ may be permitted in the notation for the special vectors. Thus, if $y = (3, 4, 5, 6, 7)$, the expression $\textbf{ϵ} \times \vect{y}$ and $\textbf{ϵ}^j \times \vect{y}$ imply that the dimensions of $\textbf{ϵ}$ and $\textbf{ϵ}^j$ are both 5. Moreover, elision of $j$ will be permitted for the interval vector $\textbf{ι}^j(n)$ (or $\textbf{ι}^j$), and for the residue operator $|_j$ when $j$ is the index origin in use.

\par It is, of course, necessary to specify the index origin in use at any given time. For example, the unit vector $\textbf{ϵ}^3(5)$ is $(0, 0, 1, 0, 0)$ in a 1-origin system and $(0, 0, 0, 1, 0)$ in a 0-origin system, even though the definition (that is, $(\textbf{ϵ}^j(n))_k = (k = j)$) remains unchanged. The prefix and suffix vectors are, of course, independent of the index origin. Unless otherwise specified, 1-origin indexing will be assumed.

\par The vector $\textbf{ϵ}(0)$ is a vector of dimension zero and will be called the \textit{null vector}. It should not be confused with the special null element $∘$.

\section{Reduction}

\par An operation (such as summation) which is applied to all components of a vector to produce a result of a simpler structure is called a \textit{reduction}. The $\odot$-reduction of a vector $\vect{x}$ is denoted by $\odot/\vect{x}$ and defined as

% Unbalanced parens, but seems intentional
$$
  \vect{z} ←
    \odot/\vect{x} \leftrightarrow \vect{z} =
      (\cdots ((\vect{x}_1 \odot \vect{x}_2) \odot \vect{x}_3) \odot \cdots)
    \odot \vect{x}_{\textit{ν}}),
$$

\noindent where $\odot$ is any binary operator with a suitable domain. Thus $+/\vect{x}$ is the sum, $\times/\vect{x}$ is the product, and $\vee/\vect{x}$ is the logical sum of the components of a vector $\vect{x}$. For example, $\times/\textbf{ι}^1(5) = 1 \times 2 \times 3 \times 4 \times 5$, $\times/\textbf{ι}^1(n) = n!$, and $+/\textbf{ι}^1(n) = n(n + 1)/2$.

\par As a further example, De Morgan's law may be expressed as $\wedge/\vect{u} = \overline{\vee/\overbar{\vect{u}}}$, where $\vect{u}$ is a logical vector of dimension two. Moreover, a simple inductive argument (Exercise 1.10) shows that the foregoing expression is the valid generalization of De Morgan's law for a logical vector $\vect{u}$ of arbitrary dimension.

\par A relation $\mathcal{R}$ incorporated into a relational statement $(x\mathcal{R}y)$ becomes, in effect, an operator on the variables $x$ and $y$. Consequently, the reduction $\mathcal{R}/\vect{x}$ can be defined in a manner analogous to that of $(\odot/\vect{x})$, that is,

$$
  \mathcal{R}/\vect{x} = (\cdots ((\vect{x}_1 \mathcal{R} \vect{x}_2) \mathcal{R} \vect{x}_3) \mathcal{R} \cdots) \mathcal{R} \vect{x}_{\textit{ν}}).
$$

\noindent The parentheses now imply relational statements as well as grouping. The relational reductions of practical interest are $\neq/\vect{u}$, and $=/\vect{u}$, the \textit{exclusive-or} and the \textit{equivalence} reduction, respectively.

\par The inductive argument of Exercise 1.10 shows that $\neq/\vect{u} = 2 |_0 (+/\vect{u})$. For example, if $\vect{u} = (1,0,1,1,0)$, then

\begin{align*}
  \neq/\vect{u} & = ((((1 \neq 0) \neq 1) \neq 1) \neq 0) \\
                & = (((1 \neq 1) \neq 1) \neq 0) \\
                & = ((0 \neq 1) \neq 0) \\
                & = (1 \neq 0) = 1,
\end{align*}

\noindent and $2 |_0 (+/\vect{u}) = 2 |_0 3 = 1$. Similarly, $=/\vect{u} = \overline{2 |_0 (+/\overbar{\vect{u}})}$, and as a consequence,

$$
  \neq/\vect{u} = \overline{=/\overbar{\vect{u}}},
$$

\noindent a useful companion to De Morgan's law.

\par To complete the system it is essential to define the value of $\odot/\textbf{ϵ}(0)$, the reduction of the null vector of dimension zero, as the identity element of the operator or relation $\odot$. Thus $+/\textbf{ϵ}(0) = \vee/\textbf{ϵ}(0) = 0$, and $\times/\textbf{ϵ}(0) = \wedge/\textbf{ϵ}(0) = 1$.

\par A reduction operation is extended to matrices in two ways. A \textit{row reduction} of a matrix $\mat{X}$ by an operator $\odot$ is denoted by

$$
  \vect{y} ← \odot/\mat{X}
$$

\noindent and specifies a vector $\vect{y}$ of dimension $μ(\mat{X})$ such that $\vect{y}_i = \odot/\mat{X}^i$. A \textit{column reduction} of $\mat{X}$ is denoted by $\vect{z} ← \odot/\!/\mat{X}$ and specifies a vector $\vect{z}$ of dimension $\textit{ν}(\mat{X})$ such that $\vect{z}_j = \odot/\!/\mat{X}_j$.

\par For example, if

$$
  \mat{U} = \begin{pmatrix} 1&0&1&0 \\ 0&0&1&1 \\ 1&1&1&0 \end{pmatrix}
$$

\noindent then $+/\mat{U} = (2, 2, 3)$, $+/\!/\mat{U} = (2, 1, 3, 1)$, $\wedge/\!/\mat{U} = (0, 0, 1, 0)$, $\neq/\mat{U} = (0, 0, 1)$, $=/\!/\mat{U} = (0, 1, 1, 1)$, and $+/(=/\!/\mat{U}) = 3$.

\section{Selection}

\subsection*{Compression}

\par The effective use of structured operands depends not only on generalized operations but also on the ability to specify and select certain elements or groups of elements. The selection of single elements can be indicated by indices, as in the expressions $\vect{v}_i$, $\mat{M}^i$, $\mat{M}_j$, and $\mat{M}_i^j$. Since selection is a binary operation (i.e., to select or not to select), more general selection is conveniently specified by a logical vector, each unit component indicating selection of the corresponding component of the operand.

\par The selection operation defined on an arbitrary vector $\vect{a}$ and a compatible (i.e., equal in dimension) logical vector $\vect{u}$ is denoted by $\vect{c} ← \vect{u}/\vect{a}$ and is defined as follows: the vector $\vect{c}$ is obtained from $\vect{a}$ by suppressing from $\vect{a}$ each component $\vect{a}_i$ for which $\vect{u}_i = 0$. The vector $\vect{u}$ is said to \textit{compress} the vector $\vect{a}$. Clearly $ν(\vect{c}) = +/\vect{u}$. For example, if $\vect{u} = (1, 0, 0, 0, 1, 1)$ and $\vect{a} = (\text{M}, \text{o}, \text{n}, \text{d}, \text{a}, \text{y})$, then $\vect{u}/\vect{a} =$ (\text{M}, \text{a}, \text{y}). Moreover, if $n$ is even and $\vect{v} = (2\textbf{ϵ}) |_0 \textbf{ι}^1(n) = (1, 0, 1, 0, 1, ...)$, then $\vect{v}/\textbf{ι}^1(n) = (1, 3, 5, ..., n-1)$ and $+/(\vect{v}/\textbf{ι}^1(n)) = (n/2)^2$.

\par \textit{Row compression} of a matrix, denoted by $\vect{u}/\mat{A}$, compresses each row vector $\mat{A}^i$ to form a matrix of dimension $μ(\mat{A}) \times +/\vect{u}$. \textit{Column compression}, denoted by $\vect{u}/\!/\mat{A}$, compresses each column vector $\mat{A}_j$ to form a matrix of dimension $+/\vect{u} \times ν(\mat{A})$. Compatibility conditions are $ν(\vect{u}) = ν(\mat{A})$ for row compression, and $ν(\vect{u}) = μ(\mat{A})$ for column compression. For example, if $\mat{A}$ is an arbitrary $3 \times 4$ matrix, $\vect{u} = (0, 1, 0, 1)$ and $\vect{v} = (1, 0, 1)$; then

$$
  \vect{u}/\mat{A} = \begin{pmatrix}
    \mat{A}_2^1 & \mat{A}_4^1 \\
    \mat{A}_2^3 & \mat{A}_4^3 \\
    \mat{A}_2^2 & \mat{A}_4^2
  \end{pmatrix}, \quad
  \vect{v}/\!/\mat{A} = \begin{pmatrix}
    \mat{A}_1^1 & \mat{A}_2^1 & \mat{A}_3^1 & \mat{A}_4^1 \\
    \mat{A}_1^3 & \mat{A}_2^3 & \mat{A}_3^3 & \mat{A}_4^3
  \end{pmatrix}
$$

\noindent and

$$
  \vect{u}/\vect{v}/\!/\mat{A} = \vect{v}/\!/\vect{u}/\mat{A} =
    \begin{pmatrix}
      \mat{A}_2^1 & \mat{A}_4^1 \\
      \mat{A}_2^3 & \mat{A}_4^3
    \end{pmatrix}
$$

\par It is clear that \textit{row} compression \textit{suppresses columns} corresponding to zeros of the logical vector and that \textit{column} compression \textit{suppresses rows}. This illustrates the type of confusion in nomenclature which is avoided by the convention adopted in Sec. 1.3: an operation is called a \textit{row operation} if the underlying operation from which it is generalized is applied to the row vectors of the matrix, and a \textit{column operation} if it is applied to columns.

\par \textbf{Example 1.1}. A bank makes a quarterly review of accounts to produce the following four lists:
\begin{enumerate}
  \item the name, account number, and balance for each account with a balance less than two dollars.
  \item the name, account number, and balance for each account with a negative balance exceeding one hundred dollars.
  \item the name and account number of each account with a balance exceeding one thousand dollars.
  \item all unassigned account numbers.
\end{enumerate}

\noindent The ledger may be described by a matrix

$$
  \mat{L}
    = (\mat{L}_1, \mat{L}_2, \mat{L}_3)
    = \begin{pmatrix} \mat{L}^1 \\.\\.\\.\\ \mat{L}^m \end{pmatrix}
$$

\noindent with column vectors $\mat{L}_1, \mat{L}_2$, and $\mat{L}_3$, representing names, account numbers, and balances, respectively, and with row vectors $\mat{L}^1$, $\mat{L}^2$, ..., $\mat{L}^m$, representing individual accounts. An unassigned account number is identified by the word ``none'' in the name position. The four output lists will be denoted by the matrices $\mat{P}$, $\mat{Q}$, $\mat{R}$, and $\mat{S}$, respectively. They can be produced by Program 1.9.

% (END EXAMPLE 1.1)

\par (TODO: PROGRAM 1.9)

\par \textbf{Program 1.9} Selection on bank ledger $\mat{L}$ (Example 1.1)

\par \textbf{Program 1.9}. Since $\mat{L}_3$ is the vector of balances, and $2\textbf{ϵ}$ is a compatible vector each of whose components equals two, the relational statement $(\mat{L}_3 < 2\textbf{ϵ})$ defines a logical vector having unit components corresponding to those accounts to be included in the list $\mat{P}$. Consequently, the column compression of step 1 selects the appropriate rows of $\mat{L}$ to define $\mat{P}$. Step 2 is similar, but step 3 incorporates an additional row compression by the compatible prefix vector $\mathbf{α}^2 = (1,1,0)$ to select columns one and two of $\mat{L}$. Step 4 represents the comparison of the name (in column $\mat{L}_1$) with the literal ``none'', the selection of each row which shows agreement, and the suppression of all columns but the second. The expression ``none $\textbf{ϵ}$'' occurring in step 4 illustrates the use of the extended definition of multiplication.

% (END PROGRAM 1.9 DESCRIPTION)

\subsection*{Mesh, mask, and expansion}

\par A logical vector $\vect{u}$ and the two vectors $\vect{a} = \overbar{\vect{u}}/\vect{c}$ and $\vect{b} = \vect{u}/\vect{c}$, obtained by compressing a vector $\vect{c}$, collectively determine the vector $\vect{c}$. The operation which specifies $\vect{c}$ as a function of $\vect{a}$, $\vect{b}$, and $\vect{u}$ is called a \textit{mesh} and is defined as follows: If $\vect{a}$ and $\vect{b}$ are arbitrary vectors and if $\vect{u}$ is a logical vector such that $+/\overbar{\vect{u}} = \textit{ν}(\vect{a})$ and $+/\vect{u} = \textit{ν}(\vect{b})$, then the \textit{mesh of} $\vect{a}$ \textit{and} $\vect{b}$ \textit{on} $\vect{u}$ is denoted by $\backslash\vect{a}, \vect{u}, \vect{b}\backslash$ and is defined as the vector $\vect{c}$ such that $\overbar{\vect{u}}/\vect{c} = \vect{a}$ and $\vect{u}/\vect{c} = \vect{b}$. The mesh operation is equivalent to choosing successive components of $\vect{c}$ from $\vect{a}$ or $\vect{b}$ according as the successive components of $\vect{u}$ are $0$ or $1$. If, for example, $\vect{a} = (\text{s}, \text{e}, \text{k})$, $\vect{b} = (\text{t}, \text{a})$, and $\vect{u} = (0, 1, 0, 1, 0)$, then $\backslash\vect{a}, \vect{u}, \vect{b}\backslash = (\text{s}, \text{t}, \text{e}, \text{a}, \text{k})$. As a further example, Program 1.10a (which describes the merging of the vectors $\vect{a}$ and $\vect{b}$, with the first and every third component thereafter chosen from $\vect{a})$ can be described alternatively as shown in Program 1.10b. Since $\textbf{ι}^1 = (1, 2, 3, 4, 5, 6, ...)$, then $(3\textbf{ϵ}) |_0 \textbf{ι}^1 = (1, 2, 0, 1, 2, 0, ...)$, and consequently the vector $\vect{u}$ specified by step 1 is of the form $\vect{u} = (0, 1, 1, 0, 1, 1, 0, ...)$.

\par (TODO: PROGRAM 1.10)

\par \textbf{Program 1.10} Interfiling program

\par Mesh operations on matrices are defined analogously, row mesh and column mesh being denoted by single and double reverse virgules, respectively.

\par The \textit{catenation} of vectors $\vect{x}, \vect{y}, ..., \vect{z}$ is denoted by $\vect{x} \oplus \vect{y} \oplus \cdots \oplus \vect{z}$ and is defined by the relation

$$
  \vect{x} \oplus \vect{y} \oplus \cdots \oplus \vect{z}
    = (\vect{x}_1, \vect{x}_2, ...,
       \vect{x}_{ν(\vect{x})}, \vect{y}_1, \vect{y}_2, ...,
       \vect{z}_{ν(\vect{z})}).
$$

\noindent Catenation is clearly associative and for two vectors $\vect{x}$ and $\vect{y}$ it is a special case of the mesh $\backslash\vect{x}, \vect{u}, \vect{y}\backslash$ in which $\vect{u}$ is a suffix vector.

\par In numerical vectors (for which addition of two vectors is defined), the effect of the general mesh operation can be produced as the sum of two meshes, each involving one zero vector. Specifically,

\begin{alignat*}{2}
  \backslash\vect{x}, \vect{u}, \vect{y}\backslash
    & = \backslash\vect{x}, \vect{u}, 0\backslash
    & + \backslash0, \vect{u}, \vect{y}\backslash
  \\& = \backslash0, \overbar{\vect{u}}, \vect{x}\backslash
    & + \backslash0, \vect{u}, \vect{y}\backslash.
\end{alignat*}

\noindent The operation $\backslash0, \vect{u}, \vect{y}\backslash$ proves very useful in numerical work and will be called \textit{expansion} of the vector $\vect{y}$, denoted by $\vect{u}\backslash\vect{y}$. Compression of $\vect{u}\backslash\vect{y}$ by $\vect{u}$ and by $\overbar{\vect{u}}$ clearly yields $\vect{y}$ and 0, respectively. Moreover, any numerical vector $\vect{x}$ can be \textit{decomposed} by a compatible vector $\vect{u}$ according to the relation

$$
  \vect{x}
    = \overbar{\vect{u}}\backslash\overbar{\vect{u}}/\vect{x}
    + \vect{u}\backslash\vect{u}/\vect{x}.
$$

\noindent The two terms are vectors of the same dimension which have no nonzero components in common. Thus if $\vect{u} = (1, 0, 1, 0, 1)$, the decomposition of $\vect{x}$ appears as

$$
  \vect{x} = (0, \vect{x}_2, 0, \vect{x}_4, 0) + (\vect{x}_1, 0, \vect{x}_3, 0, \vect{x}_5).
$$

\par Row expansion and column expansion of matrices are defined and denoted analogously. The decomposition relations become

\begin{align*}
  & \mat{X}
    = \overbar{\vect{u}}\backslash\overbar{\vect{u}}/\mat{X}
    + \vect{u}\backslash\vect{u}/\mat{X},\\
\text{and}
  & \mat{X}
    = \overbar{\vect{u}}\backslash\backslash\overbar{\vect{u}}/\!/\mat{X}
    + \vect{u}\backslash\backslash\vect{u}/\!/\mat{X}.
\end{align*}

\par The \textit{mask} operation is defined formally as follows:

$$
  \vect{c} ← /\vect{a}, \vect{u}, \vect{b}/
  \leftrightarrow 
  \overbar{\vect{u}}/\vect{c} = \overbar{\vect{u}}/\vect{a},
  \quad\text{and}\quad
  \vect{u}/\vect{c} = \vect{u}/\vect{b}.
$$

\noindent The vectors $\vect{c}$, $\vect{a}$, $\vect{u}$, and $\vect{b}$ are clearly of a common dimension and $\vect{c}_i = \vect{a}_i$ or $\vect{b}_i$ according as $\vect{u}_i = 0$ or $\vect{u}_i = 1$. Moreover, the compress, expand, mask, and mesh operations on vectors are related as follows:

\begin{align*}
  /\vect{a}, \vect{u}, \vect{b}/
    & = \backslash\overbar{\vect{u}}/\vect{a}, \vect{u}, \vect{u}/\vect{b}\backslash, \\
  \backslash\vect{a}, \vect{u}, \vect{b}\backslash
    & = /\overbar{\vect{u}}\backslash\vect{a}, \vect{u}, \vect{u}\backslash\vect{b}/. \\
\end{align*} 

\noindent Analogous relations hold for the row mask and row mesh and for the column mask and column mesh.

\par Certain selection operations are controlled by logical matrices rather than by logical vectors. The \textit{row compression} $\mat{U}/\mat{A}$ selects elements of $\mat{A}$ corresponding to the nonzero elements of $\mat{U}$. Since the nonzero elements of $\mat{U}$ may occur in an arbitrary pattern, the result must be construed as a vector rather than a matrix. More precisely, $\mat{U}/\mat{A}$ denotes the catenation of the vectors $\mat{U}^i/\mat{A}^i$ obtained by row-by-row compression of $\mat{A}$ by $\mat{U}$. The \textit{column compression} $\mat{U}/\!/\mat{A}$ denotes the catenation of the vectors $\mat{U}_j/\mat{A}_j$. If, for example

$$
  \mat{U} = \begin{pmatrix}
    0&1&0&1&1 \\
    1&1&0&0&0 \\
    0&1&1&0&0
  \end{pmatrix}
$$
\noindent then
$$ \mat{U}/\mat{A} = (\mat{A}_2^1, \mat{A}_4^1, \mat{A}_5^1, \mat{A}_1^2, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3), $$
\noindent and
$$ \mat{U}/\!/\mat{A} = (\mat{A}_1^2, \mat{A}_2^1, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3, \mat{A}_4^1, \mat{A}_5^1). $$

\par Compression by the full matrix $\mathsf{\mat{E}}$ (defined by $\overbar{\mathsf{\mat{E}}} = 0$) produces either a \textit{row list} ($\mathsf{\mat{E}}/\mat{A}$) or a \textit{column list} ($\mathsf{\mat{E}}/\!/\mat{A}$) of the matrix $\mat{A}$. Moreover, a numerical matrix $\mat{X}$ can be represented jointly by the logical matrix $\mat{U}$ and the row list $\mat{U}/\mat{X}$ (or the column list $\mat{U}/\!/\mat{X}$), where $\mat{U} = (\mat{X} \neq 0)$. If the matrix $\mat{X}$ is sparse (i.e., the components are predominantly zero), this provides a compact representation which may reduce the computer storage required for $\mat{X}$.

\par The compression operations controlled by matrices also generate a group of corresponding mesh and mask operations as shown in Sec. S.9.

\section{Selection vectors}

\par The logical vector $\vect{u}$ involved in selection operations may itself arise in various ways. It may be a prefix vector $\mathbf{α}^j$, a suffix $\textbf{ω}^j$, or an infix $(i ↓ \mathbf{α}^j)$; the corresponding compressed vectors $\mathbf{α}^j/\vect{x}, \textbf{ω}^j/\vect{x}$, and $(i ↓ \mathbf{α}^j)/\vect{x}$ are called a \textit{prefix}, \textit{suffix}, and \textit{infix} of $\vect{x}$, respectively.

\par Certain selection vectors arise as functions of other vectors, e.g., the vector $(\vect{x} \geq$ 0) can be used to select all nonnegative components of $\vect{x}$, and $(\vect{b} \neq *\textbf{ϵ})$ serves to select all components of $\vect{b}$ which are not equal to the literal ``*''. Two further types are important: the selection of the longest unbroken prefix (or suffix) of a given logical vector, and the selection of the set of distinct components occurring in a vector. The first is useful in left (or right) justification or in a corresponding compression intended to eliminate leading or trailing ``filler components'' of a vector (such as left zeros in a number or right spaces in a short name).

\par For any logical vector $u$, the \textit{maximum prefix} of $\vect{u}$ is denoted by $α/\vect{u}$ and defined as follows:

$$
  \vect{v} ← α/\vect{u} \leftrightarrow \vect{v} = \mathbf{α}^j
$$

\noindent where $j$ is the maximum value for which $\wedge/(\mathbf{α}^j/\vect{u}) = 1$. The maximum suffix is denoted by $ω/\vect{u}$ and is defined analogously. If, for example, $\vect{u} = (1, 1, 1, 0, 1, 1, 0, 0, 1, 1)$, then $α/\vect{u} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0)$, $ω/\vect{u} = (0, 0, 0, 0, 0, 0, 0, 0, 1, 1)$, $+/α/\vect{u} = 3$, and $+/ω/\vect{u} = 2$.

\par The leading zeros of a numerical vector $x$ can clearly be removed either by compression:

$$
  \vect{y} ← \overline{(α/(\vect{x} = 0))}/\vect{x}
$$

\noindent or by left justification (normalization):

$$
  \vect{z} ← (+/α/(\vect{x} = 0)) ↑ \vect{x}.
$$

\par The extension of the maximum prefix operation to the rows of a logical matrix $\mat{U}$ is denoted by $α/\mat{U}$ and defined as the compatible logical matrix $\mat{V}$, such that $\mat{V}^i = α/\mat{U}^i$. The corresponding maximum column prefix operation is denoted by $α/\!/\mat{U}$. Right justification of a numerical matrix $\mat{X}$ is achieved by the rotation $\vect{k} ↓ \mat{X}$, where $\vect{k} = +/ω/(\mat{X} = 0)$, and \textit{top justification} is achieved by the rotation $(+/\!/α/\!/(\mat{X} = 0)) \Uparrow \mat{X}$ (see Sec. S.6.)

\par A vector whose components are all distinct will be called an \textit{ordered set}. The \textit{forward set selector} on $\vect{b}$ is a logical vector denoted by $σ/\vect{b}$ and defined as follows: the statement $\vect{v} ← σ/\vect{b}$ implies that $\vect{v}_j = 1$ if and only if $\vect{b}_j$ differs from all preceding components of $\vect{b}$. Hence $\vect{v}/\vect{b}$ is a set which contains all distinct components of $\vect{b}$, and $+/\vect{v}/\textbf{ι}$ is a minimum. For example, if $\vect{c} = (\text{C}, \text{a}, \text{n}, \text{a}, \text{d}, \text{a})$, then $(σ/\vect{c})/\vect{c} = (\text{C}, \text{a}, \text{n}, \text{d})$ is a list of the distinct letters in $\vect{c}$ in order of occurrence. Clearly $(σ/\vect{b})/\vect{b} = \vect{b}$ if and only if $\vect{b}$ is a set.

\par The backward set selector $τ/\vect{b}$ is defined analogously (e.g., $(τ/\vect{c})/\vect{c} = (\text{C}, \text{n}, \text{d}, \text{a})$). Forward and backward set selection are extended to matrices by both rows ($σ/\mat{B}$, and $τ/\mat{B}$) and columns ($σ/\!/\mat{B}$, and $τ/\!/\mat{B}$) in the established manner.

\section{The generalized matrix product}

\par The ordinary matrix product of matrices $\mat{X}$ and $\mat{Y}$ is commonly denoted by $\vect{XY}$ and defined as follows:

\begin{equation*}
  \begin{split}
    \mat{Z} ← \vect{X}\vect{Y} \leftrightarrow \mat{Z}_j^i
      = \sum_{k=1}^{ν(\mat{X})} \mat{X}_k^i \times \mat{Y}_j^k,
  \end{split}
\quad\quad
  \begin{split}
    i &= 1, 2, ..., μ(\mat{X}) \\
    j &= 1, 2, ..., ν(\mat{Y}).
  \end{split}
\end{equation*}

\noindent It can be defined alternatively as follows:

$$
  (\vect{X}\vect{Y})_j^i = +/(\mat{X}^i \times \mat{Y}_j).
$$

\par This formulation emphasizes the fact that matrix multiplication incorporates two elementary operations ($+$, $\times$) and suggests that they be displayed explicitly. The ordinary matrix product will therefore be written as $\mat{X} {+ \atop \times} \mat{Y}$.

\par More generally, if $\odot_1$ and $\odot_2$ are any two operators (whose domains include the relevant operands), then the \textit{generalized matrix product} $\mat{X} {\odot_1 \atop \odot_2} \mat{Y}$ is defined as follows:

\begin{equation*}
  \begin{split}
    (\mat{X} {\odot_1 \atop \odot_2} \mat{Y})_j^i
      = \odot_1/(\mat{X}^i \odot_2 \mat{Y}_j),
  \end{split}
\quad\quad
  \begin{split}
    i &= 1, 2, ..., μ(\mat{X}) \\
    j &= 1, 2, ..., ν(\mat{Y}).
  \end{split}
\end{equation*}

\noindent For example, if

$$
  \mat{A} = \begin{pmatrix}
    1&3&2&0 \\
    2&1&0&1 \\
    4&0&0&2
  \end{pmatrix} \quad\text{and}\quad \mat{B} = \begin{pmatrix}
    4&1 \\
    0&3 \\
    0&2 \\
    2&0
  \end{pmatrix}
$$

\noindent then

$$
  \mat{A} {+ \atop \times} \mat{B} = \begin{pmatrix}
     4&14 \\
    10& 5 \\
    20& 4
  \end{pmatrix}, \quad
  \mat{A} {\wedge \atop =} \mat{B} = \begin{pmatrix}
    0&1 \\
    0&0 \\
    1&0
  \end{pmatrix}
$$

$$
  \mat{A} {\vee \atop \neq} \mat{B} = \begin{pmatrix}
    1&0 \\
    1&1 \\
    0&1
  \end{pmatrix}, \quad\text{and}\quad
  (\mat{A} \neq 0) {+ \atop /} \mat{B} = \begin{pmatrix} 
    4&6 \\
    6&4 \\
    6&1
  \end{pmatrix}
$$

\par The generalized matrix product and the selection operations together provide an elegant formulation in several established areas of mathematics. A few examples will be chosen from two such areas, symbolic logic and matrix algebra.

\par In symbolic logic, De Morgan's laws ($\wedge/\vect{u} = \overline{\vee/\overbar{\vect{u}}}$ and $=/\vect{u} = \overline{\neq/\overbar{\vect{u}}}$) can be applied directly to show that

$$
  \mat{U} {\neq \atop \wedge} \mat{V} = \overline{\overbar{\mat{U}} {= \atop \vee} \overbar{\mat{V}}}.
$$

\par In matrix algebra, the notion of partitioning a matrix into submatrices of contiguous rows and columns can be generalized to an arbitrary partitioning specified by a logical vector $\vect{u}$. The following easily verifiable identities are typical of the useful relations which result:

\begin{align*}
  \mat{X} {+ \atop \times} \mat{Y} &= (\overbar{\vect{u}}/\mat{X}) {+ \atop \times} (\overbar{\vect{u}}/\!/\mat{Y}) + (\vect{u}/\mat{X}) {+ \atop \times} (\vect{u}/\!/\mat{Y}), \\
  \vect{u}/(\mat{X} {+ \atop \times} \mat{Y}) &= \mat{X} {+ \atop \times} (\vect{u}/\mat{Y}), \\
  \vect{u}/\!/(\mat{X} {+ \atop \times} \mat{Y}) &= (\vect{u}/\!/\mat{X}) {+ \atop \times} \mat{Y}. \\
\end{align*}

\par The first identity depends on the commutativity and associativity of the operator $+$ and can clearly be generalized to other associative commutative operators, such as $\wedge$, $\vee$, and $\neq$.

\par The generalized matrix product applies directly (as does the ordinary matrix product $\mat{X} {+ \atop \times} \mat{Y}$) to vectors considered as row (that is, $1 \times n$) or as column matrices. Thus:

\begin{alignat*}{2}
  \vect{z} ← \mat{X}  {\odot_1 \atop \odot_2} \vect{y} & \leftrightarrow \vect{z}_{i} &= \odot_1/(\mat{X}^{i} \odot_2 \vect{y}), \\
  \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \mat{X}  & \leftrightarrow \vect{z}_{j} &= \odot_1/(\vect{y} \odot_2 \mat{X}_{j}), \\
  \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \vect{x} & \leftrightarrow \vect{z}     &= \odot_1/(\vect{y} \odot_2 \vect{x}).
\end{alignat*}

\par The question of whether a vector enters a given operation as a row vector or as a column vector is normally settled by the requirement of conformability, and no special indication is required. Thus $\vect{y}$ enters as a column vector in the first of the preceding group of definitions and as a row vector in the last two. The question remains, however, in the case of the two vector operands, which may be considered with the pre-operand either as a row (as in the scalar product $\vect{y} {+ \atop \times} \vect{x}$) or as a column. The latter case produces a matrix $\mat{Z}$ and will be denoted by

$$
  \mat{Z} ← \vect{y} {\circ \atop \odot_2} \vect{x},
$$

\par where $\mat{Z}_j^i = \vect{y}_i \odot_2 \vect{x}_j$, $μ(\mat{Z}) = ν(\vect{y})$, and $ν(\mat{Z}) = ν(\vect{x})$.%^{<a href="#note1b">[b]</a>}
For example, if each of the vectors indicated is of dimension three, then

$$
  \textbf{ϵ} {\circ \atop \times} \vect{y} = \begin{pmatrix}
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3 \\
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3 \\
    \vect{y}_1 & \vect{y}_2 & \vect{y}_3
  \end{pmatrix}; \quad
  \vect{y} {\circ \atop \times} \textbf{ϵ} = \begin{pmatrix}
    \vect{y}_1 & \vect{y}_1 & \vect{y}_1 \\
    \vect{y}_2 & \vect{y}_2 & \vect{y}_2 \\
    \vect{y}_3 & \vect{y}_3 & \vect{y}_3
  \end{pmatrix}
$$
$$
  \mathbf{α}^2(3) {\circ \atop \wedge} \mathbf{α}^2(3) = \begin{pmatrix}
    1&1&0 \\
    1&1&0 \\
    0&0&0
  \end{pmatrix}
$$
