<html>
<head><meta http-equiv="content-type" content="text/html;charset=utf-8">
<title>A Programming Language</title>
<link href="adoc.css" rel=stylesheet>
</head>

<body>

<table width=520 align=center>

<table border=1 cellspacing=0 cellpadding=12 align=center>

<img src="APLimg/title.jpg">

\end{tabularx}

\par $\\$
\textit{To My Many Teachers}

<a name="preface"></a>
\par $\large{Preface}$

\par Applied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or \textit{programs}. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a \textit{programming language}.

\par Much of applied mathematics, particularly the more recent computer-related areas which cut across the older disciplines, suffers from the lack of an adequate programming language. It is the central thesis of this book that the descriptive and analytic power of an adequate programming language amply repays the considerable effort required for its mastery. This thesis is developed by first presenting the entire language and then applying it in later chapters to several major topics.

\par The areas of application are chosen primarily for their intrinsic interest and lack of previous treatment, but they are also designed to illustrate the universality and other facets of the language. For example, the microprogramming of Chapter 2 illustrates the divisibility of the language, i.e., the ability to treat a restricted area using only a small portion of the complete language. Chapter 6 (Sorting) shows its capacity to compass a relatively complex and detailed topic in a short space. Chapter 7 (The Logical Calculus) emphasizes the formal manipulability of the language and its utility in theoretical work.

\par The material was developed largely in a graduate course given for several years at Harvard and in a later course presented repeatedly at the IBM Systems Research Institute in New York. It should prove suitable for a two-semester course at the senior or graduate level. Although for certain audiences an initial presentation of the entire language may be appropriate, I have found it helpful to motivate the development by presenting the minimum notation required for a given topic, proceeding to its treatment (e.g., microprogramming), and then returning to further notation. The 130-odd problems not only provide the necessary finger exercises but also develop results of general interest.

\par Chapter 1 or some part of it is prerequisite to each of the remaining ``applications'' chapters, but the applications chapters are virtually independent of one another. A complete appreciation of search techniques (Chapter 4) does, however, require a knowledge of methods of representation (Chapter 3). The cross references which do occur in the applications chapters are either nonessential or are specific to a given figure, table, or program. The entire language presented in Chapter 1 is summarized for reference at the end of the book.

\par In any work spanning several years it is impossible to acknowledge adequately the many contributions made by others. Two major acknowledgments are in order: the first to Professor Howard Aiken, Director Emeritus of the Harvard Computation Laboratory, and the second to Dr. F.P. Brooks, Jr. now of IBM.

\par It was Professor Aiken who first guided me into this work and who provided support and encouragement in the early years when it mattered. The unusually large contribution by Dr. Brooks arose as follows. Several chapters of the present work were originally prepared for inclusion in a joint work which eventually passed the bounds of a single book and evolved into our joint \textit{Automatic Data Processing} and the present volume. Before the split, several drafts of these chapters had received careful review at the hands of Dr. Brooks, reviews which contributed many valuable ideas on organization, presentation, and direction of investigation, as well as numerous specific suggestions.

\par The contributions of the 200-odd students who suffered through the development of the material must perforce be acknowledged collectively, as must the contributions of many of my colleagues at the Harvard Computation Laboratory. To Professor G.A. Salton and Dr. W.L. Eastman, I am indebted for careful reading of drafts of various sections and for comments arising from their use of some of the material in courses. Dr. Eastman, in particular, exorcised many subtle errors from the sorting programs of Chapter 6. To Professor A.G. Oettinger and his students I am indebted for many helpful discussions arising out of his early use of the notation. My debt to Professor R.L. Ashenhurst, now of the University of Chicago, is apparent from the references to his early (and unfortunately unpublished) work in sorting.

\par Of my colleagues at the IBM Research Center, Messrs. L.R. Johnson and A.D. Falkoff, and Dr. H. Hellerman have, through their own use of the notation, contributed many helpful suggestions. I am particularly indebted to L.R. Johnson for many fruitful discussions on the applications of trees, and for his unfailing support.

\par On the technical side, I have enjoyed the assistance of unusually competent typists and draughtsmen, chief among them being Mrs. Arthur Aulenback, Mrs. Philip J. Seaward, Jr., Mrs. Paul Bushek, Miss J.L. Hegeman, and Messrs. William Minty and Robert Burns. Miss Jacquelin Sanborn provided much early and continuing guidance in matters of style, format, and typography. I am indebted to my wife for assistance in preparing the final draft.

<table width=500>
<td align=right>Kenneth E. Iverson & \\
\textit{May, 1962} & \\
\textit{Mount Kisco, New York} & \\
\end{tabularx}

<a name="1"></a>
\par $\large{Chapter$ 1 The Language}

<a name="1.1"></a>
\par \textbf{1.1 Introduction}

\par Applied mathematics is concerned with the design and analysis of algorithms or \textit{programs}. The systematic treatment of complex algorithms requires a suitable \textit{programming language} for their description, and such a programming language should be concise, precise, consistent over a wide area of application, mnemonic, and economical of symbols; it should exhibit clearly the constraints on the sequence in which operations are performed; and it should permit the description of a process to be independent of the particular representation chosen for the data.

\par Existing languages prove unsuitable for a variety of reasons. Computer coding specifies sequence constraints adequately and is also comprehensive, since the logical functions provided by the branch instructions can, in principle, be employed to synthesize any finite algorithm. However, the set of basic operations provided is not, in general, directly suited to the execution of commonly needed processes, and the numeric symbols used for variables have little mnemonic value. Moreover, the description provided by computer coding depends directly on the particular representation chosen for the data, and it therefore cannot serve as a description of the algorithm per se.

\par Ordinary English lacks both precision and conciseness. The widely used 
<acronym title="Goldstine, H.H., and J. von Neumann (1947), “Planning and Coding of Problems for an Electronic Computing Instrument”, Report on the Mathematical and Logical Aspects of an Electronic Computing Instrument, Part II, vol. 1, Institute for Advanced Study, Princeton."> Goldstine-von Neumann (1947)</acronym> flowcharting provides the conciseness necessary to an over-all view of the processes, only at the cost of suppressing essential detail. The so-called pseudo-English used as a basis for certain automatic programming systems suffers from the same defect. Moreover, the potential mnemonic advantage in substituting familiar English words and phrases for less familiar but more compact mathematical symbols fails to materialize because of the obvious but unwonted precision required in their use.

\par Most of the concepts and operations needed in a programming language have already been defined and developed in one or another branch of mathematics. Therefore, much use can and will be made of existing notations. However, since most notations are specialized to a narrow field of discourse, a consistent unification must be provided. For example, separate and conflicting notations have been developed for the treatment of sets, logical variables, vectors, matrices, and trees, all of which may, in the broad universe of discourse of data processing, occur in a single algorithm.

<a name="1.2"></a>
\par \textbf{1.2 Programs}

\par A \textit{program statement} is the specification of some quantity or quantities in terms of some finite operation upon specified operands. Specification is symbolized by an arrow directed toward the specified quantity. thus $``y$ is specified by $\sin x''$ is a statement denoted by

\par $y ← \sin x$.

\par A set of statements together with a specified order of execution constitutes a \textit{program}. The program is \textit{finite} if the number of executions is finite. The \textit{results} of the program are some subset of the quantities specified by the program. The \textit{sequence} or order of execution will be defined by the order of listing and otherwise by arrows connecting any statement to its successor. A cyclic sequence of statements is called a \textit{loop}.

\begin{tabularx}
 & <td valign=bottom align=center><img src="APLimg/prog1x1.bmp"> & & <td valign=bottom align=center><img src="APLimg/prog1x2.bmp"> & \\
 & <td align=center>\textbf{Program 1.1} Finite Program & & <td align=center>\textbf{Program 1.2} Infinite Program & \\
\end{tabularx}

\par Thus Program 1.1 is a program of two statements defining the result $v$ as the (approximate) area of a circle of radius $x$, whereas Program 1.2 is an infinite program in which the quantity $z$ is specified as $(2y)^n$ on the $n$th execution of the two statement loop. Statements will be numbered on the left for reference.

\par A number of similar programs may be subsumed under a single more general program as follows. At certain \textit{branch points} in the program a finite number of alternative statements are specified as possible successors. One of these successors is chosen according to criteria determined in the statement or statements preceding the branch point. These criteria are usually stated as a \textit{comparison} or test of a specified relation between a specified pair of quantities. A branch is denoted by a set of arrows leading to each of the alternative successors, with each arrow labeled by the comparison condition under which the corresponding successor is chosen. The quantities compared are separated by a colon in the statement at the branch point, and a labeled branch is followed if and only if the relation indicated by the label holds when substituted for the colon. The conditions on the branches of a properly defined program must be disjoint and exhaustive.

\par Program 1.3 illustrates the use of a branch point. Statement $α5$ is a comparison which determines the branch to statements $β1, δ1$, or $γ1$, according as $z > n, z = n$, or $z < n$. The program represents a crude by effective process for determining $x = n^{2/3}$ for any positive cube $n$.

<img src="APLimg/prog1x3.bmp">
\par \textbf{Program 1.3} Program for $x = n^{2/3}$

\par Program 1.4 shows the preceding program reorganized into a compact linear array and introduces two further conventions on the labeling of branch points. The listed successor of a branch statement is selected if none of the labeled conditions is met. Thus statement 6 follows statement 5 if neither of the arrows (to exit or to statement 8) are followed, i.e. if $z < n$. Moreover, any unlabeled arrow is always followed; e.g., statement 7 is invariably followed by statement 3, never by statement 8.

\par A program begins at a point indicated by an \textit{entry arrow} (step 1) and ends at a point indicated by an \textit{exit arrow} (step 5). There are two useful consequences of confining a program to the form of a linear array: the statements may be referred to by a unique serial index (statement number), and unnecessarily complex organization of the program manifests itself in crossing branch lines. The importance of the latter characteristic in developing clear and comprehensible programs is not sufficiently appreciated.

\begin{tabularx}
 & <td align=center><img src="APLimg/prog1x4.bmp"> & \\
 & <td align=center>\textbf{Program 1.4} Linear arrangement of Program 1.3 & \\
\end{tabularx}

\begin{tabularx}
 & <td align=center><img src="APLimg/prog1x5.bmp"> & \\
 & <td align=center>\textbf{Program 1.5} Matrix multiplication & \\
\end{tabularx}

\par A process which is repeated a number of times is said to be \textit{iterated}, and a process (such as in Program 1.4) which includes one or more iterated subprocesses is said to be \textit{iterative}. Program 1.5 shows an iterative process for the matrix multiplication

\par $\mat{C} ← \mat{A}\mat{B}$

\par defined in the usual way as

\begin{tabularx}
 & \mat{C}_{\textit{j}}^{\textit{i}} = <img align=middle src="APLimg/sigmamm.bmp"> \mat{A}_{\textit{k}}^{\textit{i}} \times \mat{B}_{\textit{j}}^{\textit{k}} ,

 & \textit{i} = 1,2, ..., \textit{μ}(\mat{A}),\\
 \textit{j} = 1,2, ..., \textit{ν}(\mat{B}),

\end{tabularx}

\par where the dimensions of an $m \times n$ matrix $\mat{X}$ (of $m$ rows and $n$ columns) is denoted by $μ(\mat{X}) \times ν(\mat{X})$.

\begin{tabularx} & 
\par \textbf{Program 1.5}. Steps 1-3 initialize the indices, and the loop 5-7 continues to add successive products to the partial sum until $k$ reaches zero. When this occurs, the process continues through step 8 to decrement $j$ and to repeat the entire summation for the new value of $j$, providing that it is not zero. If $j$ is zero, the branch to step 10 decrements $i$ and the entire process over $j$ and $k$ is repeated from $j = ν(\mat{B})$, providing that $i$ is not zero. If $i$ is zero, the process is complete, as indicated by the exit arrow.
 & & \\\end{tabularx}

\par In all examples used in this chapter, emphasis will be placed on clarity of description of the process, and considerations of efficient execution by a computer or class of computers will be subordinated. These considerations can often be introduced later by relatively routine modifications of the program. For example, since the execution of a computer operation involving an indexed variable is normally more costly than the corresponding operation upon a nonindexed variable, the substitution of a variable $s$ for the variable $\mat{C}_j^i$ specified by statement 5 of Program 1.5 would accelerate the execution of the loop. The variable $s$ would be initialized to zero before each entry to the loop and would be used to specify $\mat{C}_j^i$ at each termination.

\par The practice of first setting an index to its maximum value and then decrementing it (e.g., the index $k$ in Program 1.5) permits the termination comparison to be made with zero. Since zero often occurs in comparisons, it is convenient to omit it. Thus, if a variable stands alone at a branch point, comparison with zero is implied. Moreover, since a comparison on an index frequently occurs immediately after it is modified, a branch at the point of modification will denote branching upon comparison of the indicated index with zero, the comparison occurring \textit{after} modification. Designing programs to execute decisions immediately after modification of the controlling variable results in efficient execution as well as notational elegance, since the variable must be present in a central register for both operations.

\par Since the sequence of execution of statements is indicated by connecting arrows as well as by the order of listing, the latter can be chosen arbitrarily. This is illustrated by the functionally identical Programs 1.3 and 1.4. Certain principles of ordering may yield advantages such as clarity or simplicity of the pattern of connections. Even though the advantages of a particular organizing principle are not particularly marked, the uniformity resulting from its consistent application will itself be a boon. The scheme here adopted is called the \textit{method of leading decisions}: the decision on each parameter is placed as early in the program as practicable, normally just before the operations indexed by the parameter. This arrangement groups at the head of each iterative segment the initialization, modification, and the termination test of the controlling parameter. Moreover, it tends to avoid program flaws occasioned by unusual values of the argument.

\begin{tabularx}
 & <td align=center><img src="APLimg/prog1x6.bmp"> & \\
 & <td align=center>\textbf{Program 1.6} Matrix multiplication using leading decisions & \\
\end{tabularx}

\par For example, Program 1.6 (which is a reorganization of Program 1.5) behaves properly for matrices of dimension zero, whereas Program 1.5 treats every matrix as if it were of dimension one or greater.

\par Although the labeled arrow representation of program branches provides a complete and graphic description, it is deficient in the following respects: (1) a routine translation to another language (such as computer code) would require the tracing of arrows, and (2) it does not permit programmed modification of the branches.

\par The following alternative form of a branch statement will therefore be used as well:

\par $x : y, \vect{r} → \vect{s}$.

\par This denotes a branch to statement number $\vect{s}_i$ of the program if the relation $x\vect{r}_iy$ holds. The parameters $\vect{r}$ and $\vect{s}$ may themselves be defined and redefined in other parts of the program. The \textit{null element} $∘$ will be used to denote the relation which complements the remaining relations $\vect{r}_i$; in particular, $(∘)→(s)$, or simply $→s$, will denote an unconditional branch to statement $s$. Program 1.7 shows the use of these conventions in a reformulation of Program 1.6. More generally, two or more otherwise independent programs may interact through a statement in one program specifying a branch in a second. The statement number occurring in the branch must then be augmented by the name of the program in which the branch is effected. Thus the statement $(∘) →$ Program 2.24 executed in Program 1 causes a branch to step 24 to occur in Program 2.

\begin{tabularx}
 & <td align=center><img src="APLimg/prog1x7.bmp"> & \\
 & <td align=center>\textbf{Program 1.7} A reformulation of Program 1.6,\\
 using an algebraic statement of the branching & \\
\end{tabularx}

\par One statement in a program can be modified by another statement which changes certain of its parameters, usually indices. More general changes in statements can be effected by considering the program itself as a vector $\vect{p}$ whose components are the individual, serially numbered statements. All the operations to be defined on general vectors can then be applied to the statements themselves. For example, the $j$th statement can be respecified by the $i$th through the occurrence of the statement $\vect{p}_j ← \vect{p}_i$.

\par The interchange of two quantities $y$ and $x$ (that is, $x$ specifies $y$ and the \textit{original} value of $y$ specifies $x)$ will be denoted by the statement $y \leftrightarrow x$.

<a name="1.3"></a>
\par \textbf{1.3 Structure of the language}

<a name="1.3.1"></a>
\par \textbf{Conventions}

\par The Summary of Notation at the end of the book summarizes the notation developed in this chapter. Although intended primarily for reference, it supplements the text in several ways. It frequently provides a more concise alternative definition of an operation discussed in the text, and it also contains important but easily grasped extensions not treated explicitly in the text. By grouping the operations into related classes it displays their family relationships.

\par A concise programming language must incorporate families of operations whose members are related in a systematic manner. Each family will be denoted by a specific operation symbol, and the particular member of the family will be designated by an associated \textit{controlling parameter} (scalar, vector, matrix, or tree) which immediately precedes the main operation symbol. The operand is placed immediately after the main operation symbol. For example, the operation $k ↑ \vect{x}$ (left rotation of $\vect{x}$ by $k$ places) may be viewed as the $k$th member of the set of rotation operators denoted by the symbol $↑$.

\par Operations involving a single operand and no controlling parameter (such as $⌊x⌋$, or $⌈x⌉)$ will be denoted by a pair of operation symbols which enclose the operand. Operations involving two operands and a controlling parameter (such as the mask operation $/\vect{a}, \vect{u}, \vect{b}/)$ will be denoted by a pair of operation symbols enclosing the entire set of variables, and the controlling parameter will appear between the two operands. In these cases the operation symbols themselves serve as grouping symbols.

\par In interpreting a compound operation such as $k ↑ (j ↓ \vect{x})$ it is important to recognize that the operation symbol and its associated controlling parameter together represent an indivisible operation and must not be separated. It would, for example, be incorrect to assume that $j ↑ (k ↓ \vect{x})$ were equivalent to $k ↑ (j ↓ \vect{x})$, although it can be shown that the complete operations $j ↓$ and $k ↑$ do commute, that is $k ↑ (j ↓ \vect{x}) = j ↓ (k ↑ \vect{x})$.

\par The need for parentheses will be reduced by assuming that compound statements are, except for intervening parentheses, executed from right to left. Thus $k ↑ j ↓ \vect{x}$ is equivalent to $k ↑ (j ↓ \vect{x})$, not to $(k ↑ j) ↓ \vect{x}$.

\par Structured operands such as vectors and matrices, together with a systematic component-by-component generalization of elementary operations, provide an important subordination of detail in the description of algorithms. The use of structured operands will be facilitated by \textit{selection operations} for extracting a specified portion of an operand, \textit{reduction operations} for extending an operation (such as logical or arithmetic multiplication) over all components, and \textit{permutation operations} for reordering components. Operations defined on vectors are extended to matrices: the extended operation is called a \textit{row} operation if the underlying vector operation is applied to each row of the matrix and a \textit{column} operation if it is applied to each column. A column operation is denoted by doubling the symbol employed for the corresponding row (and vector) operation.

\par A distinct typeface will be used for each class of operand as detailed in Table 1.8. Special quantities (such as the prefix vectors $\mathbf{α}^i$ defined in Sec. 1.7</a>) will be denoted by Greek letters in the appropriate typeface. For mnemonic reasons, an operation closely related to such a special quantity will be denoted by the same Greek letter. For example, $α/\vect{u}$ denotes the maximum prefix (Sec. 1.10) of the logical vector $\vect{u}$. Where a Greek letter is indistinguishable from a Roman, sanserif characters will be used, e.g. $\mathsf{\mat{E}}$ and $\mathsf{\mat{I}}$ for the capitals of epsilon and iota.

<table border=1 cellspacing=0 cellpadding=8>
<td rowspan=2 align=center>Type of\\
Operand & <td colspan=2 align=center>Representation & \\
<td align=center>Printed & <td align=center>Typed & \\

\begin{tabularx}
 & <td colspan=2>Literal & \\
 & & Alphabetic & \\
 & & Numeric & \\
 & <td colspan=2>Variable & \\
 & & Alphabetic & \\
 & & Numeric & \\
 & <td colspan=2>Vector & \\
 & <td colspan=2>Matrix & \\
 & <td colspan=2>Tree & \\
\end{tabularx} & 
\begin{tabularx}

Roman, u.c. and l.c. & \\
Standard numeral & \\

Italic, u.c. and l.c. & \\
Italic numeral & \\
l.c. boldface italic & \\
u.c. boldface italic & \\
u.c. boldface roman & \\
\end{tabularx} & 
\begin{tabularx}

Circled u.c. and l.c. roman & \\
Standard numeral & \\

Unmarked & \\
Underscore & \\
Underscore & \\
Underscore & \\
Wavy underscore & \\
\end{tabularx} & 

\end{tabularx}

\par \textbf{Table 1.8} Typographic conventions for classes of operands

<a name="1.3.2"></a>
\par \textbf{Literals and variables}

\par The power of any mathematical notation rests largely on the use of symbols to represent general quantities which, in given instances, are further specified by other quantities. Thus Program 1.4 represents a general process which determines $x = n^{2/3}$ for any suitable value of $n$. In a specific case, say $n =$ 27, the quantity $x$ is specified as the number 9.

\par Each operand occurring in a meaningful process must be specified ultimately in terms of commonly accepted concepts. The symbols representing such accepted concepts will be called \textit{literals}. Examples of literals are the integers, the characters of the various alphabets, punctuation marks, and miscellaneous symbols such as $ and %. The literals occurring in Program 1.4 are 0, 1, 2.

\par It is important to distinguish clearly between general symbols and literals. In ordinary algebra this presents little difficulty, since the only literals occurring are the integers and the decimal point, and each general symbol employed includes an alphabetic character. In describing more general processes, however, alphabetic literals (such as proper names) also appear. Moreover, in a computer program, numeric symbols (register addresses) are used to represent the variables.

\par In general, then, alphabetic literals, alphabetic variables, numeric literals, and numeric variables may all appear in a complex process and must be clearly differentiated. The symbols used for literals will be Roman letters (enclosed in quotes when appearing in text) and standard numerals. The symbols used for variables will be italic letters, italic numerals, and boldface letters as detailed in Table 1.8. Miscellaneous signs and symbols when used as literals will be enclosed in quotes in both programs and text.

\par It is sometimes desirable (e.g., for mnemonic reasons) to denote a variable by a string of alphabetic or other symbols rather than by a single symbol. The monolithic interpretation of such a string will be indicated by the \textit{tie} used in musical notation, thus:
<img align=middle src="APLimg/tie1x3a.bmp"> and 
<img align=middle src="APLimg/tie1x3b.bmp"> may denote the variable ``inventory'', a vector of inventory values, and a matrix of inventory values, respectively.

<a name="null_element"></a>
\par In the set of alphabetic characters, the \textit{space} plays a special role. For other sets a similar role is usually played by some one element, and this element is given the special name of \textit{null element}. In the set of numeric digits, the \textit{zero} plays a dual role as both null element and numeric quantity. The null element will be denoted by the degree symbol $∘$.

\par In any determinate process, each operand must be specified ultimately in terms of literals. In Program 1.4, for example, the quantity $k$ is specified in terms of known arithmetic operations (multiplication and division) involving the literals 1 and 2. The quantity $n$, on the other hand, is not determined within the process and must presumably be specified within some larger process which includes Program 1.4. Such a quantity is called an \textit{argument} of the process.

<a name="1.3.3"></a>
\par \textbf{Domain and range}

\par The class of arguments and the class of results of a given operator are called its \textit{domain} and \textit{range}, respectively. Thus the domain and range of the magnitude operation $(|x|)$ are the real numbers and the nonnegative real numbers, respectively.

\par A variable is classified according to the range of values it may assume: it is \textit{logical}, \textit{integral}, or \textit{numerical}, according as the range is the set of logical variables (that is, 0 and 1), the set of integers, or the set of real numbers. Each of the foregoing classes is clearly a subclass of each class following it, and any operation defined on a class clearly applies to any of its subclasses. A variable which is nonnumeric will be called \textit{arbitrary}. In the Summary of Notation, the range and domain of each of the operators defined is specified in terms of the foregoing classes according to the conventions shown in Sec. S.1.

<a name="1.4"></a>
\par \textbf{1.4 Elementary operations}

\par The elementary operations employed include the ordinary arithmetic operations, the elementary operations of the logical calculus, and the residue and related operations arising in elementary number theory. In defining operations in the text, the symbol $\leftrightarrow$ will be used to denote equivalence of the pair of statements between which it occurs.

<a name="1.4.1"></a>
\par \textbf{Arithmetic operations}

<a name="plus"></a><a name="minus"></a><a name="times"></a><a name="divide"></a> The ordinary arithmetic operations will be denoted by the ordinary symbols +, -, $\times$, and $÷$ and defined as usual except that the domain and range of multiplication will be extended slightly as follows. If one of the factors is a logical variable (0 or 1), the second may be arbitrary and the product then assumes the value of the second factor or zero according as the value of the first factor (the logical variable) is 1 or 0. Thus if the arbitrary factor is the literal ``q'', then

\begin{tabularx}
 & 0 \times q = q \times 0 = 0 & \\
and & 1 \times q = q \times 1 = q & \\
\end{tabularx}

\par According to the usual custom in ordinary algebra, the multiplication symbol may be elided.

<a name="1.4.2"></a>
\par \textbf{Logical operations}

<a name="and"></a><a name="or"></a><a name="not"></a>
\par The elementary logical operations \textit{and}, $or$, and \textit{not} will be denoted by $\wedge, \vee$ and an overbar and are defined in the usual way as follows:

\begin{tabularx}
 \textit{w} ← \textit{u} \wedge \textit{v} & \leftrightarrow & \textit{w} = 1 if and only if \textit{u} = 1 and & \textit{v} = 1, & \\
 \textit{w} ← \textit{u} \vee \textit{v} & \leftrightarrow & \textit{w} = 1 if and only if \textit{u} = 1 or & \textit{v} = 1, & \\
 \textit{w} ← \overbar{u} & \leftrightarrow & \textit{w} = 1 if and only if \textit{u} = 0. & \\
\end{tabularx}

\par If $x$ and $y$ are numerical quantities, then the expression $x < y$ implies that the quantity $x$ stands in the relation ``less than'' to the quantity $y$.
<a name="relation"></a> More generally, if $α$ and $β$ are arbitrary entities and $\mathcal{R}$ is any relation defined on them, the \textit{relational statement} $(α \mathcal{R} β)$ is a logical variable which is true (equal to 1) if and only if $α$ stands in the relation $\mathcal{R}$ to $β$. For example, if $x$ is any real number, then the function

\par $(x >$ 0) $- (x <$ 0)

\par (commonly called the \textit{sign function} or sgn $x)$ assumes the values 1, 0, or -1 according as $x$ is strictly positive, 0, or strictly negative.
<a name="abs"></a> Moreover, the magnitude function $|x|$ may be defined as $|x| = x \times$ sgn $x = x \times ((x >$ 0) $- (x <$ 0)).

\par The relational statement is a useful generalization of the Kronecker delta, that is $δ_j^i = (i = j)$. Moreover, it provides a convenient expression for a number of familiar logical operations. The \textit{exclusive or}, for example, may be denoted by $(u \neq v)$, and its negation (i.e., the equivalence function) may be denoted by $(u = v)$.

<a name="1.4.3"></a>
\par \textbf{Residues and congruence}

<a name="residue"></a>
\par For each set of integers $n, j$, and $b$, with $b >$ 0, there exists a unique pair of integers $q$ and $r$ such that

\par $n = bq + r, j \leq r < j + b$.

\par The quantity $r$ is called the \textit{j-residue of n modulo b} and is denoted by $b |_j n$. For example, 3 |_0 9 $=$ 0, 3 |_1 9 $=$ 3, and 3 |_0 10 $=$ 1. Moreover, if $n \geq$ 0, then $b$ |_0 $n$ is the remainder obtained in dividing $n$ by $b$ and $q$ is the integral part of the quotient. A number $n$ is said to be of \textit{even parity} if its 0-residue modulo 2 is zero and of \textit{odd parity} if 2 |_0 $n =$ 1.

\par If two numbers $n$ and $m$ have the same $j$-residue modulo $b$, they differ by an integral multiple of $b$ and therefore have the same $k$-residue module $b$ for any $k$. If $b |_j n = b |_j m$, then $m$ and $n$ are said to be \textit{congruent mod b}. Congruency is transitive and reflexive and is denoted by

\par $m \equiv n$ (mod $b)$.

\par In classical treatments, such as
<acronym title="Wright, H.N. (1939), First Course in Theory of Numbers, Wiley, New York.">Wright (1939)</acronym>, only the 0-residue is considered. The use of 1-origin indexing (cf. Sec. 1.5) accounts for the interest of the 1-residue.

\par A number represented in a positional notation (e.g., in a base ten or a base two number system) must, in practice, employ only a finite number of digits. It is therefore often desirable to approximate a number $x$ by an integer. For this purpose two functions are defined:
<a name="floor"></a><a name="ceiling"></a>

\begin{tabularx}
<td valign=top>1. & & the \textit{floor of x} (or integral part of \textit{x}), denoted by ⌊\textit{x}⌋ and defined as the largest integer not exceeding \textit{x},

<td valign=top>2. & & the \textit{ceiling of x}, denoted by ⌈\textit{x}⌉ and defined as the smallest integer not exceeded by \textit{x}.

\end{tabularx}

\par $Thus\par$ 

\begin{tabularx}

⌈3.14⌉ = 4, & & 
⌊3.14⌋ = 3, & & 
⌊-3.14⌋ = -4, & 

⌈3.00⌉ = 3, & & 
⌊3.00⌋ = 3, & & 
⌊-3.00⌋ = -3. & 

\end{tabularx}

\par Clearly $⌈x⌉ = -⌊-x⌋$ and $⌊x⌋ \leq x \leq ⌈x⌉$. Moreover, $n = b⌊n ÷ b⌋ + b$ |_0 $n$ for all integers $n$. Hence the integral quotient $⌊n ÷ b⌋$ is equivalent to the quantity $q$ occurring in the definition of the $j$-residue for the case $j =$ 0.

<a name="1.5"></a>
\par \textbf{1.5 Structured operands}

<a name="1.5.1"></a>
\par \textbf{Elementary operations}

\par Any operation defined on a single operand can be generalized to apply to each member of an array of related operands. Similarly, any binary operation (defined on two operands) can be generalized to apply to pairs of corresponding elements of two arrays. Since algorithms commonly incorporate processes which are repeated on each member of an array of operands, such generalization permits effective subordination of detail in their description. For example, the accounting process defined on the data of an individual bank account treats a number of distinct operands within the account, such as account number, name, and balance. Moreover, the over-all process is defined on a large number of similar accounts, all represented in a common format. Such structured arrays of variables will be called \textit{structured operands}, and extensive use will be made of three types, called \textit{vector}, \textit{matrix}, and \textit{tree}. As indicated in Sec. S.1 of the Summary of Notation, a structured operand is further classified as \textit{logical}, \textit{integral}, \textit{numerical}, or \textit{arbitrary}, according to the type of elements in contains.

\par A \textit{vector} $\vect{x}$ is the ordered array of elements $(\vect{x}_1, \vect{x}_2, \vect{x}_3$, ..., $\vect{x}_{ν(\vect{x})})$. The variable $\vect{x}_i$ is called the $i$th \textit{component} of the vector $\vect{x}$, and the number of components,
<a name="nu_vector"></a> denoted by $ν(\vect{x})$ (or simply by $ν$ when the determining vector is clear from context), is called the \textit{dimension} of $\vect{x}$. Vectors and their components will be represented in lower case boldface italics.
<a name="scalar_multiple"></a> A numerical vector $\vect{x}$ may be multiplied by a numerical quantity $k$ to produce the \textit{scalar multiple} $k \times \vect{x}$ (or $k\vect{x})$ defined as the vector $\vect{z}$ such that $\vect{z}_i = k \times \vect{x}_i$.

\par All elementary operations defined on individual variables are extended consistently to vectors as component-by-component operations. For example,

\begin{tabularx}
 & \vect{z} = \vect{x} + \vect{y} & \leftrightarrow & \vect{z}_{\textit{i}} = \vect{x}_{\textit{i}} + \vect{y}_{\textit{i}}, & \\
 & \vect{z} = \vect{x} \times \vect{y} & \leftrightarrow & \vect{z}_{\textit{i}} = \vect{x}_{\textit{i}} \times \vect{y}_{\textit{i}}, & \\
 & \vect{z} = \vect{x} ÷ \vect{y} & \leftrightarrow & \vect{z}_{\textit{i}} = \vect{x}_{\textit{i}} ÷ \vect{y}_{\textit{i}}, & \\
 & \vect{z} = ⌈\vect{x}⌉ & \leftrightarrow & \vect{z}_{\textit{i}} = ⌈\vect{x}_{\textit{i}}⌉ & 
 & \vect{w} = \vect{u} \wedge \vect{v} & \leftrightarrow & \vect{w}_{\textit{i}} = \vect{u}_{\textit{i}} \wedge \vect{v}_{\textit{i}}, & \\
 & \vect{w} = (\vect{x} < \vect{y}) & \leftrightarrow & \vect{w}_{\textit{i}} = (\vect{x}_{\textit{i}} < \vect{y}_{\textit{i}}). & \\
\end{tabularx}

\par Thus if $\vect{x} =$ (1, 0, 1, 1) and $\vect{y} =$ (0, 1, 1, 0) then $\vect{x} + \vect{y} =$ (1, 1, 2, 1), $\vect{x} \wedge \vect{y} =$ (0, 0, 1, 0), and $(\vect{x} < \vect{y}) =$ (0, 1, 0, 0).

<a name="1.5.2"></a>
\par \textbf{Matrices}

\par A matrix $\mat{M}$ is the ordered two-dimensional array of variables

\begin{tabularx} & \begin{tabularx}
<td rowspan=4><img src="APLimg/matrixl1x5.bmp"> & \mat{M}_1^1, & \mat{M}_2^1, & ..., & \mat{M}_{\textit{ν}(\mat{M})}^1 & <td rowspan=4><img src="APLimg/matrixr1x5.bmp"> & <td rowspan=4 valign=middle>. & \\
 \mat{M}_1^2, & \mat{M}_2^2, & ..., & \mat{M}_{\textit{ν}(\mat{M})}^2 & \\
 <td align=center>. & <td align=center>. & <td align=center>. & <td align=center>. & \\
 \mat{M}_1^{\textit{μ}(\mat{M})}, & \mat{M}_2^{\textit{μ}(\mat{M})}, & ..., & \mat{M}_{\textit{ν}(\mat{M})}^{\textit{μ}(\mat{M})} & \\
\end{tabularx} & \\\end{tabularx}

\par The vector $(\mat{M}_1^i, \mat{M}_2^i$, ..., $\mat{M}_ν^i)$ is called the $i$th \textit{row vector} of $\mat{M}$ and is denoted by $\mat{M}^i$.
<a name="nu_matrix"></a> Its dimension $ν(\mat{M})$ is called the \textit{row dimension} of the matrix. The vector $(\mat{M}_j^1, \mat{M}_j^2$, ..., $\mat{M}_j^μ)$ is called the $j$th \textit{column vector} of $\mat{M}$ and is denoted by $\mat{M}_j$.
<a name="mu_matrix"></a> Its dimension $μ(\mat{M})$ is called the \textit{column dimension} of the matrix.

\par The variable $\mat{M}_j^i$ is called the $(i,j)$th \textit{component} or \textit{element} of the matrix. A matrix and its elements will be represented by upper case boldface italics. Operations defined on each element of a matrix are generalized component by component to the entire matrix. Thus, if $\odot$ is any binary operator,

\par $\mat{P} = \mat{M}$ 
\odot \mat{N} \leftrightarrow \mat{M}_{\textit{j}}^{\textit{i}}
\odot \mat{N}_{\textit{j}}^{\textit{i}}.

<a name="1.5.3"></a>
\par \textbf{Index systems}

\par The subscript appended to a vector to designate a single component is called an \textit{index}, and the indices are normally chosen as a set of successive integers beginning at 1, that is, $\vect{x} = (\vect{x}_1, \vect{x}_2$, ... $\vect{x}_ν)$. It is, however, convenient to admit more general $j-\textit{origin$ indexing} in which the set of successive integers employed as indices in any structured operand begin with a specified integer $j$.

\par The two systems of greatest interest are the common 1-origin system, which will be employed almost exclusively in this chapter, and the 0-origin system. The latter system is particularly convenient whenever the index itself must be represented in a positional number system and will therefore be employed exclusively in the treatment of computer organization in Chapter 2.

<a name="1.6"></a>
\par \textbf{1.6 Rotation}

\par The \textit{left rotation} of a vector $\vect{x}$ is denoted by $k ↑ \vect{x}$ and specifies the vector obtained by cyclical left shift of the components of $\vect{x}$ by $k$ places. Thus if $\vect{a} =$ (1, 2, 3, 4, 5, 6), and $\vect{b} =$ (c, a, n, d, y), then 2 $↑ \vect{a} =$ (3, 4, 5, 6, 1, 2) and 3 $↑ \vect{b} =$ 8 $↑ \vect{b} =$ (d, y, c, a, n). Formally,^{<a href="#note1a">[a]</a>}

\par $\vect{z} = k ↑ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j$, where $j = ν|_1(i + k)$.

<a name="right_rotation"></a>
\par \textit{Right rotation} is denoted by $k ↓ \vect{x}$ and is defined analogously. Thus

\par $\vect{z} = k ↓ \vect{x} \leftrightarrow \vect{z}_i = \vect{x}_j$, where $j = ν|_1(i - k)$.

\par If $k =$ 1, it may be elided. Thus $↑ \vect{b} =$ (a, n, d, y, c).

<a name="rotate_row"></a>
<a name="rotate_col"></a>
\par Left rotation is extended to matrices in two ways as follows:

\begin{tabularx}
 & \mat{A} ← \vect{j} ↑ \mat{B} & \leftrightarrow & \mat{A}^{\textit{i}} = \vect{j}_{\textit{i}} ↑ \mat{B}^{\textit{i}} & 

 & \mat{C} ← \vect{k} \Uparrow \mat{B} & \leftrightarrow & \mat{C}_{\textit{i}} = \vect{k}_{\textit{j}} ↑ \mat{B}_{\textit{j}} & 

\end{tabularx}

\par The first operation is an extension of the basic vector rotation to each row of the matrix and is therefore called \textit{row rotation}. The second operation is the corresponding column operation and is therefore denoted by the doubled operation symbol $\Uparrow$. For example, if

\par $\vect{k} =$ (0, 1, 2),

and

\begin{tabularx}
 & \mat{B} = & \begin{tabularx}
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & \textit{a}^{}_{} & \textit{b}^{}_{} & \textit{c}^{}_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & \\
 \textit{d}^{}_{} & \textit{e}^{}_{} & \textit{f}^{}_{} & \\
 \textit{g}^{}_{} & \textit{h}^{}_{} & \textit{i}^{}_{} & \\
\end{tabularx} & \\
\end{tabularx}

then

\begin{tabularx}
\begin{tabularx}
 & \vect{k} ↑ \mat{B} = & \begin{tabularx}
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & \textit{a}^{}_{} & \textit{b}^{}_{} & \textit{c}^{}_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & \\
 \textit{e}^{}_{} & \textit{f}^{}_{} & \textit{d}^{}_{} & \\
 \textit{i}^{}_{} & \textit{g}^{}_{} & \textit{h}^{}_{} & \\
\end{tabularx} & \\
\end{tabularx}
 & and & 
\begin{tabularx}
\vect{k} \Uparrow \mat{B} = & \begin{tabularx}
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & \textit{a}^{}_{} & \textit{e}^{}_{} & \textit{i}^{}_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & \\
 \textit{d}^{}_{} & \textit{h}^{}_{} & \textit{c}^{}_{} & \\
 \textit{g}^{}_{} & \textit{b}^{}_{} & \textit{f}^{}_{} & \\
\end{tabularx} & \\
\end{tabularx}
 & . & \\\end{tabularx}

\par Right rotation is extended analogously.

<a name="1.7"></a>
\par \textbf{1.7 Special vectors}

\par Certain special vectors warrant special symbols. In each of the following definitions, the parameter $n$ will be used to specify the dimension.
<a name="interval_vector"></a> The \textit{interval vector} $\textbf{ι}^j(n)$ is defined as the vector of integers beginning with $j$. Thus $\textbf{ι}^0(4)=(0$, 1, 2, 3), $\textbf{ι}^1(4)=(1$, 2, 3, 4), and $\textbf{ι}^{-7}(5)=$ (-7, -6, -5, -4, -3). Four types of logical vectors are defined as follows.
<a name="unit_vector"></a> The $j$th \textit{unit vector} $\textbf{ϵ}^j(n)$ has a one in the $j$th position, that is, $(\textbf{ϵ}^j(n))_k = (k = j)$.
<a name="full_vector"></a> The \textit{full vector} $\textbf{ϵ}}(n)$ consists of all ones.
<a name="zero_vector"></a> The vector consisting of all zeros is denoted both by 0 and by $\overbar{\textbf{ϵ}}(n)$.
<a name="prefix_vector"></a> The \textit{prefix vector of weight j} is denoted by $\mathbf{α}^j(n)$ and possesses ones in the first $k$ positions, where $k$ is the lesser of $j$ and $n$.
<a name="suffix_vector"></a> The \textit{suffix vector} $\textbf{ω}^j(n)$ is defined analogously. Thus $\textbf{ϵ}^2(3) =$ (0, 1, 0), $\textbf{ϵ}(4) =$ (1, 1, 1, 1), $\mathbf{α}^3(5) =$ (1, 1, 1, 0, 0), $\textbf{ω}^3(5) =$ (0, 0, 1, 1, 1), and $\mathbf{α}^7(5) = \mathbf{α}^5(5) =$ (1, 1, 1, 1, 1). Moreover, $\textbf{ω}^j(n) = j ↑ \mathbf{α}^j(n)$, and $\mathbf{α}^j(n) = j ↓ \textbf{ω}^j(n)$.

<a name="infix_vector"></a>
\par A logical vector of the form $\mathbf{α}^h(n) \wedge \textbf{ω}^j(n)$ is called an \textit{infix vector}. An infix vector can also be specified in the form $j ↓ \mathbf{α}^k(n)$, which displays its weight and location more directly.

\par An operation such as $\vect{x} \wedge \vect{y}$ is defined only for \textit{compatible} vectors $\vect{x}$ and $\vect{y}$, that is, for vectors of like dimension. Since this compatibility requirement can be assumed to specify implicitly the dimension of one of the operands, elision of the parameter $n$ may be permitted in the notation for the special vectors. Thus, if $y =$ (3, 4, 5, 6, 7), the expression $\textbf{ϵ} \times \vect{y}$ and $\textbf{ϵ}^j \times \vect{y}$ imply that the dimensions of $\textbf{ϵ}$ and $\textbf{ϵ}^j$ are both 5. Moreover, elision of $j$ will be permitted for the interval vector $\textbf{ι}^j(n)$ (or $\textbf{ι}^j)$, and for the residue operator $|_j$ when $j$ is the index origin in use.

\par It is, of course, necessary to specify the index origin in use at any given time. For example, the unit vector $\textbf{ϵ}^3(5)$ is (0, 0, 1, 0, 0) in a 1-origin system and (0, 0, 0, 1, 0) in a 0-origin system, even though the definition (that is, $(\textbf{ϵ}^j(n))_k = (k = j))$ remains unchanged. The prefix and suffix vectors are, of course, independent of the index origin. Unless otherwise specified, 1-origin indexing will be assumed.

\par The vector $\textbf{ϵ}(0)$ is a vector of dimension zero and will be called the \textit{null vector}. It should not be confused with the special null element $∘$.

<a name="1.8"></a>
\par \textbf{1.8 Reduction}

\par An operation (such as summation) which is applied to all components of a vector to produce a result of a simpler structure is called a \textit{reduction}. The $\odot$-reduction of a vector $\vect{x}$ is denoted by
\odot/\vect{x} and defined as

\par $\vect{z} ←$ 
\odot/\vect{x} \leftrightarrow \vect{z} = (... ((\vect{x}_1
\odot \vect{x}_2)
\odot \vect{x}_3)
\odot ...) 
\odot \vect{x}_{\textit{ν}}),

\par where $\odot$ is any binary operator with a suitable domain. Thus $+/\vect{x}$ is the sum, $\times/\vect{x}$ is the product, and $\vee/\vect{x}$ is the logical sum of the components of a vector $\vect{x}$. For example, $\times/\textbf{ι}^1(5) =$ 1 $\times$ 2 $\times$ 3 $\times$ 4 $\times$ 5, $\times/\textbf{ι}^1(n) = n!$, and $+/\textbf{ι}^1(n) = n(n +$ 1)/2.

\par As a further example, De Morgan's law may be expressed as $\wedge/\vect{u} =$ <img src="APLimg/ex1x10a.bmp">, where $\vect{u}$ is a logical vector of dimension two. Moreover, a simple inductive argument (Exercise 1.10) shows that the foregoing expression is the valid generalization of De Morgan's law for a logical vector $\vect{u}$ of arbitrary dimension.

\par A relation $\mathcal{R}$ incorporated into a relational statement $(x\mathcal{R}y)$ becomes, in effect, an operator on the variables $x$ and $y$. Consequently, the reduction $\mathcal{R}/\vect{x}$ can be defined in a manner analogous to that of $(\odot/\vect{x})$, that is,

\par $\mathcal{R}/\vect{x} =$ (... $((\vect{x}_1 \mathcal{R} \vect{x}_2) \mathcal{R} \vect{x}_3) \mathcal{R}$ ...)
\mathcal{R} \vect{x}_{\textit{ν}}).

\par The parentheses now imply relational statements as well as grouping. The relational reductions of practical interest are $\neq/\vect{u}$, and $=/\vect{u}$, the \textit{exclusive-or} and the \textit{equivalence} reduction, respectively.

\par The inductive argument of Exercise 1.10 shows that $\neq/\vect{u} =$ 2 |_0 $(+/\vect{u})$. For example, if $\vect{u} =$ (1,0,1,1,0), then

\begin{tabularx}
 & \neq/\vect{u} & = & ((((1 \neq 0) \neq 1) \neq 1) \neq 0) & 
<td colspan=2> & = & (((1 \neq 1) \neq 1) \neq 0) & \\
<td colspan=2> & = & ((0 \neq 1) \neq 0) & \\
<td colspan=2> & = & (1 \neq 0) = 1, & \\
\end{tabularx}

\par and 2 |_0 $(+/\vect{u}) =$ 2 |_0 3 $=$ 1. Similarly, $=/\vect{u} =$ <img align=top src="APLimg/ex1x10c.bmp">, and as a consequence,

\par $\neq/\vect{u} =$ <img src="APLimg/ex1x10d.bmp">,

\par a useful companion to De Morgan's law.

\par To complete the system it is essential to define the value of 
\odot/\textbf{ϵ}(0), the reduction of the null vector of dimension zero, as the identity element of the operator or relation \odot. Thus +/\textbf{ϵ}(0) = \vee/\textbf{ϵ}(0) = 0, and \times/\textbf{ϵ}(0) = \wedge/\textbf{ϵ}(0) = 1.

<a name="reduce_row"></a>
\par A reduction operation is extended to matrices in two ways. A \textit{row reduction} of a matrix $\mat{X}$ by an operator $\odot$ is denoted by

\par $\vect{y} ←$ 
\odot/\mat{X}

\par and specifies a vector $\vect{y}$ of dimension $μ(\mat{X})$ such that $\vect{y}_i$ =
\odot/\mat{X}^{\textit{i}}.
<a name="reduce_col"></a> A \textit{column reduction} of $\mat{X}$ is denoted by $\vect{z} ←$ 
\odot/\!/\mat{X} and specifies a vector \vect{z} of dimension \textit{ν}(\mat{X}) such that \vect{z}_{\textit{j}} =
\odot/\!/\mat{X}_{\textit{j}}.

\par For example, if

\begin{tabularx}
<td rowspan=3> \mat{U} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & ^{}1 0 1 0_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & 

^{}0 0 1 1_{} & \\
^{}1 1 1 0_{} & \\
\end{tabularx}

\par then $+/\mat{U} =$ (2, 2, 3), $+/\!/\mat{U} =$ (2, 1, 3, 1), $\wedge/\!/\mat{U} =$ (0, 0, 1, 0), $\neq/\mat{U} =$ (0, 0, 1), $=/\!/\mat{U} =$ (0, 1, 1, 1), and $+/(=/\!/\mat{U}) =$ 3.

<a name="1.9"></a>
\par \textbf{1.9 Selection}

<a name="1.9.1"></a>
\par \textbf{Compression}

\par The effective use of structured operands depends not only on generalized operations but also on the ability to specify and select certain elements or groups of elements. The selection of single elements can be indicated by indices, as in the expressions $\vect{v}_i, \mat{M}^i, \mat{M}_j$, and $\mat{M}_i^j$. Since selection is a binary operation (i.e., to select or not to select), more general selection is conveniently specified by a logical vector, each unit component indicating selection of the corresponding component of the operand.

\par The selection operation defined on an arbitrary vector $\vect{a}$ and a compatible (i.e., equal in dimension) logical vector $\vect{u}$ is denoted by $\vect{c} ← \vect{u}/\vect{a}$ and is defined as follows: the vector $\vect{c}$ is obtained from $\vect{a}$ by suppressing from $\vect{a}$ each component $\vect{a}_i$ for which $\vect{u}_i =$ 0. The vector $\vect{u}$ is said to \textit{compress} the vector $\vect{a}$. Clearly $ν(\vect{c}) = +/\vect{u}$. For example, if $\vect{u} =$ (1, 0, 0, 0, 1, 1) and $\vect{a} =$ (M, o, n, d, a, y), then $\vect{u}/\vect{a} =$ (M, a, y). Moreover, if $n$ is even and $\vect{v} = (2\textbf{ϵ})$ |_0 $\textbf{ι}^1(n) =$ (1, 0, 1, 0, 1, ...), then $\vect{v}/\textbf{ι}^1(n) =$ (1, 3, 5, ..., $n-1)$ and $+/(\vect{v}/\textbf{ι}^1(n)) = (n/2)^2$.

<a name="compress_row"></a>
\par \textit{Row compression} of a matrix, denoted by $\vect{u}/\mat{A}$, compresses each row vector $\mat{A}^i$ to form a matrix of dimension $μ(\mat{A}) \times +/\vect{u}$.
<a name="compress_col"></a> \textit{Column compression}, denoted by $\vect{u}/\!/\mat{A}$, compresses each column vector $\mat{A}_j$ to form a matrix of dimension $+/\vect{u} \times ν(\mat{A})$. Compatibility conditions are $ν(\vect{u}) = ν(\mat{A})$ for row compression, and $ν(\vect{u}) = μ(\mat{A})$ for column compression. For example, if $\mat{A}$ is an arbitrary 3 $\times$ 4 matrix, $\vect{u} =$ (0, 1, 0, 1) and $\vect{v} =$ (1, 0, 1); then

\begin{tabularx}
 & \begin{tabularx}
<td rowspan=3>\vect{u}/\mat{A} = & 
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & 
 \mat{A}_2^1 \mat{A}_4^1

<td rowspan=3><img src="APLimg/matrixr3.bmp"> & 
<td rowspan=3>, & 

 \mat{A}_2^3 \mat{A}_4^3

 \mat{A}_2^2 \mat{A}_4^2

\end{tabularx} & 
 & \begin{tabularx}
<td rowspan=2>\vect{v}/\!/\mat{A} = & 
<td rowspan=2><img src="APLimg/matrixl2.bmp"> & 
 \mat{A}_1^1 \mat{A}_2^1 \mat{A}_3^1 \mat{A}_4^1

<td rowspan=2><img src="APLimg/matrixr2.bmp"> & 
<td rowspan=2>, & 

 \mat{A}_1^3 \mat{A}_2^3 \mat{A}_3^3 \mat{A}_4^3

\end{tabularx} & \\
\end{tabularx}

and

\begin{tabularx}
<td rowspan=2> \vect{u}/\vect{v}/\!/\mat{A} = \vect{v}/\!/\vect{u}/\mat{A} = & 

<td rowspan=2><img src="APLimg/matrixl2.bmp"> & 
\begin{tabularx}
 \mat{A}_2^1 \mat{A}_4^1

 \mat{A}_2^3 \mat{A}_4^3

\end{tabularx} & 
<td rowspan=2><img src="APLimg/matrixr2.bmp"> & 
<td rowspan=2>. & 
\\\end{tabularx}

\par It is clear that \textit{row} compression \textit{suppresses columns} corresponding to zeros of the logical vector and that \textit{column} compression \textit{suppresses rows}. This illustrates the type of confusion in nomenclature which is avoided by the convention adopted in Sec. 1.3: an operation is called a \textit{row operation} if the underlying operation from which it is generalized is applied to the row vectors of the matrix, and a \textit{column operation} if it is applied to columns.

\begin{tabularx} & 
\par \textbf{Example 1.1}. A bank makes a quarterly review of accounts to produce the following four lists:
\begin{tabularx}
<td valign=top>1. & & the name, account number, and balance for each account with a balance less than two dollars.
<td valign=top>2. & & the name, account number, and balance for each account with a negative balance exceeding one hundred dollars.
<td valign=top>3. & & the name and account number of each account with a balance exceeding one thousand dollars.
<td valign=top>4. & & all unassigned account numbers.
\end{tabularx}

\par The ledger may be described by a matrix

\begin{tabularx}
 \mat{L} = (\mat{L}_1,\mat{L}_2,\mat{L}_3) = & 
<img src="APLimg/matrixl1x9c.bmp"> & 
\par $\mat{L}^1\\$
.\\
.\\
.\\
\mat{L}^{\textit{m}} & 
<img src="APLimg/matrixr1x9c.bmp"> & 
\\\end{tabularx}

\par with column vectors $\mat{L}_1, \mat{L}_2$, and $\mat{L}_3$, representing names, account numbers, and balances, respectively, and with row vectors $\mat{L}^1, \mat{L}^2$, ..., $\mat{L}^m$, representing individual accounts. An unassigned account number is identified by the word ``none'' in the name position. The four output lists will be denoted by the matrices $\mat{P}, \mat{Q}, \mat{R}$, and $\mat{S}$, respectively. They can be produced by Program 1.9.
 & & \\\end{tabularx}

\begin{tabularx}<td valign=top><img src="APLimg/prog1x9.bmp"> & & 
<table border=1 cellspacing=0 cellpadding=0>\begin{tabularx}
<td valign=top>\mat{L} & & Bank ledger. & \\
<td valign=top>\mat{L}^{\textit{k}} & & \textit{k}th account.^{} & \\
<td valign=top>\mat{L}_3^{\textit{k}} & & Balance of \textit{k}th account._{}^{} & \\
<td valign=top>\mat{L}_2^{\textit{k}} & & Account number of \textit{k}th _{}^{} account. & \\
<td valign=top>\mat{L}_1^{\textit{k}} & & Name of \textit{k}th account or ``none'' if account number \mat{L}_2^{\textit{k}} unused. & \\
\end{tabularx} & \\\end{tabularx}
\par \textbf{Legend} $& \\$
\end{tabularx}

\par \textbf{Program 1.9} Selection on bank ledger $\mat{L}$ (Example 1.1)

\begin{tabularx} & 
\par \textbf{Program 1.9}. Since $\mat{L}_3$ is the vector of balances, and $2\textbf{ϵ}$ is a compatible vector each of whose components equals two, the relational statement $(\mat{L}_3 < 2\textbf{ϵ})$ defines a logical vector having unit components corresponding to those accounts to be included in the list $\mat{P}$. Consequently, the column compression of step 1 selects the appropriate rows of $\mat{L}$ to define $\mat{P}$. Step 2 is similar, but step 3 incorporates an additional row compression by the compatible prefix vector $\mathbf{α}^2 =$ (1,1,0) to select columns one and two of $\mat{L}$. Step 4 represents the comparison of the name (in column $\mat{L}_1)$ with the literal ``none'', the selection of each row which shows agreement, and the suppression of all columns but the second. The expression ``none $\textbf{ϵ}''$ occurring in step 4 illustrates the use of the extended definition of multiplication.
 & & \\\end{tabularx}

<a name="1.9.2"></a>
\par \textbf{Mesh, mask, and expansion}

\par A logical vector $\vect{u}$ and the two vectors $\vect{a} =$ 
\overbar{\vect{u}}/\vect{c} and \vect{b} = \vect{u}/\vect{c}, obtained by compressing a vector \vect{c}, collectively determine the vector \vect{c}. The operation which specifies \vect{c} as a function of \vect{a}, \vect{b}, and \vect{u} is called a \textit{mesh} and is defined as follows: If \vect{a} and \vect{b} are arbitrary vectors and if \vect{u} is a logical vector such that +/\overbar{\vect{u}} = \textit{ν}(\vect{a}) and +/\vect{u} = \textit{ν}(\vect{b}), then the \textit{mesh of} \vect{a} \textit{and} \vect{b} \textit{on} \vect{u} is denoted by \backslash\vect{a}, \vect{u}, \vect{b}\backslash and is defined as the vector \vect{c} such that \overbar{\vect{u}}/\vect{c} = \vect{a} and \vect{u}/\vect{c} = \vect{b}. The mesh operation is equivalent to choosing successive components of \vect{c} from \vect{a} or \vect{b} according as the successive components of \vect{u} are 0 or 1. If, for example, \vect{a} = (s, e, k), \vect{b} = (t, a), and \vect{u} = (0, 1, 0, 1, 0), then \backslash\vect{a}, \vect{u}, \vect{b}\backslash = (s, t, e, a, k). As a further example, 

\begin{tabularx}
<td align=center rowspan=2><img src="APLimg/prog1x10a.bmp">\\
(a) & <td nowrap rowspan=2> & <td align=center><img src="APLimg/prog1x10b.bmp">\\
(b)\\

<table border=1 cellspacing=0 cellpadding=8>
\begin{tabularx}
\vect{a}, \vect{b} & & Given vectors. & \\
\vect{c} & & \vect{c} = (\vect{a}_1, \vect{b}_1, \vect{b}_2, \vect{a}_2, \vect{b}_3, \vect{b}_4, ...) & \\
\textit{i} & & Index of \vect{a}. & \\
\textit{j} & & Index of \vect{b}. & \\
\textit{k} & & Index of \vect{c}. & \\
\textit{u} & & \vect{a} = (0, 1, 1, 0, 1, 1, 0, ...). & \\
\end{tabularx} & \\
\end{tabularx}\par \textbf{Legend} & \\
\end{tabularx}

\par \textbf{Program 1.10} Interfiling program

\par Program 1.10a (which describes the merging of the vectors $\vect{a}$ and $\vect{b}$, with the first and every third component thereafter chosen from $\vect{a})$ can be described alternatively as shown in Program 1.10b. Since $\textbf{ι}^1 =$ (1, 2, 3, 4, 5, 6, ...), then $(3\textbf{ϵ})$ |_0 $\textbf{ι}^1 =$ (1, 2, 0, 1, 2, 0, ...), and consequently the vector $\vect{u}$ specified by step 1 is of the form $\vect{u} =$ (0, 1, 1, 0, 1, 1, 0, ...).

\par Mesh operations on matrices are defined analogously, row mesh and column mesh being denoted by single and double reverse virgules, respectively.

<a name="catenation"></a>
\par The \textit{catenation} of vectors $\vect{x}, \vect{y}$, ..., $\vect{z}$ is denoted by $\vect{x} \oplus \vect{y} \oplus$ ... 
\oplus \vect{z} and is defined by the relation

\par $\vect{x} \oplus \vect{y} \oplus$ ...
\oplus \vect{z} = (\vect{x}_1, \vect{x}_2, ..., \vect{x}_{\textit{ν}(\vect{x})}, \vect{y}_1, \vect{y}_2, ..., \vect{z}_{\textit{ν}(\vect{z})}).

\par Catenation is clearly associative and for two vectors $\vect{x}$ and $\vect{y}$ it is a special case of the mesh $\backslash\vect{x}, \vect{u}, \vect{y}\backslash$ in which $\vect{u}$ is a suffix vector.

\par In numerical vectors (for which addition of two vectors is defined), the effect of the general mesh operation can be produced as the sum of two meshes, each involving one zero vector. Specifically,

\begin{tabularx}
 \backslash\vect{x}, \vect{u}, \vect{y}\backslash & = & \backslash\vect{x}, \vect{u}, 0\backslash + \backslash0, \vect{u}, \vect{y}\backslash & \\
 & = & \backslash0, \overbar{\vect{u}}, \vect{x}\backslash + \backslash0, \vect{u}, \vect{y}\backslash. & \\
\end{tabularx}

<a name="expansion"></a>
\par The operation $\backslash0, \vect{u}, \vect{y}\backslash$ proves very useful in numerical work and will be called \textit{expansion} of the vector $\vect{y}$, denoted by $\vect{u}\backslash\vect{y}$. Compression of $\vect{u}\backslash\vect{y}$ by $\vect{u}$ and by $\overbar{\vect{u}}$ clearly yields $\vect{y}$ and 0, respectively. Moreover, any numerical vector $\vect{x}$ can be \textit{decomposed} by a compatible vector $\vect{u}$ according to the relation

\par $\vect{x}$ =
\overbar{\vect{u}}\backslash\overbar{\vect{u}}/\vect{x} + \vect{u}\backslash\vect{u}/\vect{x}.

\par The two terms are vectors of the same dimension which have no nonzero components in common. Thus if $\vect{u} =$ (1, 0, 1, 0, 1), the decomposition of $\vect{x}$ appears as

\par $\vect{x} =$ (0, $\vect{x}_2$, 0, $\vect{x}_4$, 0) $+ (\vect{x}_1$, 0, $\vect{x}_3$, 0, $\vect{x}_5)$.

<a name="expand_row"></a>
<a name="expand_col"></a>
\par Row expansion and column expansion of matrices are defined and denoted analogously. The decomposition relations become

\par $\mat{X} =$ 
\overbar{\vect{u}}\backslash\overbar{\vect{u}}/\mat{X} + \vect{u}\backslash\vect{u}/\mat{X},\\
 and\\
 \mat{X} = 
\overbar{\vect{u}}\backslash\backslash\overbar{\vect{u}}/\!/\mat{X} + \vect{u}\backslash\backslash\vect{u}/\!/\mat{X}.

<a name="mask"></a>
\par The \textit{mask} operation is defined formally as follows:

\par $\vect{c} ← /\vect{a}, \vect{u}, \vect{b}/ \leftrightarrow$ 
\overbar{\vect{u}}/\vect{c} = 
\overbar{\vect{u}}/\vect{a}, and \vect{u}/\vect{c} = \vect{u}/\vect{b}.

\par The vectors $\vect{c}, \vect{a}, \vect{u}$, and $\vect{b}$ are clearly of a common dimension and $\vect{c}_i = \vect{a}_i$ or $\vect{b}_i$ according as $\vect{u}_i =$ 0 or $\vect{u}_i =$ 1. Moreover, the compress, expand, mask, and mesh operations on vectors are related as follows:

\begin{tabularx}
 /\vect{a}, \vect{u}, \vect{b}/ & = & \backslash\overbar{\vect{u}}/\vect{a}, \vect{u}, \vect{u}/\vect{b}\backslash, & \\
 \backslash\vect{a}, \vect{u}, \vect{b}\backslash & 
 = & /\overbar{\vect{u}}\backslash\vect{a}, \vect{u}, \vect{u}\backslash\vect{b}/. & \\
\end{tabularx} 
\par Analogous relations hold for the row mask and row mesh and for the column mask and column mesh.

\par Certain selection operations are controlled by logical matrices rather than by logical vectors.
<a name="compress_rowrow"></a> The \textit{row compression} $\mat{U}/\mat{A}$ selects elements of $\mat{A}$ corresponding to the nonzero elements of $\mat{U}$. Since the nonzero elements of $\mat{U}$ may occur in an arbitrary pattern, the result must be construed as a vector rather than a matrix. More precisely, $\mat{U}/\mat{A}$ denotes the catenation of the vectors $\mat{U}^i/\mat{A}^i$ obtained by row-by-row compression of $\mat{A}$ by $\mat{U}$.
<a name="compress_colcol"></a> The \textit{column compression} $\mat{U}/\!/\mat{A}$ denotes the catenation of the vectors $\mat{U}_j/\mat{A}_j$. If, for example

\begin{tabularx}
<td rowspan=3> \mat{U} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & ^{}0 1 0 1 1_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & \\
^{}1 1 0 0 0_{} & \\
^{}0 1 1 0 0_{} & \\
\end{tabularx}

\par $then\\$
 \mat{U}/\mat{A} = (\mat{A}_2^1, \mat{A}_4^1, \mat{A}_5^1, \mat{A}_1^2, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3),\\
 and\\
 \mat{U}/\!/\mat{A} = (\mat{A}_1^2, \mat{A}_2^1, \mat{A}_2^2, \mat{A}_2^3, \mat{A}_3^3, \mat{A}_4^1, \mat{A}_5^1).

\par Compression by the full matrix $\mathsf{\mat{E}}$ (defined by $\overbar{\mathsf{\mat{E}}} =$ 0) produces either a \textit{row list} $(\mathsf{\mat{E}}/\mat{A})$ or a \textit{column list} $(\mathsf{\mat{E}}/\!/\mat{A})$ of the matrix $\mat{A}$. Moreover, a numerical matrix $\mat{X}$ can be represented jointly by the logical matrix $\mat{U}$ and the row list $\mat{U}/\mat{X}$ (or the column list $\mat{U}/\!/\mat{X})$, where $\mat{U} = (\mat{X} \neq$ 0). If the matrix $\mat{X}$ is sparse (i.e., the components are predominantly zero), this provides a compact representation which may reduce the computer storage required for $\mat{X}$.

\par The compression operations controlled by matrices also generate a group of corresponding mesh and mask operations as shown in Sec. S.9.

<a name="1.10"></a>
\par \textbf{1.10 Selection vectors}

\par The logical vector $\vect{u}$ involved in selection operations may itself arise in various ways. It may be a prefix vector $\mathbf{α}^j$, a suffix $\textbf{ω}^j$, or an infix $(i ↓ \mathbf{α}^j)$; the corresponding compressed vectors $\mathbf{α}^j/\vect{x}, \textbf{ω}^j/\vect{x}$, and $(i ↓ \mathbf{α}^j)/\vect{x}$ are called a \textit{prefix}, \textit{suffix}, and \textit{infix} of $\vect{x}$, respectively.

\par Certain selection vectors arise as functions of other vectors, e.g., the vector $(\vect{x} \geq$ 0) can be used to select all nonnegative components of $\vect{x}$, and $(\vect{b} \neq *\textbf{ϵ})$ serves to select all components of $\vect{b}$ which are not equal to the literal ``*''. Two further types are important: the selection of the longest unbroken prefix (or suffix) of a given logical vector, and the selection of the set of distinct components occurring in a vector. The first is useful in left (or right) justification or in a corresponding compression intended to eliminate leading or trailing ``filler components'' of a vector (such as left zeros in a number or right spaces in a short name).

<a name="maximum_prefix"></a>
\par For any logical vector $u$, the \textit{maximum prefix} of $\vect{u}$ is denoted by $α/\vect{u}$ and defined as follows:

\par $\vect{u} ← α/\vect{u} \leftrightarrow \vect{v} = \mathbf{α}^j$

\par where $j$ is the maximum value for which $\wedge/(\mathbf{α}^j/\vect{u}) =$ 1.
<a name="maximum_suffix"></a> The maximum suffix is denoted by $ω/\vect{u}$ and is defined analogously. If, for example, $\vect{u} =$ (1, 1, 1, 0, 1, 1, 0, 0, 1, 1), then $α/\vect{u} =$ (1, 1, 1, 0, 0, 0, 0, 0, 0, 0), $ω/\vect{u} =$ (0, 0, 0, 0, 0, 0, 0, 0, 1, 1), $+/α/\vect{u} =$ 3, and $+/ω/\vect{u} =$ 2.

\par The leading zeros of a numerical vector $x$ can clearly be removed either by compression:

\par <img src="APLimg/exp1x10.bmp">

\par or by left justification (normalization):

\par $\vect{z} ← (+/α/(\vect{x} =$ 0)) $↑ \vect{x}$.

<a name="prefix_row"></a>
<a name="prefix_col"></a>
\par The extension of the maximum prefix operation to the rows of a logical matrix $\mat{U}$ is denoted by $α/\mat{U}$ and defined as the compatible logical matrix $\mat{V}$, such that $\mat{V}^i = α/\mat{U}^i$. The corresponding maximum column prefix operation is denoted by $α/\!/\mat{U}$. Right justification of a numerical matrix $\mat{X}$ is achieved by the rotation $\vect{k} ↓ \mat{X}$, where $\vect{k} = +/ω/(\mat{X} =$ 0), and \textit{top justification} is achieved by the rotation $((+/\!/α/\!/(\mat{X} =$ 0) 
\Uparrow \mat{X} (see Sec. S.6.)

<a name="forward_set_selector"></a>
\par A vector whose components are all distinct will be called an \textit{ordered set}. The \textit{forward set selector} on $\vect{b}$ is a logical vector denoted by $σ/\vect{b}$ and defined as follows: the statement $\vect{v} ← σ/\vect{b}$ implies that $\vect{v}_j =$ 1 if and only if $\vect{b}_j$ differs from all preceding components of $\vect{b}$. Hence $\vect{v}/\vect{b}$ is a set which contains all distinct components of $\vect{b}$, and $+/\vect{v}/\textbf{ι}$ is a minimum. For example, if $\vect{c} =$ (C, a, n, a, d, a), then $(σ/\vect{c})/\vect{c} =$ (C, a, n, d) is a list of the distinct letters in $\vect{c}$ in order of occurrence. Clearly $(σ/\vect{b})/\vect{b} = \vect{b}$ if and only if $\vect{b}$ is a set.

<a name="backward_set_selector"></a>
\par The backward set selector $τ/\vect{b}$ is defined analogously (e.g., $(τ/\vect{c})/\vect{c} =$ (C, n, d, a)).
<a name="set_selector_row"></a>
<a name="set_selector_col"></a> Forward and backward set selection are extended to matrices by both rows $(σ/\mat{B}$, and $τ/\mat{B})$ and columns $(σ/\!/\mat{B}$, and $τ/\!/\mat{B})$ in the established manner.

<a name="1.11"></a>
\par \textbf{1.11 The generalized matrix product}

\par The ordinary matrix product of matrices $\mat{X}$ and $\mat{Y}$ is commonly denoted by $\vect{XY}$ and defined as follows:

\begin{tabularx}
 & \mat{Z} ← \vect{XY} \leftrightarrow \mat{Z}_{\textit{j}}^{\textit{i}} =
<img align=middle src="APLimg/sigma1x11.bmp"> \mat{X}_{\textit{k}}^{\textit{i}} \times \mat{Y}_{\textit{j}}^{\textit{k}}, & 
<img src="APLimg/bracket1x11.bmp"> & 
\textit{i} = 1, 2, ..., \textit{μ}(\mat{X})\\
 \textit{j} = 1, 2, ..., \textit{ν}(\mat{Y}). & \\
\end{tabularx}

It can be defined alternatively as follows:

\par $(\vect{XY})_j^i = +/(\mat{X}^i \times \mat{Y}_j)$.

\par This formulation emphasizes the fact that matrix multiplication incorporates two elementary operations (+, $\times)$ and suggests that they be displayed explicitly. The ordinary matrix product will therefore be written as $\mat{X}$ {+ $\atop \times} \mat{Y}$.

\par More generally, if $\odot_1$ and $\odot_2$ are any two operators (whose domains include the relevant operands), then the \textit{generalized matrix product}
<img align=middle src="APLimg/gmp.bmp"> is defined as follows:

\begin{tabularx}
\par (<img align=middle $src="APLimg/gmp.bmp">)_j^i =$ 
\odot_1/(\mat{X}^{\textit{i}}
\odot_2 \mat{Y}_{\textit{j}}), & 
<img src="APLimg/bracket1x11.bmp"> & 
\textit{i} = 1, 2, ..., \textit{μ}(\mat{X})\\
 \textit{j} = 1, 2, ..., \textit{ν}(\mat{Y}). & \\
\end{tabularx}

For example, if

\begin{tabularx}
 & \begin{tabularx} <td rowspan=3>\mat{A} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & & ^{}1 3 2 0_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & & \\
^{}2 1 0 1_{} & \\
^{}4 0 0 2_{} & \\
\end{tabularx} & and & \begin{tabularx} <td rowspan=4>\mat{B} = & <td rowspan=4><img src="APLimg/matrixl4.bmp"> & & ^{}4 1_{} & <td rowspan=4><img src="APLimg/matrixr4.bmp"> & & \\
^{}0 3_{} & \\
^{}0 2_{} & \\
^{}2 0_{} & \\
\end{tabularx} & \\
\end{tabularx}

then

\begin{tabularx}
 & \begin{tabularx} <td rowspan=3>\mat{A} {+ \atop \times} \mat{B} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & & <td align=right>^{}4 14_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & & <td rowspan=3>, & \\
<td align=right>^{}10 5_{} & \\
<td align=right>^{}20 4_{} & \\
\end{tabularx} & & \begin{tabularx} <td rowspan=3>\mat{A} <img src="APLimg/andeq.bmp"> \mat{B} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & & ^{}0 1_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & & <td rowspan=3>, & \\
^{}0 0_{} & \\
^{}1 0_{} & \\
\end{tabularx} & \\
\end{tabularx}

\begin{tabularx}
 & \begin{tabularx} <td rowspan=3>\mat{A} <img src="APLimg/orne.bmp"> \mat{B} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & & <td align=right>^{}1 0_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & & <td rowspan=3>, and & \\
<td align=right>^{}1 1_{} & \\
<td align=right>^{}0 1_{} & \\
\end{tabularx} & & \begin{tabularx} <td rowspan=3>(\mat{A} \neq 0) <img src="APLimg/plusslash.bmp"> \mat{B} = & <td rowspan=3><img src="APLimg/matrixl3.bmp"> & & ^{}4 6_{} & <td rowspan=3><img src="APLimg/matrixr3.bmp"> & & <td rowspan=3>. & \\
^{}6 4_{} & \\
^{}6 1_{} & \\
\end{tabularx} & \\
\end{tabularx}

\par The generalized matrix product and the selection operations together provide an elegant formulation in several established areas of mathematics. A few examples will be chosen from two such areas, symbolic logic and matrix algebra.

\par In symbolic logic, De Morgan's laws $(\wedge/\vect{u} =$ <img src="APLimg/ex1x10a.bmp"> and $=/\vect{u} =$ <img src="APLimg/demorgan.bmp">) can be applied directly to show that

\par $\mat{U}$ <img src="APLimg/neand.bmp"> $\mat{V} =$ <img src="APLimg/ex1x10f.bmp">.

\par In matrix algebra, the notion of partitioning a matrix into submatrices of contiguous rows and columns can be generalized to an arbitrary partitioning specified by a logical vector $\vect{u}$. The following easily verifiable identities are typical of the useful relations which result:

\begin{tabularx}
 & <td align=right>\mat{X} {+ \atop \times} \mat{Y} & & = & & (\overbar{\vect{u}}/\mat{X}) {+ \atop \times} (\overbar{\vect{u}}/\!/\mat{Y}) + (\vect{u}/\mat{X}) {+ \atop \times} (\vect{u}/\!/\mat{Y}), \\
 & <td align=right>\vect{u}/(\mat{X} {+ \atop \times} \mat{Y}) & & = & & \mat{X} {+ \atop \times} (\vect{u}/\mat{Y}), & \\
 & <td align=right>\vect{u}/\!/(\mat{X} {+ \atop \times} \mat{Y}) & & = & & (\vect{u}/\!/\mat{X}) {+ \atop \times} \mat{Y}. & \\
\end{tabularx}

\par The first identity depends on the commutativity and associativity of the operator $+$ and can clearly be generalized to other associative commutative operators, such as $\wedge, \vee$, and $\neq$.

<a name="gmp_mv"></a>
<a name="gmp_vm"></a>
<a name="gmp_vv"></a>
\par The generalized matrix product applies directly (as does the ordinary matrix product $\mat{X}$ {+ $\atop \times} \mat{Y})$ to vectors considered as row (that is, 1 $\times n)$ or as column matrices. Thus:

\begin{tabularx}
 & \vect{z} ← \mat{X} {\odot_1 \atop \odot_2} \vect{y} & \leftrightarrow & \vect{z}_{\textit{i}} & = & \odot_1/(\mat{X}^{\textit{i}} \odot_2 \vect{y}), & \\
 & \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \mat{X} & \leftrightarrow & \vect{z}_{\textit{j}} & = & \odot_1/(\vect{y} \odot_2 \mat{X}_{\textit{j}}), & \\
 & \vect{z} ← \vect{y} {\odot_1 \atop \odot_2} \vect{x} & \leftrightarrow & \vect{z} & = & \odot_1/(\vect{y} \odot_2 \vect{x}). & \\
\end{tabularx}

\par The question of whether a vector enters a given operation as a row vector or as a column vector is normally settled by the requirement of conformability, and no special indication is required. Thus $\vect{y}$ enters as a column vector in the first of the preceding group of definitions and as a row vector in the last two. The question remains, however, in the case of the two vector operands, which may be considered with the pre-operand either as a row (as in the scalar product $\vect{y}$ {+ $\atop \times} \vect{x})$ or as a column. The latter case produces a matrix $\mat{Z}$ and will be denoted by

\par $\mat{Z} ← \vect{y} {\circ \atop \odot_2}_2 \vect{x}$,

\par where $\mat{Z}_j^i = \vect{y}_i$ 
\odot_2 \vect{x}_{\textit{j}}, \textit{μ}(\mat{Z}) = \textit{ν}(\vect{y}), and \textit{ν}(\mat{Z}) = \textit{ν}(\vect{x}).^{<a href="#note1b">[b]</a>} For example, if each of the vectors indicated is of dimension three, then

\begin{tabularx}
<td rowspan=3> & \begin{tabularx}
<td rowspan=3>\textbf{ϵ} {\circ \atop \times} \vect{y} = & 
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & 
^{}\vect{y}_1 \vect{y}_2 \vect{y}_3 & 
<td rowspan=3><img src="APLimg/matrixr3.bmp"> & 
<td rowspan=3>; & \\
^{}\vect{y}_1 \vect{y}_2 \vect{y}_3 & \\
^{}\vect{y}_1 \vect{y}_2 \vect{y}_3 & \\
\end{tabularx} & & \begin{tabularx}
<td rowspan=3>\vect{y} {\circ \atop \times} \textbf{ϵ} = & 
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & 
^{}\vect{y}_1 \vect{y}_1 \vect{y}_1 & 
<td rowspan=3><img src="APLimg/matrixr3.bmp"> & 
<td rowspan=3>; & \\
^{}\vect{y}_2 \vect{y}_2 \vect{y}_2 & \\
^{}\vect{y}_3 \vect{y}_3 \vect{y}_3 & \\
\end{tabularx} & \\

<td colspan=3 align=center>\begin{tabularx}
<td rowspan=3>\mathbf{α}^2(3) {\circ \atop \wedge} \mathbf{α}^2(3) = & 
<td rowspan=3><img src="APLimg/matrixl3.bmp"> & 
^{}1 1 0_{} & 
<td rowspan=3><img src="APLimg/matrixr3.bmp"> & 
<td rowspan=3>. & \\
^{}1 1 0_{} & \\
^{}0 0 0_{} & \\
\end{tabularx} & \\
\end{tabularx}

<a name="1.12"></a>
\par \textbf{1.12 Transpositions}

\par Since the generalized matrix product is defined on columns of the post-operand and rows of the pre-operand, convenient description of corresponding operations on the rows of the post-operand and columns of the pre-operand demands the ability to \textit{transpose} a matrix $\mat{B}$, that is, to specify a matrix $\mat{C}$ such that $\mat{C}_i^j = \mat{B}_j^i$. In ordinary matrix algebra this type of transposition suffices, but in more general work transpositions about either diagonal and about the horizontal and the vertical are also useful. Each of these transpositions of a matrix $\mat{B}$ is denoted by a superior arrow whose inclination indicates the axis of the transposition. Thus:

\begin{tabularx}
 & \mat{C} ← <img src="APLimg/bnwarr.bmp"> & & \mat{C}_{\textit{i}}^{\textit{j}} = \mat{B}_{\textit{j}}^{\textit{i}} & <td rowspan=4><img src="APLimg/bracket1x12.bmp"> & <td rowspan=4>\textit{i} = 1, 2, ..., \textit{μ}(\mat{B})\\
 \textit{j} = 1, 2, ..., \textit{ν}(\mat{B}) & \\
 & \mat{C} ← <img src="APLimg/bnearr.bmp"> & & <img src="APLimg/1x12a.bmp"> & \\
 & \mat{C} ← <img src="APLimg/brarr.bmp"> & & <img src="APLimg/1x12b.bmp"> & \\
 & \mat{C} ← <img src="APLimg/buarr.bmp"> & & <img src="APLimg/1x12c.bmp"> & \\
\end{tabularx}

<a name="reversal"></a>
\par For a vector $\vect{x}$, either <img src="APLimg/xrarr.bmp"> or <img src="APLimg/xuarr.bmp"> will denote reversal of the order of the components.
<a name="b_tilde"></a> For ordinary matrix transposition (that is, <img src="APLimg/bnwarr.bmp">), the commonly used notation <img src="APLimg/btilde.bmp"> will also be employed.

\par Since transpositions can effect any one or more of three independent alternatives (i.e., interchange of row and column indices or reversal of order of row or of column indices), repeated transpositions can produce eight distinct configurations. There are therefore seven distinct transformations possible; all can be generated by any pair of transpositions having nonperpendicular axes.^{<a href="#note1c">[c]</a>}

<a name="1.13"></a>
\par \textbf{1.13 Special logical matrices}

\par Certain of the special logical vectors introduced in Sec. 1.7 have useful analogs in logical matrices. Dimensions will again be indicated in parentheses (with the column dimension first) and may be elided whenever the dimension is determined by context. If not otherwise specified, a matrix is assumed to be square.

<a name="full_matrix"></a>
<a name="zero_matrix"></a>
\par Cases of obvious interest are the \textit{full matrix}
\mathsf{\mat{E}}(\textit{m} \times \textit{n}), defined by \overbar{\mathsf{\mat{E}}}(\textit{m} \times \textit{n}) = 0,
<a name="identity_matrix"></a> and the \textit{identity matrix} 
\mathsf{\mat{I}}(\textit{m} \times \textit{n}), defined by \mathsf{\mat{I}}_{\textit{j}}^{\textit{i}} = (\textit{i} = \textit{j}).
<a name="superdiagonal_matrix"></a> More generally, \textit{superdiagonal} matrices 
^{\textit{k}}\mathsf{\mat{I}}(\textit{m} \times \textit{n}) are defined such that 
^{\textit{k}}\mathsf{\mat{I}}_{\textit{j}}^{\textit{i}}(\textit{m} \times \textit{n}) = (\textit{j} = \textit{i} + \textit{k}), for \textit{k} \geq 0. Clearly ^{\textit{0}}\mathsf{\mat{I}} =
\mathsf{\mat{I}}. Moreover, for square matrices,
^{\textit{h}}\mathsf{\mat{I}}
{+ \atop \times}
^{\textit{k}}\mathsf{\mat{I}} =
^{(\textit{h} + \textit{k})}\mathsf{\mat{I}}.

<a name="triangular_matrix"></a>
\par Four \textit{triangular} matrices will be defined, the geometrical symbols employed for each indicating the (right-angled isosceles) triangular area of the $m \times n$ rectangular matrix which is occupied by \textit{ones}. Thus

<table cellpadding=3>
 & \mat{C} ← <img src="APLimg/quadnw.bmp">(\textit{m} \times \textit{n}) & \leftrightarrow & \mat{C}_{\textit{j}}^{\textit{i}} & <td rowspan=4><img src="APLimg/bracket1x13.bmp"> & <td rowspan=4>= (\textit{i} + \textit{j} \leq min(\textit{m}, \textit{n})) & <td rowspan=4 align=right>for \textit{i} = 1, 2, ..., \textit{m}\\
 and \textit{j} = 1, 2, ..., \textit{n}. & \\
 & \mat{C} ← <img src="APLimg/quadne.bmp">(\textit{m} \times \textit{n}) & \leftrightarrow & <img src="APLimg/1x13a.bmp"> & \\
 & \mat{C} ← <img src="APLimg/quadsw.bmp">(\textit{m} \times \textit{n}) & \leftrightarrow & <img src="APLimg/1x13b.bmp"> & \\
 & \mat{C} ← <img src="APLimg/quadse.bmp">(\textit{m} \times \textit{n}) & \leftrightarrow & <img src="APLimg/1x13c.bmp"> & \\
\end{tabularx}

\par The use of the matrices $\mathsf{\mat{E}}$ and
\mathsf{\mat{I}} will be illustrated briefly. The relation \vect{u} <img src="APLimg/neand.bmp"> \vect{v} = 2 |_0 (\vect{u} 
{+ \atop \times} \vect{v}) can be extended to logical matrices as follows:

\par $\mat{U}$ <img src="APLimg/neand.bmp"> $\mat{V} = (2\mathsf{\mat{E}})$ |_0 $(\mat{U}$ {+ $\atop \times} \mat{V})$;

\par the trace of a square numerical matrix $\mat{X}$ may be expressed as $t = +/\mathsf{\mat{I}}/\mat{X}$. The triangular matrices are employed in the succeeding section.

<a name="1.14"></a>
\par \textbf{1.14 Polynomials and positional number systems}

\par Any positional representation of a number $n$ in a base $b$ number system can be considered as a numerical vector $\vect{x}$ whose \textit{base b value} is the quantity $n = \vect{w}$ 
{+ \atop \times} \vect{x}, where the \textit{weighting vector} \vect{w} is defined by \vect{w} = (\textit{b}^{\textit{ν}(\vect{x})-1}, \textit{b}^{\textit{ν}(\vect{x})-2}, ... \textit{b}^2, \textit{b}^1, 1). More generally, \vect{x} may represent a number in a mixed-radix system in which the successive radices (from high to low order) are the successive components of a \textit{radix vector} \vect{y}.

<a name="base_value"></a>
\par The \textit{base} $\vect{y}$ \textit{value of} $\vect{x}$ is a scalar denoted by $\vect{y} ⊥ \vect{x}$ and defined as the scalar product $\vect{y} ⊥ \vect{x} = \vect{w}$ {+ $\atop \times} \vect{x}$, where $\vect{w} =$ <img src="APLimg/quadne.bmp">
{\times \atop /} \vect{y} is the weighting vector. For example, if \vect{y} = (7, 24, 60, 60) is the radix vector for the common temporal system of units, and if \vect{x} = (0, 2, 1, 18) represents elapsed time in days, hours, minutes, and seconds, then

\par $t = \vect{w}$ 
{+ \atop \times} \vect{x} = (86400, 3600, 60, 1) {+ \atop \times} (0, 2, 1, 18) = 7278

\par is the elapsed time in seconds, and the weighting vector $\vect{w}$ is obtained as the product

\begin{tabularx}
<td rowspan=4> & 
<td rowspan=4><img src="APLimg/quadne.bmp">
{\times \atop /} \vect{y} = & 
<td rowspan=4><img src="APLimg/matrixl4.bmp"> & 
^{}0 1 1 1_{} & 
<td rowspan=4><img src="APLimg/matrixr4.bmp"> & 
<td rowspan=4> {\times \atop /} & 
<td rowspan=4><img src="APLimg/matrixl4.bmp"> & 
<td align=right>^{}7_{} & 
<td rowspan=4><img src="APLimg/matrixr4.bmp"> & 
<td rowspan=4> = & 
<td rowspan=4><img src="APLimg/matrixl4.bmp"> & 
<td align=left>^{}\times/(24, 60, 60)_{} & 
<td rowspan=4><img src="APLimg/matrixr4.bmp"> & 
<td rowspan=4> = & 
<td rowspan=4><img src="APLimg/matrixl4.bmp"> & 
<td align=right>^{}86400_{} & 
<td rowspan=4><img src="APLimg/matrixr4.bmp"> & \\
 ^{}0 0 1 1_{} & <td align=right>^{}24_{} & <td align=left>^{}\times/(60, 60)_{} & <td align=right>^{}3600_{} & \\
 ^{}0 0 0 1_{} & <td align=right>^{}60_{} & <td align=left>^{}\times/(60)_{} & <td align=right>^{}60_{} & \\
 ^{}0 0 0 0_{} & <td align=right>^{}60_{} & <td align=left>^{}\times/\textbf{ϵ}(0)_{} & <td align=right>^{}1_{} & \\
\end{tabularx}

\par If $b$ is any integer, then the value of $\vect{x}$ in the fixed base $b$ is denoted by $(b\textbf{ϵ}) ⊥ \vect{x}$. For example, $(2\textbf{ϵ}) ⊥ \mathbf{α}^2(5) =$ 24. More generally, if $y$ is any real number, then $(y\textbf{ϵ}) ⊥ \vect{x}$ is clearly a polynomial in $y$ with coefficients $\vect{x}_1, \vect{x}_2$, ... $\vect{x}_ν$, that is,

\par $(y\textbf{ϵ}) ⊥ \vect{x} = \vect{x}_1 y^{ν(\vect{x})-1} +$ ... $+ \vect{x}_{ν-1} y + \vect{x}_ν$ .

\par Writing the definition of $\vect{y} ⊥ \vect{x}$ in the form

\par $\vect{y} ⊥ \vect{x} =$ (<img src="APLimg/quadne.bmp"> ${\times \atop$ /} $\vect{y})$ {+ $\atop \times} \vect{x}$

\par exhibits the fact that the operation $⊥$ is of the double operator type. Its use in the generalized matrix product therefore requires no secondary scan operator. This will be indicated by a null placed over the symbol $⊥$. Thus

\par $\mat{Z} ← \mat{X}$
{\circ \atop ⊥} \mat{Y} \leftrightarrow \mat{Z}_{\textit{j}}^{\textit{i}} = \mat{X}^{\textit{i}} ⊥ \mat{Y}_{\textit{j}}.

\par For example, $(y\textbf{ϵ})$ 
{\circ \atop ⊥} \mat{X} represents a set of polynomials in \textit{y} with coefficients \mat{X}_1, \mat{X}_2, ..., \mat{X}_{\textit{ν}}, and \mat{Y} {\circ \atop ⊥} \vect{x} represents a set of evaluations of the vector \vect{x} in a set of bases \mat{Y}^1, \mat{Y}^2, ..., \mat{Y}^{\textit{μ}}.

<a name="1.15"></a>
\par \textbf{1.15 Set operations}

\par In conventional treatments, such as
<acronym title="Jacobson, N. (1951), Lectures in Abstract Algebra, vol. 1, Van Nostrand, New York.">Jacobson (1951)</acronym> or
<acronym title="Birkhoff, G., and S. MacLane (1941), A Survey of Modern Algebra, Macmillian, New York.">Birkhoff and MacLane (1941)</acronym>, a \textit{set} is defined as an unordered collection of distinct elements. A calculus of sets is then based on such elementary relations as set membership and on such elementary operations as \textit{set intersection} and \textit{set union}, none of which imply or depend on an ordering among members of a set. In the present context it is more fruitful to develop a calculus of \textit{ordered sets}.

\par A vector whose components are all distinct has been called (Sec. 1.10) an \textit{ordered set} and (since no other types are to be considered) will hereafter be called a \textit{set}. In order to provide a closed system, all of the ``set operations'' will, in fact, be defined on vectors. However, the operations will, in the special case of sets, be analogous to classical set operations. The following vectors, the first four of which are sets, will be used for illustration throughout.

\begin{tabularx}
 & \vect{t} = (t, e, a) & \\
 & \vect{a} = (a, t, e) & \\
 & \vect{s} = (s, a, t, e, d) & \\
 & \vect{d} = (d, u, s, k) & \\
 & \vect{n} = (n, o, n, s, e, t) & \\
 & \vect{r} = (r, e, d, u, n, d, a, n, t) & \\
\end{tabularx}

<a name="membership"></a>
\par A variable $z$ is a \textit{member} of a vector $\vect{x}$ if $z = \vect{x}_i$ for some $i$. Membership is denoted by $z ϵ \vect{x}$.
<a name="inclusion"></a> A vector $\vect{x}$ \textit{includes} a vector $\vect{y}$ (denoted by either $\vect{x} \supseteq \vect{y}$ or $\vect{y} \subseteq \vect{x})$ if each element $\vect{y}_i$ is a member of $\vect{x}$.
<a name="similarity"></a> If both $\vect{x} \supseteq \vect{y}$ and $\vect{x} \subseteq \vect{y}$, then $\vect{x}$ and $\vect{y}$ are said to be \textit{similar}. Similarity of $\vect{x}$ and $\vect{y}$ is denoted by $\vect{x} ≡ \vect{y}$. For example, $\vect{t} \subseteq \vect{s}, \vect{t} \subseteq \vect{r}, \vect{t} \subseteq \vect{a}, \vect{a} \subseteq \vect{t}, \vect{t} ≡ \vect{a}$, and $\vect{t} ≢ \vect{r}$.
<a name="strict_inclusion"></a> If $\vect{x} \subseteq \vect{y}$ and $\vect{x} ≢ \vect{y}$, then $\vect{x}$ is \textit{strictly} included in $\vect{y}$. Strict inclusion is denoted by $\vect{x} ⊂ \vect{y}$.

<a name="characteristic_vector"></a>
\par The \textit{characteristic vector} of $\vect{x}$ on $\vect{y}$ is a logical vector denoted by $\textbf{ϵ}_{\vect{y}}^{\vect{x}}$, and defined as follows:

\par $\vect{u} = \textbf{ϵ}_{\vect{y}}^{\vect{x}} \leftrightarrow ν(\vect{u}) = ν(\vect{y})$, and $\vect{u}_j = (\vect{y}_j ϵ \vect{x})$.

\par For example, $\textbf{ϵ}_{\vect{s}}^{\vect{t}} =$ (0, 1, 1, 1, 0), $\textbf{ϵ}_{\vect{t}}^{\vect{s}} =$ (1, 1, 1), $\textbf{ϵ}_{\vect{s}}^{\vect{d}} =$ (1, 0, 0, 0, 1), $\textbf{ϵ}_{\vect{d}}^{\vect{s}} =$ (1, 0, 1, 0), and $\textbf{ϵ}_{\vect{n}}^{\vect{r}} =$ (1, 0, 1, 0, 1, 1). 

<a name="intersection"></a>
\par The intersection of $\vect{y}$ with $\vect{x}$ is denoted by $\vect{y} ∩ \vect{x}$, and defined as follows:

\par $\vect{y} ∩ \vect{x} = \textbf{ϵ}_{\vect{y}}^{\vect{x}}/\vect{y}$.

\par For example, $\vect{s} ∩ \vect{d} =$ (s, d), $\vect{d} ∩ \vect{s} =$ (d, s), $\vect{s} ∩ \vect{r} =$ (a, t, e, d), and $\vect{r} ∩ \vect{s} =$ (e, d, d, a, t). Clearly, $\vect{x} ∩ \vect{y} ≡ \vect{y} ∩ \vect{x}$, although $\vect{x} ∩ \vect{y}$ is not, in general, equal to $\vect{y} ∩ \vect{x}$, since the components may occur in a different order and may be repeated a different number of times. The vector $\vect{x} ∩ \vect{y}$ is said to be \textit{ordered} on $\vect{x}$. Thus $\vect{a}$ is ordered on $\vect{s}$. If $\vect{x}$ and $\vect{y}$ contain no common elements (that is, $(\vect{x} ∩ \vect{y}) = \textbf{ϵ}(0))$, they are said to be \textit{disjoint}.

<a name="difference"></a>
\par The \textit{set difference} of $\vect{y}$ and $\vect{x}$ is denoted by $\vect{y} ∆ \vect{x}$ and is defined as follows:

\par $\vect{y} ∆ \vect{x} =$ 
\overbar{\textbf{ϵ}}_{\vect{y}}^{\vect{x}}/\vect{y}.

\par Hence $\vect{y} ∆ \vect{x}$ is obtained from $\vect{y}$ by suppressing those components which belong to $\vect{x}$. For example, 
\overbar{\textbf{ϵ}}_{\vect{s}}^{\vect{t}} = (1, 0, 0, 0, 1) and \vect{s} ∆ \vect{t} = (s, d). Moreover, 
\overbar{\textbf{ϵ}}_{\vect{t}}^{\vect{s}} = (0, 0, 0) and \vect{t} ∆ \vect{s} = \textbf{ϵ}(0).

<a name="union"></a>
\par The \textit{union} of $\vect{y}$ and $\vect{x}$ is denoted by $\vect{y} ∪ \vect{x}$ and defined as follows:^{<a href="#note1d">[d]</a>} $\vect{y} ∪ \vect{x} = \vect{y} \oplus (\vect{x} ∆ \vect{y})$. For example, $\vect{s} ∪ \vect{d} =$ (s, a, t, e, d, u, k), $\vect{d} ∪ \vect{s} =$ (d, u, s, k, a, t, e), $\vect{s} ∪ \vect{a} = \vect{s} ∪ \vect{t} = \vect{s}$, and $\vect{n} ∪ \vect{t} =$ (n, o, n, s, e, t, a). In general, $\vect{x} ∪ \vect{y} ≡ \vect{y} ∪ \vect{x}$, and $\vect{x} ≡ (\vect{x} ∩ \vect{y}) ∪ (\vect{x} ∆ \vect{y})$. If $\vect{x}$ and $\vect{y}$ are disjoint, their union is equivalent to their catenation, that is, $\vect{x} ∩ \vect{y} = \textbf{ϵ}(0)$ implies that $\vect{x} ∪ \vect{y} = \vect{x} \oplus \vect{y}$.

\par In the foregoing development, the concepts of inclusion and similarity are equivalent to the concepts of inclusion and equality in the conventional treatment of (unordered) sets. The remaining definitions of intersection, difference, and union differ from the usual formulation in that the result of any of these operations on a pair of ordered sets is again an \textit{ordered} set. With respect to \textit{similarity}, these operations satisfy the same identities as do the analogous conventional set operations on unordered sets with respect to equality.

\par The forward selection $σ/\vect{b}$ and the backward selection $τ/\vect{b}$ defined in Sec. 1.10 can both be used to reduce any vector $\vect{b}$ to a similar set, that is,

\par $(σ/\vect{b})/\vect{b} ≡ (τ/\vect{b})/\vect{b} ≡ \vect{b}$.

\par Moreover, if $\vect{f} = (σ/\vect{x})/\vect{b}, \vect{g} = (σ/\vect{y})/\vect{y}$, and $\vect{h} = (σ/\vect{z})/\vect{z}$, then $\vect{x} = \vect{y} ∩ \vect{z}$ implies that $\vect{f} = \vect{g} ∩ \vect{h}$, and $\vect{x} = \vect{y} ∪ \vect{z}$ implies that $\vect{f} = \vect{g} ∪ \vect{h}$.

\par The unit vector $\textbf{ϵ}^j(n)$ will be recognized as a special case of the characteristic vector $\textbf{ϵ}_{\vect{y}}^{\vect{x}}$ in which $\vect{x}$ consists of the single component $j$, and $\vect{y} = \textbf{ι}^h(n)$, where $h$ is the index origin in use. In fact, the notation $\textbf{ϵ}^j_{i^h}$ can be used to make explicit the index origin $h$ assumed for $\textbf{ϵ}^j$.

<a name="cartesian"></a>
\par If $\vect{z}$ is any vector of dimension two such that $\vect{z}_1 ϵ \vect{x}$ and $\vect{z}_2 ϵ \vect{y}$, then $\vect{z}$ is said to belong to the \textit{Cartesian product} of $\vect{x}$ and $\vect{y}$. Thus if $\vect{x} =$ (a, b, c) and $\vect{y} =$ (0, 1), the rows of the matrix

\begin{tabularx}
 <td rowspan=6> & <td rowspan=6>\mat{A} = <td rowspan=6><img src="APLimg/matrixl6.bmp"> & ^{}a 0_{} & <td rowspan=6><img src="APLimg/matrixr6.bmp"> & \\
^{}a 1_{} & \\
^{}b 0_{} & \\
^{}b 1_{} & \\
^{}c 0_{} & \\
^{}c 1_{} & \\
\end{tabularx}

\par are a complete list of the vectors $\vect{z}$ belonging to the product set of $\vect{x}$ and $\vect{y}$. The matrix $\mat{A}$ will be called the Cartesian product of $\vect{x}$ and $\vect{y}$ and will be denoted $\vect{x}$ <img src="APLimg/circletimes.bmp"> $\vect{y}$.

\par The foregoing definition by example will be formalized in a more general way that admits the Cartesian product of several vectors (that is, $\vect{u}$ <img src="APLimg/circletimes.bmp"> $\vect{v}$ <img src="APLimg/circletimes.bmp"> ... <img src="APLimg/circletimes.bmp"> $\vect{y})$ which need not be sets, and which specifies a unique ordering of the rows of the resulting matrix. Consider a family of vectors $\vect{x}^1, \vect{x}^2$, ..., $\vect{x}^s$ of dimensions $\vect{d}_1, \vect{d}_2$, ..., $\vect{d}_s$. Then

\par $\mat{A} ← \vect{x}^1$ <img src="APLimg/circletimes.bmp"> $\vect{x}^2$ <img src="APLimg/circletimes.bmp"> ... <img src="APLimg/circletimes.bmp"> $\vect{x}^s \leftrightarrow \mat{A}^{1 + \vect{d} ⊥ (\vect{k} - \textbf{ϵ})} = (\vect{x}^1_{\vect{k}_1}, \vect{x}^2_{\vect{k}_2}$, ..., $\vect{x}^s_{\vect{k}_s})$,

\par for all vectors $\vect{k}$ such that 1 $\leq \vect{k}_i \leq \vect{d}_i$. Clearly, $ν(\mat{A}) = s$, and $μ(\mat{A}) = \times/\vect{d}$. As illustrated by Table 1.11, the rows of the Cartesian product $\mat{A}$ are not distinct if any one of the vectors $\vect{x}^i$ is not a set.

\begin{tabularx}
 & \begin{tabularx}
\vect{x}^1 & = (a, b, a) & \\
\vect{x}^2 & = (#, *) & \\
\vect{x}^3 & = (0, 1) & \\
\vect{d} & = (3, 2, 2) & \\
\end{tabularx} & & \begin{tabularx}
 <td rowspan=12>\mat{A} = & <td rowspan=12><img src="APLimg/matrixl1x15.bmp"> a # 0 & <td rowspan=12><img src="APLimg/matrixr1x15.bmp">\\
a # 1 & \\
a * 0 & \\
a * 1 & \\
b # 0 & \\
b # 1 & \\
b * 0 & \\
b * 1 & \\
a # 0 & \\
a # 1 & \\
a * 0 & \\
a * 1 & \\
\end{tabularx} & \\
\end{tabularx}

\par \textbf{Table 1.11} The Cartesian product $\mat{A} = \vect{x}^1$ 
<img src="APLimg/circletimes.bmp"> \vect{x}^2 
<img src="APLimg/circletimes.bmp"> \vect{x}^3

<a name="cp_reduce"></a>
\par If the vectors $\vect{x}^i$ are all of the the same dimension, they may be considered as the columns of a matrix $\mat{X}$, that is, $\mat{X}_i = \vect{x}^i$. The product $\vect{x}^1$ 
<img src="APLimg/circletimes.bmp"> \vect{x}^2
<img src="APLimg/circletimes.bmp"> ... 
<img src="APLimg/circletimes.bmp"> \vect{x}^{\textit{s}} = \mat{X}_1 
<img src="APLimg/circletimes.bmp"> \mat{X}_2
<img src="APLimg/circletimes.bmp"> ... 
<img src="APLimg/circletimes.bmp"> \mat{X}_{\textit{s}} may then be defined by <img src="APLimg/circletimes.bmp">/\mat{X}, or alternatively by <img src="APLimg/circletimes.bmp">/\!/\mat{Y}, where \mat{Y} is the transpose of \mat{X}. For example, if

\begin{tabularx}<td rowspan=2> \mat{X} = \textbf{ι}^0(2) 
{\circ \atop \wedge} \textbf{ϵ}(3) = & 
<td rowspan=2><img src="APLimg/matrixl2.bmp"> & 
^{}0 0 0_{} & 
<td rowspan=2><img src="APLimg/matrixr2.bmp"> & 
<td rowspan=2>, & 

^{}1 1 1_{} & \\
\end{tabularx}

\par then <img $src="APLimg/circletimes.bmp">/\mat{X}$ is the matrix of arguments of the truth table for three variables.

<a name="1.16"></a>
\par \textbf{1.16 Ranking}

\par The \textit{rank} or \textit{index} of an element $c ϵ \vect{b}$ is called the $\vect{b}$ \textit{index of c} and is defined as the smallest value of $i$ such that $c = \vect{b}_i$. To establish a closed system, the $\vect{b}$ index of any element $\vect{a}$ <img src="APLimg/noteps.bmp"> $\vect{b}$ will be defined as the null characer $∘$. The $\vect{b}$ index of any element $c$ will be denoted by $\vect{b} ι c$; if necessary, the index origin in use will be indicated by a subscript appended to the operator $ι$. Thus, if $\vect{b} =$ (a, p, e), $\vect{b} ι_0$ p $=$ 1, and $\vect{b} ι_1$ p $=$ 2.

\par The $\vect{b}$ index of a vector $\vect{c}$ is defined as follows:

\par $\vect{k} ← \vect{b} ι \vect{c} \leftrightarrow \vect{k}_i = \vect{b} ι \vect{c}_i.\par$ 

<a name="iota_row"></a>
<a name="iota_col"></a>
\par The extension to matrices may be either row by row or (as indicated by a doubled operator symbol $ιι)$ column by column, as follows:

\begin{tabularx}
 & \mat{J} ← \mat{B} ι \mat{C} & \leftrightarrow & \mat{J}^{\textit{i}} ← \mat{B}^{\textit{i}} ι \mat{C}^{\textit{i}}, & \\
 & \mat{K} ← \mat{B} ιι \mat{C} & \leftrightarrow & \mat{K}_{\textit{j}} ← \mat{B}_{\textit{j}} ι \mat{C}_{\textit{j}}. & \\
\end{tabularx}

\par Use of the ranking operator in a matrix product requires no secondary scan and is therefore indicated by a superior null symbol. Moreover, since the result must be limited to a two-dimensional array (matrix), either the pre- or post-operand is required to be a vector. Hence

\begin{tabularx}
 & \mat{J} ← \mat{B} <img src="APLimg/jotiota.bmp"> \vect{c} & \leftrightarrow & \mat{J}^{\textit{i}} ← \mat{B}^{\textit{i}} ι \vect{c}^{\textit{i}}, & \\
 & \mat{K} ← \vect{b} <img src="APLimg/jotiota.bmp"> \mat{C} & \leftrightarrow & \mat{K}_{\textit{j}} ← \vect{b}_{\textit{j}} ι \mat{C}_{\textit{j}}. & \\
\end{tabularx}

\par The first of these ranks the components of $\vect{c}$ with respect to each of a set of vectors $\mat{B}^1, \mat{B}^2$, ..., $\mat{B}^μ$, whereas the second ranks each of the vectors $\mat{C}_1, \mat{C}_2$, ..., $\mat{C}_ν$ with respect to the fixed vector $\vect{b}$. 

\par The use of the ranking operation can be illustrated as follows. Consider the vector $\vect{b} =$ (a, b, c, d, e) and the set of all 3^5 three-letter sequences (vectors) formed from its components. If the set is ordered lexically, and if $\vect{x}$ is the $j$th member of the set (counting from zero), then

\par $j = (ν(\vect{b})\textbf{ϵ}) ⊥ (\vect{b} ι_0 \vect{x})$.

\par For example, if $\vect{x} =$ (c, a, b), then $(\vect{b} ι_0 \vect{x}) =$ (2, 0, 1), and $j =$ 51.

<a name="1.17"></a>
\par \textbf{1.17 Mapping and permutation}

<a name="1.17.1"></a>
\par \textbf{Reordering operations}

\par The selection operations employed thus far do not permit convenient reorderings of the components. This is provided by the \textit{mapping} operation defined as follows:^{<a href="#note1e">[e]</a>}

\par $\vect{c} ← \vect{a}_{ \vect{k}} \leftrightarrow \vect{c}_i = \vect{a}_{ \vect{k}_i}$

\par For example, if $\vect{a} =$ (a, b, ..., z) and $\vect{k} =$ (6, 5, 4), then $\vect{c} =$ (f, e, d).

\par The foregoing definition is meaningful only if the components of $\vect{k}$ each lie in the range of the indices of $\vect{a}$, and it will be extended by defining $\vect{a}_j$ as the null element $∘$ if $j$ does not belong to the index set of $\vect{a}$. Formally,

\begin{tabularx}
<td rowspan=2> & <td rowspan=2>\vect{c} ← \vect{a}_{ \vect{m}} \leftrightarrow \vect{c}_{\textit{i}} = & <td rowspan=2> & <td rowspan=2><img src="APLimg/bracket1x17.bmp"> & \vect{a}_{ \vect{m}_{\textit{i}}} & <td rowspan=2> & if \vect{m}_{\textit{i}} ϵ \textbf{ι}^1(\textit{ν}(\vect{a})) & 
∘ & if \vect{m}_{\textit{i}} <img src="APLimg/noteps.bmp"> \textbf{ι}^1(\textit{ν}(\vect{a})). & 
\end{tabularx}

\par The ability to specify an arbitrary index origin for the vector $\vect{a}$ being mapped is provided by the following alternative notation for mapping:

\begin{tabularx}
<td rowspan=2> & <td rowspan=2>\vect{c} ← \vect{m} \int_{\textit{j}} \vect{a} \leftrightarrow \vect{c}_{\textit{i}} = & <td rowspan=2> & <td rowspan=2><img src="APLimg/bracket1x17.bmp"> & \vect{a}_{ \vect{m}_{\textit{i}}} & <td rowspan=2> & if \vect{m}_{\textit{i}} ϵ \textbf{ι}^{\textit{j}}(\textit{ν}(\vect{a})) & 
∘ & if \vect{m}_{\textit{i}} <img src="APLimg/noteps.bmp"> \textbf{ι}^{\textit{j}}(\textit{ν}(\vect{a})), & 
\end{tabularx}

\par where $j$-origin indexing is assumed for the vector $\vect{a}$. For example, if $\vect{a}$ is the alphabet and $\vect{m} =$ (5, $∘, ∘$, 4, 27, $∘$, 3), then $\vect{c} = \vect{m} \int_0 \vect{a} =$ (f, $∘, ∘$, e, $∘, ∘$, d), and $(\vect{c} \neq ∘\textbf{ϵ})/\vect{c} =$ (f, e, d). Moreover, $\vect{m} \int_2 \vect{a} =$ (d, $∘, ∘$, c, z, $∘$, b). Elision of $j$ is permitted.

\par If $\vect{a} ⊆ \vect{b}$, and $\vect{m} = \vect{b} ι_j \vect{a}$, then clearly $\vect{m} \int_j \vect{b} = \vect{a}$. If $\vect{a}$ <img src="APLimg/notsube.bmp"> $\vect{b}$, then $\vect{m} \int_j \vect{b}$ contains (in addition to certain nulls) those components common to $\vect{b}$ and $\vect{a}$, arranged in the order in which they occur in $\vect{a}$. In other words,

\par $(\vect{m} \neq ∘\textbf{ϵ})/(\vect{m} \int_j \vect{b}) = \vect{a} ∩ \vect{b}$.

\par Consequently, if $\vect{p}, \vect{q}$, ..., $\vect{t}$ are vectors, each contained in $\vect{b}$, then each can be represented jointly by the vector $\vect{b}$ and a mapping vector. If, for example, $\vect{b}$ is a glossary and $\vect{p}, \vect{q}$, etc. are texts, the total storage required for $\vect{b}$ and the mapping vectors might be considerably less than for the entire set of texts.

\par Mapping may be shown to be associative, that is, $\vect{m}^1 \int_i (\vect{m}^2 \int_j \vect{a}) = (\vect{m}^1 \int_i \vect{m}^2) \int_j \vect{a}$. Mapping is not, in general, commutative.

<a name="index_row"></a>
<a name="index_col"></a>
\par Mapping is extended to matrices as follows:

\begin{tabularx}
 & ^{}\mat{A} ← \mat{M} \int_{\textit{h}} \mat{B} & \leftrightarrow & \mat{A}^{\textit{i}} = \mat{M}^{\textit{i}} \int_{\textit{h}} \mat{B}^{\textit{i}}, & \\
 & _{}\mat{C} ← \mat{M} \int\int_{\textit{h}} \mat{B} & \leftrightarrow & \mat{C}_{\textit{j}} = \mat{M}_{\textit{j}} \int_{\textit{h}} \mat{B}_{\textit{j}}. & \\
\end{tabularx}

\par Row and column mappings are associative. A row mapping $^1\mat{M}$ and a column mapping $^2\mat{M}$ do not, in general, commute, but do if all rows of $^1\mat{M}$ agree (that is, $^1\mat{M} = \textbf{ϵ}$ 
{\circ \atop \times} \vect{p}), and if all columns of ^2\mat{M} agree (that is, ^2\mat{M} = \vect{q} 
{\circ \atop \times} \textbf{ϵ}). The generalized matrix product is defined for the cases \mat{M} <img src="APLimg/jotindex.bmp"> \mat{A}, and \mat{M} <img src="APLimg/jotindex.bmp"> \vect{a}.

<a name="sub_row"></a>
<a name="sub_col"></a>
\par The alternative notation (that is, $\vect{c} = \vect{a}_{\vect{m}})$, which does not incorporate specification of the index origin, is particularly convenient for matrices and is extended as follows:

\begin{tabularx}
 & \mat{A} ← \mat{B}^{\vect{m}} & \leftrightarrow & \mat{A}^{\textit{i}} = \mat{B}^{\vect{m}_{\textit{i}}}, & \\
 & \mat{A} ← \mat{B}_{\vect{m}} & \leftrightarrow & \mat{A}_{\textit{i}} = \mat{B}_{\vect{m}_{\textit{i}}}. & \\
\end{tabularx}

<a name="1.17.2"></a>
\par \textbf{Permutations}

\par A vector $\vect{k}$ of dimension $n$ is called a $j$-origin \textit{permutation vector} if $\vect{k} ≡ \textbf{ι}^j(n)$. A permutation vector used to map any set of the same dimension produces a reordering of the set without either repetition or suppression of elements, that is, $\vect{k} \int_j \vect{a} ≡ \vect{a}$ for any set $\vect{a}$ of dimension $ν(\vect{k})$. For example, if $\vect{a} =$ (f, 4, *, 6, z), and $\vect{k} =$ (4, 2, 5, 1, 3), then $\vect{k} \int_1 \vect{a} =$ (6, 4, z, f, *).

\par If $\vect{p}$ is an $h$-origin permutation vector and $\vect{q}$ is any $j$-origin permutation vector of the same dimension, then $\vect{q} \int_j \vect{p}$ is an $h$-origin permutation vector.

<a name="identity_permutation"></a>
\par Since

\par $\textbf{ι}^j(ν(\vect{a})) \int_j \vect{a} = \vect{a}$,

\par the interval vector $\textbf{ι}^j(n)$ will also be called the \textit{j-origin identity permutation vector}. If $\vect{p}$ and $\vect{q}$ are two $j$-origin permutation vectors of the same dimension $n$ and if $\vect{q} \int_j \vect{p} = \textbf{ι}^j(n)$, then $\vect{p} \int_j \vect{q} = \textbf{ι}^j(n)$ also and $\vect{p}$ and $\vect{q}$ are said to be \textit{inverse} permutations. If $\vect{p}$ is any $j$-origin permutation vector, then $\vect{q} = \vect{p} ι_j \textbf{ι}^j$ is inverse to $\vect{p}$.

\par The rotation operation $k ↑ \vect{x}$ is a special case of permutation.

<a name="1.17.3"></a>
\par \textbf{Function mapping}

\par A function $f$ which defines for each element $\vect{b}_i$ of a set $\vect{b}$ a unique correspondent $\vect{a}_k$ in a set $\vect{a}$ is called a \textit{mapping from} $\vect{b} to \vect{a}$. If $f(\vect{b}_i) = \vect{a}_k$, the element $\vect{b}_i$ is said to \textit{map into} the element $\vect{a}_k$. If the elements $f(\vect{b}_i)$ exhaust the set $\vect{a}$, the function $f$ is set to map $\vect{b}$ onto $\vect{a}$. If $\vect{b}$ maps onto $\vect{a}$ and the elements $f(\vect{b}_i)$ are all distinct, the mapping is said to be one-to-one or \textit{biunique}. In this case, $ν(\vect{a}) = ν(\vect{b})$, and there exists an inverse mapping from $\vect{a}$ to $\vect{b}$ with the same correspondences.

\par A program for performing the mapping $f$ from $\vect{b}$ to $\vect{a}$ must therefore determine for any given element $b ϵ \vect{b}$, the correspondent $a ϵ \vect{a}$, such that $a = f(b)$. Because of the convenience of operating upon integers (e.g., upon register addresses or other numeric symbols) in the automatic execution of programs, the mapping is frequently performed in three successive phases, determining in turn the following quantities:

\begin{tabularx}
<td valign=top>1. & & the index \textit{i} = \vect{b} ι \textit{b}, & \\
<td valign=top>2. & & the index \textit{k} such that \vect{a}_{\textit{k}} = \textit{f}(\vect{b}_{\textit{i}}), & \\
<td valign=top>3. & & the element \vect{a}_{\textit{k}}. & \\
\end{tabularx}

\par The three phases are shown in detail in Program 1.12a. The ranking is performed (steps 1-3) by scanning the set $\vect{b}$ in order and comparing each element with the argument $b$. The second phase is a permutation of the integers 1, 2, ..., $ν(\vect{b})$, which may be described by a permutation vector $\vect{j}$, such that $\vect{j}_i = k$. The selection of $\vect{j}_i$ (step 4) then defines $k$, which, in turn, determines the selection of $\vect{a}_k$ on step 5.

\begin{tabularx} & 
\par \textbf{Example 1.2}. If

\begin{tabularx}
 & \vect{b} = (apple, booty, dust, eye, night), & \\
 & \vect{a} = (Apfel, Auge, Beute, Nacht, Staub) & \\
\end{tabularx}

\par are, respectively, a set of English words and a set of German correspondents (both in alphabetical order), and if the function required is the mapping of a given English word $b$ into its German equivalent $a$ according to the dictionary correspondences:

\begin{tabularx}
 & English: & & apple & booty & dust & eye & night & \\
 & English: & & Apfel & Beute & Staub & Auge & Nacht & \\
\end{tabularx}

\par then $\vect{j} =$ (1, 3, 5, 2, 4). If $b =$ ``night'', then $i =$ 5, $\vect{j}_i =$ 4, and $a = \vect{a}_4 =$ Nacht.
 & & \end{tabularx}

\par If $\vect{k}$ is a permutation vector inverse to $\vect{j}$, then Program 1.12b describes a mapping inverse to that of Program 1.12a. If $\vect{j} =$ (1, 3, 5, 2, 4), then $\vect{k} =$ (1, 4, 2, 5, 3). The inverse mapping can also be described in terms of $\vect{j}$, as is done in Program 1.12c. The selection of the $i$th component of the permutation vector is then necessarily replaced by a scan of its components. Programs 1.12d and 1.12e show alternative formulations of Program 1.12a.

<table align=center>\begin{tabularx}
<img src="APLimg/prog1x12a.bmp"> & \\
<td align=center>(a) \vect{b}_{\textit{i}} \leftrightarrow \vect{a}_{\vect{j}_{\textit{i}}} & \\
\end{tabularx} & & \begin{tabularx}
<img src="APLimg/prog1x12b.bmp"> & \\
<td align=center>(b) \vect{a}_{\textit{i}} \leftrightarrow \vect{b}_{\vect{k}_{\textit{i}}} & \\
\end{tabularx} & \\

<td rowspan=2>\begin{tabularx}
<img src="APLimg/prog1x12c.bmp"> & \\
<td align=center>(c) \vect{a}_{\textit{i}} \leftrightarrow \vect{b}_{\vect{k}_{\textit{i}}} & \\
\end{tabularx} & & \begin{tabularx}
<img src="APLimg/prog1x12d.bmp"> & \\
<td align=center>(d) \vect{b}_{\textit{i}} \leftrightarrow \vect{a}_{\vect{j}_{\textit{i}}} \\

\end{tabularx} & \\
 & \begin{tabularx}
<img src="APLimg/prog1x12e.bmp"> & \\
<td align=center>(e) \vect{b}_{\textit{i}} \leftrightarrow \vect{a}_{\vect{j}_{\textit{i}}} & \\
\end{tabularx} & \\
\end{tabularx}

<table border=1 cellspacing=0 cellpadding=5 align=center width=65%>\begin{tabularx}
<td valign=top>\vect{a} & <td nowrap> & Set of correspondents in Programs (a, d, e) and set of arguments in Programs (b, c).

<td valign=top>\vect{b} & & Set of arguments in Programs (a, d, e) and set of correspondents in Programs (b, c).

<td valign=top nowrap>\vect{j}, \vect{k} & & Mutually inverse permutation vectors.

\end{tabularx} & \\
\end{tabularx}
\par \textbf{Legend}

\par \textbf{Program 1.12} Mapping defined by a permutation vector $j$

<a name="1.17.4"></a>
\par \textbf{Ordering vector}

\par If $\vect{x}$ is a numeric vector and $\vect{k}$ is a $j$-origin permutation vector such that the components of $\vect{y} = \vect{k} \int_j \vect{x}$ are in ascending order, then $\vect{k}$ is said to \textit{order} $\vect{x}$. The vector $\vect{k}$ can be determined by an ordering operation defined as follows:

\par $\vect{k} ← θ_j/\vect{x}$

\par implies that $\vect{k}$ is a $j$-origin permutation vector, and that if $\vect{y} = \vect{k} \int_j \vect{x}$, then either $\vect{y}_i < \vect{y}_{i+1}$ or $\vect{y}_i = \vect{y}_{i+1}$ and $\vect{k}_i < \vect{k}_{i+1}$. The resulting vector $\vect{k}$ is unique and preserves the original relative order among equal components. For example, if $\vect{x} =$ (7, 3, 5, 3), then $θ_1/\vect{x} =$ (2, 4, 3, 1).

\par The ordering operation is extended to arbitrary vectors by treating all nonnumeric quantities as equal and as greater than any numeric quantity. For example, if $\vect{a} =$ (7, $∘, 3,∘$, 5, 3), then $θ_1/\vect{a} =$ (3, 6, 5, 1, 2, 4), and if $\vect{b}$ is any vector with no numerical components, then $θ_j/\vect{b} = \textbf{ι}^j(ν(\vect{b}))$.

\par Ordering of a vector $\vect{a}$ with respect to a vector $\vect{b}$ is achieved by ordering the $\vect{b}$-index of $\vect{a}$. For example, if $\vect{a} =$ (e, a, s, t, 4, 7, t, h), and $\vect{b}$ is the alphabet, then $\vect{m} = \vect{b} ι_1 \vect{a} =$ (5, 1, 19, 20, $∘, ∘$, 20, 8) and $θ_1/\vect{m} =$ (2, 1, 8, 3, 4, 7, 5, 6).

<a name="theta_col"></a>
\par The ordering operation is extended to matrices by the usual convention. If $\mat{K} = θ_j/\!/\mat{A}$, then each column of the matrix $\mat{B} = \mat{K} \int\int_j \mat{A}$ is in ascending order.

<a name="1.18"></a>
\par \textbf{1.18 Maximization}

\par In determining the maximum $m$ over components of a numerical vector $x$, it is often necessary to determine the indices of the maximum components as well. The maximization operator is therefore defined so as to determine a logical vector $\vect{v}$ such that $\vect{v}/\vect{x} = m\textbf{ϵ}$.

\par Maximization over the entire vector $\vect{x}$ is denoted by $\textbf{ϵ}⌈\vect{x}$, and is defined as follows: if $\vect{v} = \textbf{ϵ}⌈\vect{x}$, then there exists a quantity $m$ such that $\vect{v}/\vect{x} = m\textbf{ϵ}$ and such that all components of 
\overbar{\vect{v}}/\vect{x} are strictly less than \textit{m}. The maximization is assumed by a single component of \vect{x} if and only if +/\vect{v} = 1. The actual value of the maximum is given by the first (or any) component of \vect{v}/\vect{x}. Moreover, the \textit{j}-origin indices of the maximum components are the components of the vector \vect{v}/\textbf{ι}^{\textit{j}}.

<a name="maximum_selector"></a>
\par More generally, the maximization operation $\vect{v} ← \vect{u}⌈\vect{x}$ will be defined so as to determine the maximum over the subvector $\vect{u}/\vect{x}$ only, but to express the result $\vect{v}$ with respect to the entire vector $\vect{x}$. More precisely,

\par $\vect{v} ← \vect{u}⌈\vect{x} \leftrightarrow \vect{v} = \vect{u}\backslash(\textbf{ϵ}⌈(\vect{u}/\vect{x}))$.

\par The operation may be visualized as follows --- a horizontal plane punched at points corresponding to the zeros of $\vect{u}$ is lowered over a plot of the components of $\vect{x}$, and the positions at which the plane first touches them are the positions of the unit components of $\vect{v}$. For example, maximization over the negative components of $\vect{x}$ is denoted by

\par $\vect{v} ← (\vect{x} < 0)⌈\vect{x}$

\par and if $\vect{x} =$ (2, -3, 7, -5, 4, -3, 6), then $(\vect{x} <$ 0) $=$ (0, 1, 0, 1, 0, 1, 0), $\vect{v} =$ (0, 1, 0, 0, 0, 1, 0), $\vect{v}/\vect{x} =$ (-3, -3), $(\vect{v}/\vect{x})_1 =$ -3, and $\vect{v}/\textbf{ι}^1 =$ (2, 6).
<a name="minimum_selector"></a> Minimization is defined analogously and is denoted by $\vect{u}⌊\vect{x}$.

\par The extension of maximization and minimization to arbitrary vectors is the same as for the ordering operation, i.e., all nonnumeric quantities are treated as equal and as exceeding all numeric quantities.
<a name="max_row"></a>
<a name="max_col"></a> The extensions to matrices are denoted and defined as follows:

\begin{tabularx}
 & \mat{V} ← \mat{U} ⌈ \mat{X} & \leftrightarrow & \mat{V}^{\textit{i}} & = & \mat{U}^{\textit{i}} ⌈ \mat{X}^{\textit{i}}, & \\
 & \mat{V} ← \mat{U} ⌈⌈ \mat{X} & \leftrightarrow & \mat{V}_{\textit{j}} & = & \mat{U}_{\textit{j}} ⌈ \mat{X}_{\textit{j}}, & \\
 & \mat{V} ← \mat{U} <img src="APLimg/jotmax.bmp"> \vect{x} & \leftrightarrow & \mat{V}^{\textit{i}} & = & \mat{U}^{\textit{i}} ⌈ \vect{x}, & \\
 & \mat{V} ← \vect{u} <img src="APLimg/jotmax.bmp"> \mat{X} & \leftrightarrow & \mat{V}_{\textit{j}} & = & \vect{u} ⌈ \mat{X}_{\textit{j}}. & \\
\end{tabularx}

\par As in the case of the ordering operation, maximization in a vector $\vect{a}$ with respect to order in a set $\vect{b}$ is achieved by maximizing over the $\vect{b}$-index of $\vect{a}$. Thus if

\begin{tabularx}
<td rowspan=2> \mat{H} & <td rowspan=2> = & <td rowspan=2><img src="APLimg/matrixl2.bmp"> & ^{}d c h d h s h d c h c h d_{} & <td rowspan=2><img src="APLimg/matrixr2.bmp"> & \\
 ^{}a 6 k q 4 3 5 k 8 2 j 9 2_{} & \\
\end{tabularx}

\par represents a hand of thirteen playing cards, and if

\begin{tabularx}
<td rowspan=2 align=right> \mat{B} & <td rowspan=2> = & <td rowspan=2><img src="APLimg/matrixl2.bmp"> & ^{}c d h s ∘ ∘ ∘ ∘ ∘ ∘ ∘ ∘ ∘_{} & <td rowspan=2><img src="APLimg/matrixr2.bmp"> & <td rowspan=2>, & \\
 ^{}2 3 4 5 6 7 8 9 10 j q k a_{} & \\
\end{tabularx}

then

\begin{tabularx}
<td rowspan=2 align=right> \mat{B} ι_0 \mat{H} & <td rowspan=2> = & <td rowspan=2><img src="APLimg/matrixl2.bmp"> & ^{} 1 0 2 1 2 3 2 1 0 2 0 2 1_{} & <td rowspan=2><img src="APLimg/matrixr2.bmp"> & <td rowspan=2>, & \\
 ^{}12 4 11 10 2 1 3 11 6 0 9 7 0_{} & \\
\end{tabularx}

\par (4, 13) ${\circ \atop ⊥} (\mat{B} ι_0 \mat{H}) =$ (25, 4, 37, 23, 28, 40, 29, 24, 6, 26, 9, 33, 13),

and
\par $(\textbf{ϵ}⌈((4$, 13) ${\circ \atop ⊥} (\mat{B} ι_0 \mat{H})))/\mat{H} =$ (s, 3)

\par is the highest ranking card in the hand.

<a name="1.19"></a>
\par \textbf{1.19 Inverse functions}

\par To every biunique^{<a href="#note1f">[f]</a>} function $f$ there corresponds an \textit{inverse} function $g$ such that $g(f(x)) = x$ for each argument $x$ in the domain of the function $f$. It is common practice either to introduce a distinct symbolism for the inverse function, as for the inverse functions of logarithm (log_b $x)$ and exponentiation $(b^3)$, or to use a superscript -1, as in $sin^{-1}x$ or $f^{ -1}(x)$.

\par The first alternative doubles the number of distinct operator symbols required and obscures the relation between pairs of inverse functions; the second raises other difficulties. The solution adopted here is that of \textit{implicit} specification; i.e. a statement is permitted to specify not only a variable but also any function of that variable. Functions may therefore appear on both sides of the specification arrow in a statement. For example,

\par $(2\textbf{ϵ}) ⊥ x ← z$

\par specifies the variable $x$ as the vector whose base two value is the number $z$.

\par Certain ambiguities remain in the foregoing statement. First, the dimension of $x$ is not specified. For example, if $z =$ 12, $x =$ (1, 1, 0, 0) is an admissible solution, but so are (0, 1, 1, 0, 0) and (0, 0, 0, 1, 1, 0, 0). This could be clarified by compatibility with a specified dimension of $\textbf{ϵ}$. Thus the statement

\par $(2\textbf{ϵ}(5)) ⊥ x ← z$

\par specifies $x$ unambiguously as (0, 1, 1, 0, 0). More generally, however, any previously specified auxiliary variables will be listed to the right of the main statement, with a semicolon serving as a separation symbol. The current example could therefore be written as

\par $ν(x) ← 5\\$
 (2\textbf{ϵ}) ⊥ \textit{x} ← \textit{z}; \textit{ν}(\textit{x})

\par The second ambiguity concerns the permissible range of the individual components of $x$. For example, the base two value of $x =$ (5, 2) is also twelve. For certain functions it is therefore necessary to adopt some obvious conventions concerning the range of the result. The assumption implicit in the preceding paragraph is that each component of $x$ is limited to the range of the residues modulo the corresponding radix. This convention will be adopted. Hence the pair of statements

\par $y ←$ (7, 24, 60, $60)\\$
 \textit{y} ⊥ \textit{x} ← 7278; \textit{y}

\par determines $x$ unambiguously as the vector (0, 2, 1, 18).

\par it is also convenient, though not essential, to use selection operations on the left of a statement. Thus the statement

\par $\vect{u}/\vect{b} ← \vect{a}$

\par is understood to respecify only the selected components of $\vect{b}$ and to leave all others unchanged. It is therefore equivalent to the statement

\par $\vect{b} ← \backslash\overbar{\vect{u}}/\vect{b}, \vect{u}, \vect{a}\backslash$.

\par Similarly,

\par $\vect{u}/\vect{b} ← \vect{u}/\vect{a}$

\par is equivalent to

\par $\vect{b} ← /\vect{b}, \vect{u}, \vect{a}/$.

<a name="1.20"></a>
\par \textbf{1.20 Levels of structure}

\par Vectors and matrices are arrays which exhibit one level and two levels of structure, respectively. Although in certain fields, such as tensor analysis, it is convenient to define more general arrays whose \textit{rank} specifies the number of levels of structure (i.e., zero for a scalar, one for a vector of scalars, two for a vector of vectors (matrix), three for a vector of matrices, etc.), the notation will here be limited to the two levels provided by the matrix.^{<a href="#note1g">[g]</a>} The present section will, however, indicate methods for removing this limitation.

\par The only essential particularization to two levels occurs in the provision of single and double symbols (e.g. ``/'' and ``/\!/'', $``⊥''$ and ``<img src="APLimg/decode2.bmp">'') for row and column operations, respectively, and in the use of superscripts and subscripts for denoting rows and columns, respectively. In applications requiring multiple levels, the former can be generalized by adjoining to the single symbol an index which specifies the coordinate (e.g. ``/_1'' and ``/_2'', for row and for column compression, and, in general, $``/_j''.)$ The latter can be generalized by using a vector index subscript possessing one component index for each coordinate.

\par The generalized notation can be made compatible with the present notation for vectors and matrices by adopting the name \textit{tensor} and a symbol class (such as capital letters) for the general array of arbitrary rank.

<a name="1.21"></a>
\par \textbf{1.21 Subroutines}

\par Detail can be subordinated in a more general manner by the use of subroutines. The name of one program appearing as a single statement in a second program implies execution of the named program at that point; the named program is called a \textit{subroutine} of the second program. If, for example, ``Cos'' is the name of a program which specifies $z$ as the cosine of the angle between the vectors $\vect{x}$ and $\vect{y}$, then Program 1.13a uses the program ``Cos'' as a subroutine to determine $r$ as the cosine of the angle between the vectors $\vect{p}$ and $\vect{q}$.

<table align=center>
<td valign=top><img src="APLimg/prog1x13a.bmp"> & & <td valign=top><img src="APLimg/prog1x13b.bmp"> & & <td valign=top><img src="APLimg/prog1x13c.bmp"> & \\
<td align=center>(a) & & <td align=center>(b) & & <td align=center>(c) & \\
\end{tabularx}

\par \textbf{Program 1.13} Modes of subroutine reference

\par It is sometimes convenient to include the names of the arguments or results or both in the name of the subroutine as dummy variables. Thus if $``Cos(\vect{x}, \vect{y})''$ is the name of a subroutine which determines $z$ as the cosine of the angle between $\vect{x}$ and $\vect{y}$, then Program 1.13b uses $Cos(x, y)$ as a subroutine to determine $r$ as the cosine of the angle between $\vect{p}$ and $\vect{q}$. Similarly, the program $``z ← Cos(x, y)''$ can be used as in Program 1.13c to produce the same result.

<a name="1.22"></a>
\par \textbf{1.22 Files}

\par Many devices used for the storage of information impose certain restrictions upon its insertion or withdrawal. The items recorded on a magnetic tape, for example, may be read from the tape much more quickly in the order in which they appear physically on the tape than in some other prescribed order.

\par Certain storage devices are also self-indexing in the sense that the item selected in the next read from the device will be determined by the current state or position of the device. The next item read from a magnetic tape, for example, is determined by the position in which the tape was left by the last preceding read operation.

\par To allow the convenient description of algorithms constrained by the characteristics of storage devices, the following notation will be adopted.
<a name="file_defn"></a> A \textit{file} is a representation of a vector $\vect{x}$ arranged as follows:

\par $\vect{p}_1, \vect{x}_1, \vect{p}_2, \vect{x}_2$, ..., $\vect{x}_{ν(\vect{x})}, \vect{p}_{ν(\vect{x}) +$ 1}, $∘, \vect{p}_{ν(\vect{x}) +$ 2}, $∘$, ..., $\vect{p}_{ν(\vect{p})}$.

\par The null elements denote ``unused'' portion of the file not employed in representing $\vect{x}$. Each \textit{partition} $\vect{p}_j$ determines a \textit{position} (position $j)$ in the file.
<a name="forward_read"></a> If a file $\tree{Φ}$ is in position $j$, then a \textit{forward read}, denoted by

\par $x, p ← _0\tree{Φ}$,

\par specifies $x$ by the component $\vect{x}_j$, the auxiliary variable $p$ by the succeeding partition $\vect{p}_{j+1}$, and stops the file in the position $j +$ 1.

<a name="file_position"></a>
\par The position of a file $\tree{Φ}$ will be denoted by $π(\tree{Φ})$. Thus the statement $j ← π(\tree{Φ})$ specifies $j$ as the position of $\tree{Φ}$, whereas $π(\tree{Φ}) ← j$ \textit{positions} the file to $j$. In particular, $π(\tree{Φ}) ←$ 1 denotes the \textit{rewinding} of the file and $π(\tree{Φ}) ← ν$ denotes \textit{winding}, i.e. positioning to the extreme end of the file. Any file for which the general positioning operation $π(\tree{Φ}) ← j$ is to be avoided as impossible or inefficient is called a \textit{serial} or \textit{serial-access} file.

\par Each terminal partition (that is, $\vect{p}_1$ and $\vect{p}_{ν(\vect{p})})$ assumes a single fixed value denoted by $λ$. Each nonterminal partition $\vect{p}_j$ may assume one of several values denoted by $\mathbf{λ}_1, \mathbf{λ}_2$, ..., $\mathbf{λ}_{ν(\mathbf{λ})}$, the partitions with larger indices normally demarking larger subgroups of components within the file. Thus if $\vect{x}$ were the row list of a matrix, the last component might be followed by the partition $\mathbf{λ}_3$, the last component of each of the preceding rows by $\mathbf{λ}_2$, and the remaining components by $\mathbf{λ}_1$. The auxiliary variable $p$ specified by the partition symbol during the read of a file is normally used to control a subsequent branch.

<a name="forward_record"></a>
\par A file may be produced by a sequence of \textit{forward record} statements:

\par 
_0\tree{Φ} ← \vect{x}_{\textit{i}}, \textit{p} for \textit{i} ϵ \textbf{ι}^1(\textit{ν}(\vect{x})),

\par where $p$ is the partition symbol recorded after the component $\vect{x}_i$. As in reading, each forward record operation increments the position of the file by one. A file which is only recorded during a process is called an \textit{output file} of the process; a file which is only read is called an \textit{input file}.

\par Different files occurring in a process will be distinguished by righthand subscripts and superscripts, the latter being usually employed to denote major classes of files, such as input and output.

\begin{tabularx} & 
\par \textbf{Example 1.3}. A set of $m$ input files $\tree{Φ}_i^1, i ϵ \textbf{ι}^1(m)$, each terminated by a partition $λ_2$, is to be copied to a single output file $\tree{Φ}_1^2$ as follows. Successive items (components) are chosen in turn from files $\tree{Φ}_1^1, \tree{Φ}_2^1$, ..., $\tree{Φ}_m^1, \tree{Φ}_1^1, \tree{Φ}_2^1$, ..., always omitting from the sequence any exhausted file. A partition $λ_2$ is to be recorded with the last item recorded on $\tree{Φ}_1^2$, and all files are to be rewound. The process is described by Program 1.14.
 & & \\\end{tabularx}

<table align=center>
<img src="APLimg/prog1x14.bmp"> & & <td valign=top><table border=1 cellspacing=0 cellpadding=4> \begin{tabularx} <td valign=top>\tree{Φ}_{\textit{i}}^1 & & Input files for \textit{i} ϵ \textbf{ι}^1(\textit{m}). & \\ <td valign=top>\tree{Φ}_1^2 & & Output file. & \\ <td valign=top>\vect{u} & & File \tree{Φ}_{\textit{i}}^1 is exhausted\\
 if and only if \vect{u}_{\textit{i}} = 1. & <td valign=top>\textit{b} & & Item to be recorded. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend & \\
<td colspan=3 align=center>\textbf{Program 1.14} Program for Example 1.3 & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.14}. Step 8 cycles $k$ through the values 1 to $m$, and step 9 allows the read on steps 10 to occur only if $\vect{u}_k =$ 0. The logical vector $\vect{u}$ is of dimension $m$ and designates the set of exhausted files. Its $k$th component is set to unity by step 11 when file $k$ is exhausted, as indicated by the occurrence of the partition $\mathbf{λ}_2$. Each read is normally followed by step 13, which records on the output file the item read. However, when the last file becomes exhausted, step 14 is executed instead to record the last item, together with the final partition $\mathbf{λ}_2$.

\par Steps 1-6 initialize the parameters $\vect{u}$ and $k$ and rewind all files. After the last item is recorded by step 14, the file rewinds are repeated before the final termination on step 7.
 & & \\\end{tabularx}

\par It is sometimes convenient to suppress explicit reference to the partition symbol read from a file by using a statement of the $form\\$

\begin{tabularx}
 & <img src="APLimg/filebranch.bmp"> & <td valign=middle>, & \\
\end{tabularx}

where the indicated branches depend on the value of the partition \vect{p}_{\textit{j}+1} which terminates the read. Thus the left or the right branch is taken according to whether \vect{p}_{\textit{j}+1} = \vect{λ}_1 or \vect{p}_{\textit{j}+1} = \vect{λ}_2. Certain files (such as the IBM 7090 tape files) permit only such ``immediate'' branching and do not permit the partition symbol to be stored for use in later operations, as was done in Program 1.14.

\par In recording, the lowest level partition $\vect{λ}_1$ may be elided. Thus statement 13 of Program 1.14 may be written as

\par $\tree{Φ}_1^2 ← b$.

<a name="backward_read"></a>
<a name="backward_record"></a>
\par A file may be read or recorded backward as well as forward. A backward read is denoted by

\par $x, p ← _1\tree{Φ}$,

\par and if $\tree{Φ}$ is initially in position $j +$ 1, then $x = \vect{x}_j, p = \vect{p}_j$, and the final position becomes $j$. Backward recording is defined analogously. The zero prescript may be omitted from the symbol $_0\tree{Φ}$ for both forward reading and recording.

<a name="file_array"></a>
\par The conventions used for matrices can be applied in an obvious way to an array of files $\tree{Φ}_j^i$. For example, the statement

\par $π(\tree{Φ}^i) ← \textbf{ϵ}$

\par denotes the rewinding of the \textit{row of files} $\tree{Φ}_j^i, j ϵ \textbf{ι}^1(ν(\tree{Φ}))$; the statement

\par $π(\tree{Φ}_j) ← \textbf{ϵ}$

\par denotes the rewinding of the \textit{column of files} $\tree{Φ}_j^i, i ϵ \textbf{ι}^1(μ(\tree{Φ}))$; and the statement

\par $\vect{u}/\tree{Φ}^i ← \vect{u}/\vect{x}, \vect{u}/\vect{p}$

\par denotes the recording of the vector components $\vect{x}_j$ on the file $\tree{Φ}_j^i$ together with partition $\vect{p}_j$ for all $j$ such that $\vect{u}_j =$ 1.

\par As for vectors and matrices, $j$-origin indexing may be used and will apply to the indexing of the file positions and the partition vector $\mathbf{λ}$ as well as to the array indices. However, the prescripts (denoting direction of read and record) are independent of index origin. 0-origin indexing is used in the following example.

\begin{tabularx} & 
\par \textbf{Example 1.4}. Files $\tree{Φ}_0^0$ and $\tree{Φ}_1^0$ contain the vectors $\vect{x}$ and $\vect{y}$, respectively, each of dimension $n$. In the first phase, the components are to be merged in the order $\vect{x}_0, \vect{y}_0, \vect{x}_1, \vect{y}_1$, ... $\vect{x}_{ν-1}, \vect{y}_{ν-1}$, and the first $n$ components of the resulting vector are to be recorded on file $\tree{Φ}_0^1$, and the last $n$ on file $\tree{Φ}_1^1$. In other words, the vectors $\vect{x}^1 = \mathbf{α}^n/\vect{z}$ and $\vect{y}^1 = \textbf{ω}^n/\vect{z}$ are to be recorded on $\tree{Φ}_0^1$ and $\tree{Φ}_1^1$, respectively, where $\vect{z} = \backslash\vect{x}, \vect{u}, \vect{y}\backslash$, and $\vect{u} =$ (0, 1, 0, 1, ..., 0, 1). In the next phase, the roles of input and output files are reversed, and the same process is performed on $\vect{x}^1$ and $\vect{y}^1$, that is, $\vect{x}^2 = \mathbf{α}^n/(\backslash\vect{x}^1, \vect{u}, \vect{y}^1\backslash)$ and $\vect{y}^2 = \textbf{ω}^n/(\backslash\vect{x}^1, \vect{u}, \vect{y}^1\backslash)$ are recorded on $\tree{Φ}_0^0$ and $\tree{Φ}_1^0$, respectively. The process is to be continued through $m$ phases.
 & & \\\end{tabularx}

<table align=center>
<td valign=top><img src="APLimg/prog1x15.bmp"> & <td nowrap> & <td valign=top><table border=1 cellspacing=0 cellpadding=4> <td align=center>0-origin indexing & \\ \begin{tabularx} <td valign=top>\tree{Φ}^{} & & File array of dimension 2 \times 2; original input \tree{Φ}^0; original output \tree{Φ}^1. & \\ <td valign=top>\vect{u} & & Control vector. & \\ <td valign=top>\vect{u}_0 & & Column index of input file. & \\ <td valign=top>\vect{u}_1 & & Column index of output file. & \\ <td valign=top>\vect{u}_2 & & Row index of current input file, and direction of read and record. & \\ <td valign=top>\textit{n} & & Number of items per file. & \\ <td valign=top>\textit{m} & & Required number of merges. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend & \\
<td colspan=3 align=center>\textbf{Program 1.15} Program for Example 1.4 & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.15}. The program for Example 1.4 begins with the rewind of the entire 2 $\times$ 2 array of files. To obviate further rewinding, the second (and each subsequent even-numbered) execution is performed by reading and recording all files in the backward direction. Step 6 performs the essential read and record operation under control of the logical vector $\vect{u}$, whose components $\vect{u}_0, \vect{u}_1, \vect{u}_2$ determine, respectively, the subscript of the file to be read, the subscript of the file to be recorded, and the direction of read and record. The file superscripts (determining which classes serve as input and output in the current repetition) are also determined by $\vect{u}_2$, the input being $\vect{u}_2$ and the output $\overbar{\vect{u}}}_2$. The loop 6-8 copies $n$ items, alternating the input files through the negation of $\vect{u}_0$ on step 7. When the loop terminates, $\vect{u}_1$ is negated to interchange the outputs, and the loop is repeated unless $\vect{u}_1 = \vect{u}_2$. Equality occurs and causes a branch to step 3 if and only if all $2n$ items of the current phase have already been copied.

\par Step 3 decrements $m$ and is followed by the negation of $\vect{u}$ on step 4. The component $\vect{u}_2$ must, of course, be negated to reverse direction, but the need to negate $\vect{u}_0$ and $\vect{u}_1$ is not so evident. It arises because the copying order was prescribed for the forward direction, beginning always with the operation

\par <img src="APLimg/fcopyinit0.bmp">.

\par An equivalent backward copy must therefore begin with the operation

\par <img src="APLimg/fcopyinit1.bmp">.
 & & \\\end{tabularx}

\par Not all computer files have the very general capabilities indicated by the present notation. Some files, for example, can be read and recorded in the forward direction only and, except for rewind, cannot be positioned directly. Positioning to an arbitrary position $k$ must then be performed by a rewind and a succession of $(k -$ 1) reads. In some files, recording can be performed in the forward direction only, and the positions are defined only by the recorded data. Consequently, recording in position $k$ makes unreliable the data in all subsequent positions, and recording must always proceed through all successive positions until terminated.

<a name="1.23"></a>
\par \textbf{1.23 Ordered trees}

<a name="1.23.1"></a>
\par \textbf{Directed graphs}

\par For many processes it is convenient to use a structured operand with the treelike structure suggested by Fig. 1.16. It is helpful to begin with a more general structure (such as Fig. 1.17) in which a unidirectional association may be specified between any pair of its components.

\begin{tabularx}
<td align=center><img src="APLimg/fig1x16.bmp"> & \\

<td align=center>\textbf{Figure 1.16} A general triply root tree with \textit{λ}(\mat{T}) = 16, \mathbf{ν}(\mat{T}) = (3, 3, 4, 3, 2),\\
 \textit{ν}(\mat{T}) = 5, \mathbf{μ}(\mat{T}) = (3, 7, 8, 5, 3), and \textit{μ}(\mat{T}) = 26 & \\
\end{tabularx}

\par A \textit{directed graph} comprises a vector $\vect{n}$ and an arbitrary set of unilateral associations specified between pairs of its components. The vector $\vect{n}$ is called a \textit{node vector} and its components are also called \textit{nodes}. The associations are conveniently specified by a (logical) \textit{connection matrix} $\mat{U}$ of dimensions $ν(\vect{n}) \times μ(\vect{n})$ with the following convention: there is an association, called a \textit{branch}, from node $i$ to node $j$ if and only if $\mat{U}_j^i =$ 1.

\par A directed graph admits of a simple graphical interpretation, as illustrated by Fig. 1.17. The nodes might, for example, represent places, and the lines, connecting streets. A two-way street is then represented by a pair of oppositely directed lines, as shown between nodes 3 and 4.

<table align=center>
<img src="APLimg/fig1x17.bmp"> & 

\begin{tabularx}
<td rowspan=7>\mat{U} = & 
<td rowspan=7><img src="APLimg/matrixl7.bmp"> & 
^{}0 0 0 0 1 0 1_{} & 
<td rowspan=7><img src="APLimg/matrixr7.bmp"> & 

^{}0 0 0 0 0 1 0_{} & \\
^{}0 0 0 1 1 1 0_{} & \\
^{}0 0 1 0 0 0 0_{} & \\
^{}0 0 0 0 0 1 1_{} & \\
^{}1 0 0 0 0 0 0_{} & \\
^{}0 1 0 0 0 0 1_{} & \\
\end{tabularx} & \\
<td align=center colspan=3>\\
\textbf{Figure 1.17} A graphical representation of the graph (\vect{n}, \mat{U}).

\end{tabularx}

\par If $\vect{k}$ is any mapping vector such that

\begin{tabularx}
 <img src="APLimg/ukk.bmp"> & 
 = 1 for \textit{i} = 2, 3, ..., \textit{ν}(\vect{k}), & \\
\end{tabularx}

\par then the vector $\vect{p} = \vect{k} \int \vect{n}$ is called \textit{a path vector} of the graph $(\vect{n}, \mat{U})$. The dimension of a path vector is also called its \textit{length}. Nodes $\vect{k}_1$ and $\vect{k}_ν$ are called the \textit{initial} and \textit{final} nodes, respectively; both are also called \textit{terminal} nodes. If $\vect{j}$ is any infix of $\vect{k}$, then $\vect{q} = \vect{j} \int \vect{n}$ is also a path. It is called a subpath of $\vect{p}$ and is said to be \textit{contained} in $\vect{p}$. If $ν(\vect{q}) < ν(\vect{p})$, then $\vect{q}$ is a \textit{proper} subpath of $\vect{p}$. If $\vect{k}_1 = \vect{k}_ν$ and $\vect{p} = \vect{k} \int \vect{n}$ is a path of a length exceeding one, $\vect{p}$ is called a \textit{circuit}. For example, if $\vect{k} =$ (6, 1, 7, 7, 2, 6, 1, 5), then $\vect{p} = (\vect{n}_6, \vect{n}_1, \vect{n}_7, \vect{n}_7, \vect{n}_2, \vect{n}_6, \vect{n}_1, \vect{n}_5)$ is a path vector of the graph of Fig. 1.17, which contains the proper subpaths $(\vect{n}_7, \vect{n}_2, \vect{n}_6), (\vect{n}_1, \vect{n}_7, \vect{n}_7, \vect{n}_2, \vect{n}_6, \vect{n}_1)$, and $(\vect{n}_7, \vect{n}_7)$, the last two of which are circuits. Node $j$ is said to be reachable from node $i$ if there exists a path from node $i$ to node $j$.

<a name="1.23.2"></a>
\par \textbf{Ordered trees}

\par A graph (such as Fig. 1.16) which contains no circuits and which has at most one branch entering each node is called a \textit{tree}. Since each node is entered by at most one branch, a path existing between any two nodes in a tree is unique, and the length of path is also unique. Moreover, if any two paths have the same final node, one is a subpath of the other.

\par Since a tree contains no circuits, the length of path in a finite tree is bounded. There therefore exist \textit{maximal paths} which are proper subpaths of no longer paths. The initial and final nodes of a maximal path are called a \textit{root} and \textit{leaf} of the tree, respectively. A root is said to lie on the \textit{first level} of the tree, and, in general, a node which lies at the end of a path of length $j$ from a root, lies in the $j$th \textit{level} of the tree.

\par A tree which contains $n$ roots is said to be \textit{n-tuply rooted}. The sets of nodes reachable from each of the several roots are disjoint, for if any node is reachable by paths from each of two disjoint roots, one is a proper subpath of the other and is therefore not maximal. Similarly, any node of a tree defines a \textit{subtree} of which it is the root, consisting of itself and all nodes reachable from it, with the same associations as the parent tree.

<a name="nu_tree"></a>
<a name="mu_tree"></a>
<a name="lambda_tree"></a>
\par If for each level $j$, a simple ordering is assigned to each of the disjoint sets of nodes reachable from each node of the preceding level, and if the roots are also simply ordered, the tree is said to be \textit{ordered}. Attention will henceforth be restricted to ordered trees, which will be denoted by uppercase boldface roman characters. The \textit{height} of a tree \mat{T} is defined as the length of the longest path in \mat{T} and is denoted by $ν(\mat{T})$. The number of nodes on level $j$ is called the \textit{moment of level j} and is denoted by $\mathbf{μ}_j(\mat{T})$. The vector $\mathbf{μ}(\mat{T})$ is called the \textit{moment vector}. The total number of nodes in \mat{T} is called the moment of \mat{T} and is denoted by $μ(\mat{T})$. Clearly, $ν(\mathbf{μ}(\mat{T})) = ν(\mat{T})$, and $+/\mathbf{μ}(\mat{T}) = μ(\mat{T}) = ν(\vect{n})$. The number of roots is equal to $\mathbf{μ}_1(\mat{T})$, and the number of leaves will be denoted by $λ(\mat{T})$.

<a name="delta_tree"></a>
<a name="dispersion_vector"></a>
\par The number of branches leaving a node is called its \textit{branching ratio} or \textit{degree}, and the maximum degree occurring in a tree \mat{T} is denoted by $δ(\mat{T})$. The \textit{dispersion vector} of a tree \mat{T} is denoted by $\mathbf{ν}(\mat{T})$ and is defined as follows: $\mathbf{ν}_1(\mat{T}) = \mathbf{μ}_1(\mat{T})$, and for $j =$ 2, 3, ..., $ν(\mat{T}), \mathbf{ν}_j(\mat{T})$ is equal to the maximum over the branching ratios of the nodes on level $j -$ 1. For the tree of Fig. 1.16, $\mathbf{ν}(\mat{T}) =$ (3, 3, 4, 3, 2). The number of roots possessed by a tree \mat{T} (that is, $\mathbf{ν}_1(\mat{T}))$ is called its \textit{dispersion}. A tree possessing unity dispersion is called \textit{rooted} or \textit{singular}.

\par Each node $\vect{n}_i$ of a graph (and hence of a tree) may be identified by its index $i$. Since a tree admits of more convenient index vectors, the underlying index $i$ will henceforth be referred to as the \textit{graph index}.

<a name="node"></a>
<a name="delta2"></a>
\par In an ordered tree, any path of length $k$ from a root can be uniquely specified by an \textit{index vector} $\vect{i}$ of dimension $k$, where $\vect{i}_1$ specifies the particular root, and the remaining components specify the (unique) path as follows: the path node on level $j$ is the $\vect{i}_j$th element of the set of nodes on level $j$ reachable from the path node on level $j -$ 1. The node at the end of the path can therefore be designated uniquely by the index vector $\vect{i}$. The degree of node $\vect{i}$ will be denoted by $δ(\vect{i}$, \mat{T}). The index vectors are shown to the left of each node in Fig. 1.16.

<a name="path"></a>
\par The path from a root whose terminal node is $\vect{i}$ will be denoted by $\mat{T}^{\vect{i}}$. In Fig. 1.16, for example, $\mat{T}^{\vect{i}} = (\vect{n}_2, \vect{n}_8, \vect{n}_{13}, \vect{n}_{24})$ if $\vect{i} =$ (2, 2, 2, 3). A vector $\vect{i}$ is said to be an index of \mat{T} if it is the index of some node in \mat{T}.

<a name="subtree"></a>
\par The subtree of \mat{T} rooted in node $\vect{i}$ will be denoted by $\mat{T}_{\vect{i}}$. Thus in Fig. 1.16, \mat{P} $=$ \mat{T}_{(2,2,2)} is a rooted subtree with $\mathbf{ν}(\mat{P}) =$ (1, 3, 2), and $\mathbf{μ}(\mat{P}) =$ (1, 3, 3). A path in $\mat{T}_{\vect{i}}$ is denoted by $(\mat{T}_{\vect{i}})^{\vect{$ j}}. For example, if \mat{G} is an ascending genealogical tree^{<a href="#note1h">[h]</a>} with the sword and distaff sides denoted by the indices 1 and 2, respectively, then any individual $x$ and the nearest $(n -$ 1) paternal male ancestors are represented by the path vector $(\mat{G}_{\vect{i}})^{\textbf{ϵ}(n)}$, where $\vect{i}$ is the index of $x$ in \mat{G}.

\begin{tabularx} & 
\par \textbf{Example 1.5}. Determine the index $\vect{i}$ such that the path \mat{T}^{ $\vect{i}}$ is equal to a given argument $\vect{x}$ and is the ``first'' such path in \mat{T}; that is, the function

\par $(\mathbf{α}^{ν(\vect{x})}/\mathbf{ν}(\mat{T})) ⊥ \vect{i}$

\par is a minimum.
 & & \\\end{tabularx}

<table align=center>
<td valign=top><img src="APLimg/prog1x18.bmp"> & <td nowrap> & <td valign=top><table border=1 cellspacing=0 cellpadding=4> <td align=center>1-origin indexing & \\ \begin{tabularx} \mat{T} & <td nowrap> & Given tree. & \\ <td valign=top>\vect{x} & & Given path vector. & \\ <td valign=top>\vect{i} & & Path index vector to be determined. & \\ <td valign=top>\mathbf{μ}_1(\mat{T}) & & Number of roots of \mat{T}. & \\ <td valign=top>\textit{δ}(\vect{i},\mat{T}) & & Degree of node \vect{i}. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend} & \\
<td colspan=3 align=center>\textbf{Program 1.18} Determination of \vect{i} such that \mat{T}^{\vect{i}} = \vect{x} & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.18}. The index vector $\vect{i}$ specifies the path currently under test. Its last component is incremented repeatedly by step 7 until the loop 6-8 is terminated. If the path $\mat{T}^{\vect{i}}$ agrees with the corresponding prefix of the argument $\vect{x}$, termination occurs through the branch to step 9, which tests for completion before step 10 augments $\vect{i}$ by a final zero component. Step 5 then respecifies $d$ as the degree of the penultimate node of the set of $d$ paths next to be tested by the loop. Termination by a branch from step 6 to step 2 occurs if all $d$ possible paths are exhausted without finding agreement on step 8. In this event, retraction by one level occurs on step 2, and $d$ is again respecified. If $ν(\vect{i}) =$ 1, the paths to be searched comprise the roots of the tree and $d$ must therefore be specified as the number of roots. This is achieved by executing step 3 and skipping step 5. Retraction to a vector $\vect{i}$ of dimension zero occurs only if all roots have been exhausted, and final termination from step 4 indicates that the tree possesses no path equal to the argument $\vect{x}$.
 & & \\\end{tabularx}

\par If $\vect{d}$ is a vector of dimension $ν(\vect{n})$ such that $\vect{d}_i$ is the degree of node $\vect{n}_i$ of a tree \mat{T}, then $\vect{d}$ is called the \textit{degree vector associated with} $\vect{n}$. In Fig. 1.16, for example,

\par $\vect{d} =$ (3, 2, 4, 0, 0, 0, 2, ..., 1, 0, 0).

\par Moreover, if $\vect{n}$ is itself the alphabet (that is, $\vect{n} =$ (a, b, c, ... , z)), then the vector $\vect{n}'$ of Table 1.19a is a permutation of $\vect{n}$, and $\vect{d}'$ is the associated degree vector. Table 1.19b shows another such pair, $\vect{n}''$ and $\vect{d}''$.

\par The degree vector provides certain useful information most directly. For example, since each leaf is of degree zero, $λ(\mat{T}) = +/(\vect{d} =$ 0). Moreover, the number of roots is equal to the number of nodes less the total of the degrees, that is, $\mathbf{μ}_1(\mat{T}) = ν(\vect{d}) - +/\vect{d}$, and the maximal degree occurring in \mat{T} is given by $δ(\mat{T}) = ((\textbf{ϵ}⌈\vect{d})/\vect{d})_1$. Finally, the degree vector and the node vector together can, in certain permutations (those of Table 1.19), provide a complete and compact description of the tree.

<table align=center>
<table border=1 cellspacing=0 cellpadding=0>
 <td align=center>\vect{d} ' & <td align=center>\vect{n}' & <td align=center>\mat{I} ' & \\
\begin{tabularx} 3 & \\ 4 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 1 & \\ 2 & \\ 0 & \\ 0 & \\ 2 & \\ 0 & \\ 3 & \\ 0 & \\ 3 & \\ 0 & \\ 2 & \\ 0 & \\ 0 & \\ 1 & \\ 0 & \\ 0 & \\ 2 & \\ 0 & \\ 0 & \\ \end{tabularx} & \begin{tabularx} a & \\ c & \\ f & \\ d & \\ r & \\ e & \\ z & \\ n & \\ i & \\ p & \\ q & \\ b & \\ k & \\ h & \\ o & \\ m & \\ u & \\ s & \\ t & \\ w & \\ x & \\ y & \\ v & \\ g & \\ j & \\ l & \\ \end{tabularx} & \begin{tabularx}
 & 1 & ∘ & ∘ & ∘ & ∘ & & \\
 & 1 & 1 & ∘ & ∘ & ∘ & & \\
 & 1 & 1 & 1 & ∘ & ∘ & & \\
 & 1 & 1 & 2 & ∘ & ∘ & & \\
 & 1 & 1 & 3 & ∘ & ∘ & & \\
 & 1 & 1 & 4 & ∘ & ∘ & & \\
 & 1 & 2 & ∘ & ∘ & ∘ & & \\
 & 1 & 3 & ∘ & ∘ & ∘ & & \\
 & 1 & 3 & 1 & ∘ & ∘ & & \\
 & 1 & 3 & 1 & 1 & ∘ & & \\
 & 1 & 3 & 1 & 2 & ∘ & & \\
 & 2 & ∘ & ∘ & ∘ & ∘ & & \\
 & 2 & 1 & ∘ & ∘ & ∘ & & \\
 & 2 & 2 & ∘ & ∘ & ∘ & & \\
 & 2 & 2 & 1 & ∘ & ∘ & & \\
 & 2 & 2 & 2 & ∘ & ∘ & & \\
 & 2 & 2 & 2 & 1 & ∘ & & \\
 & 2 & 2 & 2 & 2 & ∘ & & \\
 & 2 & 2 & 2 & 2 & 1 & & \\
 & 2 & 2 & 2 & 2 & 2 & & \\
 & 2 & 2 & 2 & 3 & ∘ & & \\
 & 2 & 2 & 2 & 3 & 1 & & \\
 & 2 & 2 & 3 & ∘ & ∘ & & \\
 & 3 & ∘ & ∘ & ∘ & ∘ & & \\
 & 3 & 1 & ∘ & ∘ & ∘ & & \\
 & 3 & 2 & ∘ & ∘ & ∘ & & \\ \end{tabularx} & 

\end{tabularx}
\par Full left list matrix $[\mat{T}\\$
(\textit{a})

<td valign=top>\begin{tabularx}

<tr align=center>1 & \\
<tr align=center>2 & \\
<tr align=center>3 & \\
<tr align=center>4 & \\
<tr align=center>5 & \\
<tr align=center>6 & \\
<tr align=center>7 & \\
<tr align=center>8 & \\
<tr align=center>9 & \\
10 & \\
11 & \\
12 & \\
13 & \\
14 & \\
15 & \\
16 & \\
17 & \\
18 & \\
19 & \\
20 & \\
21 & \\
22 & \\
23 & \\
24 & \\
25 & \\
26 & \\
\end{tabularx} & 

<table border=1 cellspacing=0 cellpadding=0>
 <td align=center>\vect{d} '' & <td align=center>\vect{n}'' & <td align=center>\mat{I} '' & \\
\begin{tabularx} 3 & \\ 2 & \\ 2 & \\ 4 & \\ 0 & \\ 1 & \\ 0 & \\ 3 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 2 & \\ 0 & \\ 3 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 2 & \\ 1 & \\ 0 & \\ 0 & \\ 0 & \\ \end{tabularx} & \begin{tabularx} a & \\ b & \\ g & \\ c & \\ z & \\ n & \\ k & \\ h & \\ j & \\ l & \\ f & \\ d & \\ r & \\ e & \\ i & \\ o & \\ m & \\ v & \\ p & \\ q & \\ u & \\ s & \\ x & \\ t & \\ w & \\ y & \\ \end{tabularx} & \begin{tabularx}
 & ∘ & ∘ & ∘ & ∘ & 1 & & \\
 & ∘ & ∘ & ∘ & ∘ & 2 & & \\
 & ∘ & ∘ & ∘ & ∘ & 3 & & \\
 & ∘ & ∘ & ∘ & 1 & 1 & & \\
 & ∘ & ∘ & ∘ & 1 & 2 & & \\
 & ∘ & ∘ & ∘ & 1 & 3 & & \\
 & ∘ & ∘ & ∘ & 2 & 1 & & \\
 & ∘ & ∘ & ∘ & 2 & 2 & & \\
 & ∘ & ∘ & ∘ & 3 & 1 & & \\
 & ∘ & ∘ & ∘ & 3 & 2 & & \\
 & ∘ & ∘ & 1 & 1 & 1 & & \\
 & ∘ & ∘ & 1 & 1 & 2 & & \\
 & ∘ & ∘ & 1 & 1 & 3 & & \\
 & ∘ & ∘ & 1 & 1 & 4 & & \\
 & ∘ & ∘ & 1 & 3 & 1 & & \\
 & ∘ & ∘ & 2 & 2 & 1 & & \\
 & ∘ & ∘ & 2 & 2 & 2 & & \\
 & ∘ & ∘ & 2 & 2 & 3 & & \\
 & ∘ & 1 & 3 & 1 & 1 & & \\
 & ∘ & 1 & 3 & 1 & 2 & & \\
 & ∘ & 2 & 2 & 2 & 1 & & \\
 & ∘ & 2 & 2 & 2 & 2 & & \\
 & ∘ & 2 & 2 & 2 & 3 & & \\
 & 2 & 2 & 2 & 2 & 1 & & \\
 & 2 & 2 & 2 & 2 & 2 & & \\
 & 2 & 2 & 2 & 3 & 1 & & \\ \end{tabularx} & 

\end{tabularx}
\par Full right list matrix $]\mat{T}\\$
(\textit{b})

\end{tabularx}
\par \textbf{Table 1.19}. Full list matrices of the tree of Fig. 1.16

<a name="1.23.3"></a>
\par \textbf{Right and left list matrices}

\par If each one of the $μ(\mat{T})$ index vectors $\vect{i}$ of a tree \mat{T} is listed together with its associated node $(\mat{T}^{\vect{i}})_{ν(\vect{i})}$, the list determines the tree completely. Since the index vectors are, in general, of different dimensions, it is convenient to append null components^{<a href="#note1i">[i]</a>} to extend each to the common maximum dimension $ν(\mat{T})$. They may then be combined in an \textit{index matrix} of dimension $μ(\mat{T}) \times ν(\mat{T})$, which, together with the associated node vector, completely describes the tree. If, for example, the node vector $\vect{n}$ is the alphabet, the tree of Fig. 1.16 is described by the node vector $\vect{n}'$ and index matrix $\mat{I} '$ of Table 1.19a or, alternatively, by $\vect{n}''$ and $\mat{I}$ '' of Table 1.19b.

\par Because of the utility of the degree vector, it will be annexed to the array of node vector and index matrix, as shown in Table 1.19a to form a \textit{full list matrix} of the tree. The degree vector and node vector together will be called a \textit{list matrix}. As remarked, the list matrix can in certain permutations, alone describe the tree.

\par Formally, the full list matrix $\mat{M}$ of a tree \mat{T} is defined as follows: $\overbar{\vect{α}}^2/\mat{M}$ is an index matrix of the tree, $\mat{M}_1$ is the associated degree vector, and $\mat{M}_2$ is the associated node vector. Thus for each $k ϵ \textbf{ι}^1(μ(\mat{T})), \mat{M}_1^k = δ(\vect{i}$, \mat{T}), and $\mat{M}_2^k = (\mat{T}^{\vect{i}})_{ν(\vect{i})}$, where $\vect{i}$ is the nonnull portion of $\overbar{\vect{α}}^2/\mat{M}^k$, that is, $\vect{i} = ((\overbar{\vect{α}}^2/\mat{M}^k) \neq ∘\textbf{ϵ})/(\overbar{\vect{α}}^2/\mat{M}^k)$. The corresponding list matrix is $\mathbf{α}^2/\mat{M}$.

\par Since a full list matrix provides a complete description of a tree regardless of the order in which the nodes occur in the list, any column permutation $\mat{M}^{ \vect{p}}$ (that is, any reordering among the rows) is also a full list matrix. Two particular arrangements of the full list matrix are of prime interest because each possesses the following properties: (1) the nodes are grouped in useful ways, and (2) the list matrix (i.e., the degree vector and node vector) alone describes the tree without reference to the associated index matrix. They are called the full \textit{left} list matrix and full \textit{right} list matrix and are denoted by [\mat{T} and ]\mat{T}, respectively. Table 1.19 shows the full left and full right lists of the tree of Fig. 1.16.

\par The left list index matrix $\mat{I}$ is left justified,^{<a href="#note1j">[j]</a>} that is, the null elements are appended at the right of each index. The rows $\mat{I}^{ j}$ are arranged in increasing order on their values as decimal (or rather $(δ(\mat{T}) +$ 1)-ary) fractions with the radix point at the left and the nulls replaced by zeros. More precisely, the rows are arranged in increasing order on the function $(ν(\vect{a})\textbf{ϵ}) ⊥ (\vect{a} \textbf{ι}_0 \mat{I}^{ j})$, where $\vect{a} = (∘$, 1, 2, ..., $δ(\mat{T})).^{<a$ href="#note1k">[k]</a>}

\par The right list matrix is right justified and is ordered on the same function, namely $(ν(\vect{a})\textbf{ϵ}) ⊥ (\vect{a} \textbf{ι}_0 \mat{I}^{ j})$. The rows are therefore ordered on their values as integers, i.e., with the decimal point at the right. From the example of Table 1.19b it is clear that the right list groups the nodes by levels, i.e., level $j$ is represented by the infix $(i ↓ \mathbf{α}^k)/\!/(]\mat{T})$, where $k = \mathbf{μ}_j(\mat{T})$, and $i = +/\mathbf{α}^{j-1}/\mathbf{μ}(\mat{T})$. In Table 1.19b, for example, $\mathbf{μ}(\mat{T}) =$ (3, 7, 8, 5, 3), and if $j =$ 3, then $k =$ 8, $i =$ 10, and level $j$ is represented by rows $i +$ 1 $=$ 11 to $i + k =$ 18. The right list is therefore useful in executing processes (such as the $p$th degree selection sort) which require a scan of successive levels of the tree.

\par The left list groups the nodes by subtrees, i.e., any node $\vect{i}$ is followed immediately by the remaining nodes of its subtree $\mat{T}_{\vect{i}}$. Formally, if $\mat{I} = \overbar{\vect{α}}^2/[\mat{T}$, and if $\vect{i} = (\mat{I}^k \neq ∘\textbf{ϵ})/\mat{I}^k$, then the tree $\mat{T}_{\vect{i}}$ is represented by the infix $((k -$ 1) $↓ \mathbf{α}^{μ(\mat{T}_{\vect{i}})})/\!/[\mat{T}$. In Fig. 1.19a, for example, if $\vect{k} =$ 16, then $\vect{i} =$ (2, 2, 2), $μ(\mat{T}_{\vect{i}}) =$ 7, and $\mat{T}_{\vect{i}}$ is represented by rows 16 to 22 of [\mat{T}. The left list is therefore useful in processes (such as the construction of a Huffman code and the evaluation of a compound statement) which require a treatment of successive subtrees.

\par The row index of a node in a right (left) list matrix is a graph index of the node and will be called the \textit{right} (\textit{left}) \textit{list index}.

<a name="1.23.4"></a>
\par \textbf{Well formation}

\par A two-column matrix which forms the right list of some tree is said to be a \textit{well formed right list}. Since the ordering of the nodes in a right list of a given tree is unique, the right list of a given tree is unique. Conversely, any well formed right list specifies a unique tree according to the algorithm of Program 1.20.

\par Identical remarks apply to the left list, except that Program 1.20 is replaced by Program 1.21. Moreover, the necessary and sufficient conditions for the well formation of a left list are identical with those for a right list and are derived by virtually identical arguments. The case will be stated for the right list only.

\par If $\mat{R}$ is a well formed right list representing a tree \mat{T}, then the dispersion (i.e., the number of roots) $\mathbf{ν}_1(\mat{T}) = ν(\mat{R}_1) - (+/\mat{R}_1)$ must be strictly positive. Moreover, if $\mat{S} =$ 
\overbar{\vect{α}}^{\textit{j}}/\!/\mat{R} is any suffix of \mat{R}, then \mat{S} is a right list of the tree obtained by deleting from \mat{T} the first \textit{j} nodes of the original list. For, such level-by-level deletion always leaves a legitimate tree with the degrees of the remaining nodes unchanged. Consequently, the number of roots determined by every suffix of \mat{R}_1 must also be strictly positive. In other words, all components of the \textit{suffix dispersion vector} \vect{s} defined by

\par $\vect{s}_j = ν(\overbar{\vect{α}}^{j-1}/\mat{R}_1), j ϵ \textbf{ι}^1(ν(\mat{R}_1))$
\par must be strictly positive. The condition is also sufficient.

\par Sufficiency is easily established by induction on the column dimension of $\mat{R}$. The condition is clearly sufficient for $ν(\mat{R}_1) =$ 1. Assume it sufficient for dimension $ν(\mat{R}_1) -$ 1. If $\vect{s}$, the suffix dispersion vector of $\mat{R}$, is strictly positive, then $\overbar{\vect{α}}^1/\vect{s}$, the suffix dispersion vector of $\overbar{\vect{α}}^1/\!/\mat{R}$, is also positive, and by hypothesis 
\overbar{\vect{α}}^1/\!/\mat{R} represents a tree \mat{G} possessing \vect{s}_2 roots. Moreover,

\par 0 $< \vect{s}_1 = \vect{s}_2 +$ (1 $- \mat{R}_1^1)$

\par implies that $\vect{s}_2 \geq \mat{R}_1^1$, and the number of roots possessed by $\mat{G}$ therefore fulfills the number of branches required by the added node $\mat{R}_2^1$. A legitimate tree corresponding to $\mat{R}$ can therefore be formed by joining the last $\mat{R}_1^1$ roots of $\mat{G}$ to the node $\mat{R}_2^1$.

\par Tests for well formation can therefore be incorporated in any algorithm defined on a right or left list matrix $\mat{M}$ by computing the components of the suffix dispersion vector $\vect{s}$. The recursion $\vect{s}_{i-1} = \vect{s}_i +$ 1 $- \mat{M}_1^{i-1}$ is convenient in a backward scan of $\mat{M}$, and the equivalent recursion $\vect{s}_i = \vect{s}_{i-1} -$ 1 $+ \mat{M}_1^{i-1}$ serves for a forward scan. The starting condition for a forward scan is $\vect{s}_1 = ν(\mat{M}_1) - (+/\mat{M}_1)$, and for a backward scan is $\vect{s}_ν =$ 1 $- \mat{M}_1^μ$. Since the criteria of well formation are identical for right and left lists, a matrix may be characterized simply as well or ill formed.

\par The purpose served by the degree vector $\vect{d}$ in the description of a tree is sometimes served instead [e.g., <acronym title="Burks, A.W., D.W. Warren, and J.B. Wright (1954), $“An$ Analysis of a Logical Machine Using Parenthesis-free $Notation”$, Mathematical Tables and Other Aids to Computation, vol. VIII, pp. 53-57.">Burks et al. (1954)</acronym>] by the vector $\vect{g} = \textbf{ϵ} - \vect{d}$. It is somewhat more convenient in the analysis of well formation, since the expression for the suffix dispersion vector then simplifies to

\par $\vect{s}_{j+1} = (+/\overbar{\vect{α}}^j/\vect{g})$, or $\vect{s} = (\mathsf{\mat{I}} +$ <img src="APLimg/quadne.bmp">)<img $src="APLimg/minusslash.bmp">/\vect{g}$.

<a name="1.23.5"></a>
\par \textbf{The index matrix as a function of the degree vector}

\par The complete determination of the tree corresponding to a given list matrix $\mat{M}$ is best described as the determination of the associated index matrix $\mat{I}$. For both left and right lists this can be achieved by a single forward scan of the rows of $\mat{M}$ and of $\mat{I}$.

\par For a right list $\mat{R}$ it is first necessary to determine $r$, the number of roots. The first $r$ components of $\mat{R}$ are then the roots of the tree in order, the next $\mat{R}_1^1$ components of $\mat{R}$ are the second-level nodes reachable from the first root, and so forth. Programs 1.20 and 1.21 describe the processes for a right list and a left list, respectively.

<table align=center>
<td valign=top><img src="APLimg/prog1x20.bmp"> & <td nowrap> & <td valign=top><table border=1 cellspacing=0 cellpadding=4> <td align=center>1-origin indexing & \\ \begin{tabularx} \mat{R} & <td nowrap> & Right list of \mat{T}. & \\ <td valign=top>\mat{I} & & Right index matrix of \mat{T}. & \\ <td valign=top>\vect{i} & & Index of row of \mat{R} currently examined. & \\ <td valign=top>\textit{j} & & Right list index of node reachable from node \textit{i}. & \\ <td valign=top>\vect{v} & & Current index vector. & \\ <td valign=top>\textit{φ} & & Origin with respect to which \mat{I} is finally expressed. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend} & \\
<td colspan=3 align=center>\textbf{Program 1.20} Determination of the index matrix \mat{I} \\
 associated with a right list matrix \mat{R} & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.20}. In each execution of the main loop 13-16, the $i$th row of the right list $\mat{R}$ is examined to determine the index vector of each node on the succeeding level which is directly reachable from it. The number of such nodes is controlled by the parameter $d$, initialized to the degree of the $i$th node by step 12. The (right list) index of the nodes reachable from node $i$ is determined by $j$, which is incremented on step 14 as the index vector of each node is determined. The index vectors of the successive nodes reachable from node $i$ have the final components 1, 2, 3, ..., and each must be prefixed by the index vector of node $i$. This assignment is effected by the vector $\vect{v}$, which is initialized by the index vector of node $i$ rotated left by one (step 11), and which is incremented by step 15 before each assignment occurring on step 16. At the outset, $\vect{v}$ is set to zero and $d$ is set to the number of roots as determined by step 4.

\par Since $j$ is, at step 10, equal to the current number of roots $r$ augmented by the cumulative degrees of the first $i -$ 1 nodes, then $r = j - i +$ 1 and the exit on step 10 therefore occurs always and only in the event of ill formation. Alternatively, the test can be viewed as an assurance that each row of the matrix $\mat{I}$ is specified before it is itself used in specification.

\par When step 5 is first reached, the index matrix $\mat{I}$ is complete but is expressed in 1-origin indexing with zeros representing the null elements. Steps 5-7 translate the matrix to the origin $φ$ and mask in the necessary null elements.
 & & \\\end{tabularx}

<table align=center>
<td valign=top><img src="APLimg/prog1x21.bmp"> & <td nowrap> & <td valign=top><table border=1 cellspacing=0 cellpadding=4> <td align=center>1-origin indexing & \\ \begin{tabularx} \mat{L} & <td nowrap> & Right list of \mat{T}. & \\ <td valign=top>\mat{I} & & Left index matrix of \mat{T}. & \\ <td valign=top>\textit{j} & & Index of row of \mat{I} currently determined. & \\ <td valign=top>\textit{i} & & Left list index of path node \textit{j} in current path (step 16), or index of last previous node whose branches remain unexhausted (step 22). & \\ <td valign=top>\vect{c}_{\textit{i}+1} & & Index of node following node \textit{i} in the last path traced from \textit{i}. & \\ <td valign=top>\textit{r} & & Pararmeter for testing well formation. & \\ <td valign=top>\vect{v} & & Current index vector. & \\ <td valign=top>\textit{φ} & & Origin with respect to which \mat{I} is expressed. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend} & \\
<td colspan=3 align=center>\textbf{Program 1.21} Determination of the index matrix \mat{I} \\
 associated with a left list matrix \mat{L} & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.21}. The index vectors $\mat{I}^j$ are determined in order under control of the parameter. The loop 5-18 traces a continuous path through the tree, determining the index of each successive node of the path by rotating the index of the preceding node (step 17) and adding one to the last component (step 13), and maintaining in the connection vector $\vect{c}$ a record $\vect{c}_{i+1}$ of the index $j$ of the successor of node $i$ in the path traced. The path is interrupted by the occurrence of a leaf (that is, $\mat{L}_1^j =$ 0 on step 18), and the degree vector $\mat{L}_1$ is then scanned by the loop (19-20) to determine the index $i$ of the last preceding node whose branches remain incompleted. Steps 22-23 then respecify $\vect{v}$ as the index vector of the node following node $i$ in the path last traced, and step 21 decrements the component $\mat{L}_1^i$ of the degree vector. The branch from step 19 to step 22 occurs at the completion of each rooted subtree. The test for well formation (step 12) is the same as that applied to the right list in Program 1.20, except that the notation for the relevant parameters differs. The concluding operations (6-9) include left justification on step 7.
 & & \\\end{tabularx}

<a name="1.23.6"></a>
\par \textbf{Tree, path, and level compression}

\par The \textit{tree compression}

\par \mat{P} $←$ \mat{U}/\mat{T}

\par specifies a tree \mat{P} obtained from \mat{T} by suppressing those nodes corresponding to zeros of the logical tree \mat{U}, and reconnecting so that for every pair of nodes $x, y$ of \mat{P}, $x$ belongs to the subtree of \mat{P} rooted in $y$ if and only if $x$ belongs to the subtree of \mat{T} rooted in $y$. If, for example, \mat{T} is the tree of Fig. 1.16 with $\vect{n}$ as the alphabet, and \mat{U} is the tree of Fig. 1.22a, then \mat{P} is the tree of Fig. 1.22b. The new indices are shown to the left of each node of \mat{P}. The set of nodes 221, 222, ..., 226, are all on the same level of \mat{P} although they have been shown with branches of different lengths to permit easy identification with the nodes of the original tree \mat{T}.

<table align=center>
 <td align=center><img src="APLimg/fig1x22a.bmp"> <p align center>\mat{U}\\
(\textit{a}) & <td valign=top><img src="APLimg/fig1x22v.bmp"> & <td align=center><img src="APLimg/fig1x22b.bmp"> <p align center>\mat{U}/\mat{T}\\
(\textit{b}) & \\
<td align=center colspan=3>\textbf{Figure 1.22}. Compression of tree \mat{T} of Fig. 1.16 (with \vect{n} = alphabet) & \\
\end{tabularx}

\par The compress operation is best executed on the left list because of the grouping by subtrees. Program 1.23 gives a suitable algorithm which also serves as a formal definition of the compress operation.

<table align=center>
<td valign=top><img src="APLimg/prog1x23.bmp"> & <td nowrap> & <td valign=top><table border=1 cellspacing=0 cellpadding=4> <td align=center>1-origin indexing & \\ \begin{tabularx} \vect{u} & <td nowrap> & Left node vector of \mat{U}. & \\ <td valign=top>\mat{L} & & Left list of \mat{T}. & \\ <td valign=top>\textit{j} & & Index of first zero of \vect{u} (steps 4-8). Index of root of smallest subtree containing deleted node (step 12). & \\ <td valign=top>\textit{d} & & Change of degree caused by deletion of \textit{j}. & \\ <td valign=top>\textit{r} & & Number of roots indicated by infix (\mathbf{α}^{\textit{j}} \wedge \overbar{\vect{α}}^{\textit{k}})/\!/\mat{L}, where \textit{j} is initial value and \textit{k} + 1 is current value of \textit{j}. & \\ \end{tabularx} & \\\end{tabularx} \par \textbf{Legend} & \\
<td colspan=3 align=center>\textbf{Program 1.23} Determination of the left list \mat{L} = \mathbf{α}^2/[(\mat{U}/\mat{T}) & \\ 
\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 1.23}. The vector $\vect{u}$ is specified as the node vector of the left list of the controlling logical tree \mat{U} and controls the subsequent process. Step 4 determines $j$ as the index of the first zero component of $\vect{u}$. Steps 6 and 7 then delete the corresponding nodes of $\vect{u}$ and of the left list of \mat{T}, but only after step 5 has determined $d$ as the change in degree which this deletion will occasion to the root of the smallest subtree containing the deleted node. Steps 9-11 perform a backward scan of the degree vector to determine $j$ as the index of the root of the subtree, and step 12 effects the requisite change in its degree. The exit on step 9 occurs only if the node deleted is a root of the original tree, in which event no change is produced in the degree of any other node.
 & & \\\end{tabularx}

<a name="path_compression"></a>
\par Two further compress operations controlled by logical vectors are defined as follows. \textit{Path compression} is denoted by

\par \mat{P} $← \vect{u}/\mat{T}$.

\par \mat{P} is obtained from \mat{T} by suppressing every node on level $j$ if $\vect{u}_j =$ 0, and reconnecting as in tree compression.
<a name="level_compression"></a> \textit{Level compression} is denoted by

\par \mat{P} $← \vect{u}/\!/\mat{T}$,

\par and \mat{P} is obtained from \mat{T} by deleting each rooted subtree $\mat{T}_i$ for which $\vect{u}_i =$ 0.

\par Path compression by a unit vector $\textbf{ϵ}^j$ produces a tree of height one. Such a tree is, in effect, a vector and will be treated as one.

<a name="full_tree"></a>
\par Two related special logical trees are defined: the \textit{path tree}
^{\vect{u}}\mat{E} such that \overbar{\vect{u}}/^{\vect{ u}}\mat{E} = 0 and \vect{u}/^{\vect{ u}}\mat{E} is the full tree \mat{E} whose nodes are all unity, and the \textit{level tree} 
_{\vect{u}}\mat{E} such that \overbar{\vect{u}}/\!/_{\vect{u}}\mat{E} = 0, and \vect{u}/\!/_{\vect{u}}\mat{E} = \mat{E}.

<a name="1.23.7"></a>
\par \textbf{Extension of other operations to trees}

\par Two trees are \textit{compatible} if they have the same structure. Elementary binary operations are extended node by node to compatible trees. For example,

\par \mat{Z} $←$ \mat{X} $\times$ \mat{Y}

\par implies that node $\vect{i}$ of \mat{Z} is the product of node $\vect{i}$ of \mat{X} and node $\vect{i}$ of \mat{Y} for all $\vect{i}$.
<a name="tree_iota"></a> Similarly,

\par \mat{M} $← \vect{b} \textbf{ι}_j$ \mat{T}

\par specifies \mat{M} as a tree (of the same structure as \mat{T}) such that node $\vect{i}$ of \mat{M} is the $j-origin \vect{b}$-index of node $\vect{i}$ of \mat{T}.

<a name="tree_map"></a>
\par The mapping operation is extended to trees so as to permute the rooted subtrees of a tree. Formally

\par \mat{P} $← \vect{m} \int_j$ \mat{T}

\par implies that $\mathbf{μ}_1(\mat{P}) = ν(\vect{m})$, that $\mat{P}_i$ is a single null character if $\vect{m}_i \notin \textbf{ι}^j(\mathbf{μ}_1(\mat{T}))$, and otherwise $\mat{P}_i = \mat{T}_{\vect{m}}$, where $j$-origin indexing is used for \mat{T}.

\par Permutation of the subtrees rooted in node $\vect{i}$ of \mat{T} can be effected as follows:

\par 
\overbar{\vect{α}}^1/\mat{T}_{\vect{i}} ← \vect{m} \int (\overbar{\vect{α}}^1/\mat{T}_{\vect{i}})

<a name="tree_reduction"></a>
\par The notation $\odot/\!/\mat{T}$ will denote the application of the binary operator or relation $\odot$ to the nodes of \mat{T} in right list order (i.e., \textit{down} successive levels) and $\odot/\mat{T}$ will denote the same application in left list order (i.e., \textit{across} paths). If the operator is symmetric (i.e., its operands commute), then $\odot/\!/\mat{T} =$ 
\odot/\mat{T}.

<a name="tree_max"></a>
\par Maximization $(\mat{U}⌈\mat{T})$ and minimization $(\mat{U}⌊\mat{T})$ are extended to trees in the obvious way.

<a name="fig1.24"></a>
<table align=center><table align=center border=1 cellspacing=0 cellpadding=10>
<td colspan=2 align=center>\mat{U} <img align=middle src="APLimg/fig1x24a.bmp"> & \\
α/\mat{U} <img align=middle src="APLimg/fig1x24b.bmp"> & <img align=middle src="APLimg/fig1x24c.bmp"> α/\!/\mat{U} & \\
ω/\mat{U} <img align=middle src="APLimg/fig1x24d.bmp"> & <img align=middle src="APLimg/fig1x24e.bmp"> ω/\!/\mat{U} & \\
\textit{σ}/\mat{U} <img align=middle src="APLimg/fig1x24f.bmp"> & <img align=middle src="APLimg/fig1x24g.bmp"> \textit{σ}/\!/\mat{U} & \\
\textit{τ}/\mat{U} <img align=middle src="APLimg/fig1x24h.bmp"> & <img align=middle src="APLimg/fig1x24i.bmp"> \textit{τ}/\!/\mat{U} & \\
\end{tabularx} & \\
<td align=center>\\
\textbf{Figure 1.24}. Set selection and maximum prefix and suffix operations & \\
\end{tabularx}

\par The operations $α/\vect{u}, ω/\vect{u}, σ/\vect{u}$, and $τ/\vect{u}$ are each extended in two ways: across paths and down levels. Examples of each appear in Fig. 1.24. Operations extending down levels are denoted by double virgules and represent an application of the corresponding vector operation to each level of the tree considered as a vector. For example, the statement

\par \mat{V} $← σ/\!/\mat{A}$

\par implies that each level of \mat{V} is the forward set selection of the corresponding level of \mat{A}, that is, $\textbf{ϵ}^j/\mat{V} = σ/\textbf{ϵ}^j/\mat{A}$. Operations extending across paths are denoted by single virgules and are defined in terms of subtrees. Thus

\par \mat{V} $← α/\mat{U}$

\par implies that \mat{V} is obtained from the logical tree \mat{U} by setting to zero all nodes of any subtree rooted in a zero node, and

\par \mat{V} $← ω/\mat{U}$

\par implies that \mat{V} is obtained from \mat{U} by setting to zero every node whose subtree contains a zero node. The definitions of $σ/\mat{U}$ and $τ/\mat{U}$ are analogous.

<a name="1.23.8"></a>
\par \textbf{Homogeneous trees}

\par If, for all $j$, every node on level $j$ of a tree \mat{T} is either of degree zero or of degree $\mathbf{ν}_{j+1}(\mat{T})$, then the tree \mat{T} is said to be \textit{uniform}. If all leaves of a uniform tree \mat{T} lie in the same level (necessarily the top), then the tree is said to be \textit{homogeneous}. The structure of a homogeneous tree is completely characterized by its dispersion vector $\mathbf{ν}(\mat{T}))$. All maximal paths in a homogeneous tree are clearly of the same length, namely $ν(\mat{T}) = ν(\mathbf{ν}(\mat{T}))$. Figure 1.25 shows a homogeneous tree and its associated dispersion vector.

<table align=center>
<td align=center><img src="APLimg/fig1x25.bmp"> & \\
<td align=center>\begin{tabularx}
\mathbf{ν}(\mat{H}) & = & (2, 3, 2) & \\
\mathbf{μ}(\mat{H}) & = & (2, 6, 12) & \\
\end{tabularx} & \\
<td align=center>\textbf{Figure 1.25}. Homogeneous tree \mat{H} and dispersion and moment vectors & \\
\end{tabularx}

\par A tree \mat{T} for which $\mathbf{ν}(\mat{T}) = m\textbf{ϵ}$ is called an \textit{m-way tree}, and a tree for which $\mathbf{ν}_1(\mat{T}) =$ 1 and $\overbar{\vect{α}}^1/\mathbf{ν}(\mat{T}) = m\textbf{ϵ}$ is called a \textit{singular m-way tree}.

\par The $j$th component of the moment vector of a homogeneous tree is clearly equal to the product of the first j components of the dispersion vector, that is, $\mathbf{μ}(\mat{T}) =$ (<img src="APLimg/quadsw.bmp"> $+ \mathsf{\mat{I}})$ 
{\times \atop /} \mathbf{ν}(\mat{T})). The dispersion vector is, in turn, uniquely determined by the moment vector. The total number of nodes is given by \textit{μ}(\mat{T}) = +/\mathbf{μ}(\mat{T}), and it can also be shown that \textit{μ}(\mat{T}) = \vect{y} ⊥ \vect{y}, where \vect{y} is the dispersion vector in reverse order.

\par Tree compression of a homogeneous tree \mat{H} (that is, \mat{U}/\mat{H}) does not generally produce a homogeneous tree, and, in fact, any tree \mat{P} of arbitrary structure can be represented by a pair of homogeneous trees \mat{U} and \mat{H} such that \mat{P} $=$ \mat{U}/\mat{H}. On the other hand, both path and level compression of homogeneous trees produce homogeneous trees. Moreover, if \mat{P} $= \vect{u}/\mat{H}$, then $\mathbf{ν}(\mat{P}) = \vect{u}/\mathbf{ν}(\mat{H})$, and if \mat{P} $= \vect{u}/\!/\mat{H}$, then $\mathbf{ν}(\mat{P}) = \mathbf{ν}(\mat{H}) - (+/\overbar{\vect{u}})\mathbf{α}^1$.

<a name="full_tree_k"></a>
\par Since the structure of a homogeneous tree is completely specified by its dispersion vector $\vect{k}$, the structure of the special logical trees can be specified in the forms 
\mathsf{\mat{E}}(\vect{k}), 
^{\vect{u}}\mathsf{\mat{E}}(\vect{k}), and 
_{\vect{u}}\mathsf{\mat{E}}(\vect{k}).

\par In a homogeneous tree, the right list or left list index of a node can be determined as an explicit function of its index vector. Conversely, the index vector $\vect{i}$ can be determined directly from the corresponding left list index, to be denoted by $l(\vect{i})$, or from the right list index $r(\vect{i})$. In developing the relations between indices it will be convenient to use 0-origin indexing throughout.

\par The right list index is given by

\par $r(\vect{i}) = f(\vect{i}) + g(\vect{i})$,

\par $where\\$
 \textit{f}(\vect{i}) = +/\mathbf{α}^{\textit{ν}(\vect{i})-1}/\mathbf{μ}(\mat{T})

\par is the number of nodes in the first $ν(\vect{i}) -$ 1 levels, and

\par $g(\vect{i}) = (\mathbf{α}^{ν(\vect{i})}/\mathbf{ν}(\mat{T})) ⊥ \vect{i}$

\par is the rank of node $\vect{i}$ in the $ν(\vect{i})$th level. For example, if $\vect{i} =$ (1, 0, 1) in the tree of Fig. 1.25, then $\mathbf{μ}(\mat{H}) =$ (2, 6, 12), $f(\vect{i}) =$ +/(2, 6) $=$ 8, and $g(\vect{i}) =$ (2, 3, 2) $⊥$ (1, 0, 1) $=$ 7.

\par Since $f(\vect{i})$ depends only on $ν(\vect{i})$, the index $\vect{i}$ may be determined from $r$ by first determining $ν(\vect{i})$ as the largest value for which $f(\vect{i}) \leq r$, and then determining $\vect{i}$ such that

\par $(\mathbf{α}^{ν(\vect{i})}/\mathbf{ν}(\mat{T})) ⊥ \vect{i} = r - f(\vect{i})$.

\par In tracing a path through a tree, the $k$th node of the set reachable from node $\vect{i}$ is the node $\vect{j} = \vect{i} \oplus (k)$. It is therefore useful to express $r(\vect{j})$ as a function of $r(\vect{i})$. Clearly

\begin{tabularx}
 & \textit{f}(\vect{j}) = \textit{f}(\vect{i}) + (\mathbf{μ}(\mat{T}))_{\textit{ν}(\vect{i})-1}, & \\
 & \textit{g}(\vect{j}) = \textit{g}(\vect{i}) \times (\mathbf{ν}(\mat{T}))_{\textit{ν}(\vect{i})} + \vect{j}_{\textit{ν}-1}. & \\\end{tabularx}

\par In the special case of a singular homogeneous $m-way$ tree,

\begin{tabularx}
 & \textit{f}(\vect{i}) & = & 
1+ \textit{m} + \textit{m}^2 + ... + \textit{m}^{\textit{ν}(\vect{i})-2}
<td colspan=2> & <td valign=middle>= & (\textit{m}\textbf{ϵ}) ⊥ \textbf{ϵ}(\textit{ν}(\vect{i}) - 1) & \\
<td colspan=2> & <td valign=middle>= & \begin{tabularx}
\par $m^{ν(\vect{i})-1} - 1\\$

<img src="APLimg/hline1x23x8.bmp">\\
 \textit{m} - 1 & \\
\end{tabularx} & \\
\end{tabularx}

\par Hence $f(\vect{j}) =$ 1 $+ m \times f(\vect{i})$, and $g(\vect{j}) =$ m $\times g(\vect{i}) + \vect{j}_{ν-1}$. Recursion can therefore be performed simply upon the single function $r(\vect{i})$ as follows:

\par $r(\vect{j}) = m \times r(\vect{i}) +$ 1 $+ \vect{j}_{ν-1}$.

\par The left list index $l(\vect{i})$ is most conveniently expressed as a function of $ν(\vect{i})$ and of the vector $\vect{z}(\vect{i})$ (zero extension of $\vect{i})$, where $\vect{z} = \mathbf{α}^{ν(\vect{i})}(ν(\mat{T}))\backslash \vect{i}$. Clearly $ν(\vect{z}) = ν(\mat{T})$ and $\vect{z}$ is the index of the ``earliest'' leaf reachable from node $\vect{i}$. In Fig. 1.25, for example, $\vect{z}((1$, 2)) $=$ (1,2,0).

\par The zero extension has the obvious property that every node above the path \mat{T}^{ $\vect{z}(\vect{i})}$ precedes node $\vect{i}$ in the left list, and every node below the path follows it. The number of nodes in the path which precede node $\vect{i}$ is $ν(\vect{i}) -$ 1.

\par The number of leaves above the path \mat{T}^{ $\vect{z}(\vect{i})}$ is $\mathbf{ν}(\mat{T}) ⊥ \vect{z}(\vect{i})$, and more generally, the number of $(j -$ 1)th level nodes above it is given by $(\mathbf{α}^j/\mathbf{ν}(\mat{T})) ⊥ (\mathbf{α}^j/\vect{z}(\vect{i}))$. Consequently,

\par $l(\vect{i}) = ν(\vect{i}) -$ 1 $+$ <img align=middle src="APLimg/sigma1x23.bmp"> $(\mathbf{α}^j/\mathbf{ν}(\mat{T})) ⊥ (\mathbf{α}^j/\vect{z}(\vect{i}))$.

\par For example, if $\vect{i} =$ (1,0) in Fig. 1.25, then $\vect{z}(\vect{i}) =$ (1, 0, 0) and $l(\vect{i}) = ν(\vect{i}) -$ 1 $+$ (2) $⊥$ (1) $+$ (2, 3) $⊥$ (1, 0) $+$ (2, 3, 2) $⊥$ (1, 0, 0) $=$ 11. The foregoing result may be written alternatively as

\par $l(\vect{i}) = ν(\vect{i}) -$ 1 $+ \vect{w}$ {+ $\atop \times} \vect{z}(\vect{i})$,

\par where $\vect{w}_ν =$ 1, and $\vect{w}_{i-1} =$ 1 $+ (\vect{w}_i \times \mathbf{ν}_i(\mat{T}))$. In the foregoing example, $\vect{w} =$ (10, 3, 1), and $\vect{w}$ {+ $\atop \times} \vect{z}(\vect{i}) =$ 10. This form is most convenient for determining $\vect{i}$ as a function of $l$, for since $\vect{w}$ {+ $\atop \times} \vect{z} = l +$ 1 $- ν(\vect{i})$, then $\vect{z}_{ 0}(\vect{i}) = ⌊l ÷ \vect{w}_0⌋, \vect{z}_{ 1}(\vect{i}) = ⌊((\vect{w}_0 | l) -$ 1) $÷ \vect{w}_1⌋$, etc. for all positive values of the quotient, and all components thereafter are zero. The dimension $ν(\vect{i})$ is then determined from the relation $ν(\vect{i}) = l +$ 1 $- \vect{w}$ {+ $\atop \times} \vect{z}(\vect{i})$.

<a name="ref1"></a>
\par \textbf{References}

\begin{tabularx}
<td valign=top nowrap>• & Birkhoff, G., and S. MacLane (1941), \textit{A Survey of Modern Algebra}, Macmillian, New York. & \\
<td valign=top>• & Burks, A.W., D.W. Warren, and J.B. Wright (1954), ``An Analysis of a Logical Machine Using Parenthesis-free Notation'', \textit{Mathematical Tables and Other Aids to Computation}, vol. VIII, pp. 53-57. & \\
<td valign=top>• & Dickson, L.E. (1939), \textit{New First Course in the Theory of Equations}, Wiley, New York. & \\
<td valign=top>• & Garner, Harvey L. (1959), ``The Residue Number System'', \textit{IRE Transactions}, vol. EC-8, pp. 140-147. & \\
<td valign=top>• & Goldstine, H.H., and J. von Neumann (1947), ``Planning and Coding of Problems for an Electronic Computing Instrument'', \textit{Report on the Mathematical and Logical Aspects of an Electronic Computing Instrument}, Part II, vol. 1, Institute for Advanced Study, Princeton. & \\
<td valign=top>• & Iverson, K.E. (1954), 
<a target=_parent href="http:/\!/www.jsoftware.com/papers/MSLDE.htm">``Machine Solutions of Linear Differential Equations''</a>, Doctoral Thesis, Harvard University. $& \\$
<td valign=top>• & Jacobson, N. (1951), \textit{Lectures in Abstract Algebra}, vol. 1, Van Nostrand, New York. & \\
<td valign=top>• & Kunz, K.S. (1957), \textit{Numerical Analysis}, McGraw-Hill, New York. & \\
<td valign=top>• & Margenau, H., and G.M. Murphy (1943), \textit{The Mathematics of Physics and Chemistry}, Van Nostrand, New York. & \\
<td valign=top>• & Phister, M. (1958), \textit{Logical Design of Digital Computers}, Wiley, New York. & \\
<td valign=top>• & Richards, R.K. (1955), \textit{Arithmetic Operations in Digital Computers}, Van Nostrand, New York. & \\
<td valign=top>• & Riordan, J. (1958), \textit{An Introduction to Combinatorial Analysis}, Wiley, New York. & \\
<td valign=top>• & Rutishauser, H. (1959), ``Zur Matrizeninversion nach Gauss-Jordan'', \textit{Zeitschrift für Angewandte Mathematik und Physik}, vol. X, pp. 281-291. & \\
<td valign=top>• & Wright, H.N. (1939), \textit{First Course in Theory of Numbers}, Wiley, New York. & \\
\end{tabularx}

<a name="notes1"></a>
\par \textbf{Notes}

\begin{tabularx}
<td valign=top nowrap>a.<a name="note1a"></a> & Restating the relation in terms of the 0-residue will illustrate the convenience of the 1-residue used here. & \\
<td valign=top>b.<a name="note1b"></a> & Since each ``vector'' \vect{y}_{\textit{i}}
\odot_2 \vect{x}_{\textit{j}} is of dimension one, no scan operator
\odot_1 is required, and the symbol ∘ may be interpreted as a ``null'' scan. & \\
<td valign=top>c.<a name="note1c"></a> & These transpositions generate the rotation group of the square [cf. <acronym title="Birkhoff, G., and S. MacLane (1941), A Survey of Modern Algebra, Macmillian, New York.">Birkhoff and MacLane (1941)</acronym> Chap. VI]. A pair of transpositions commute if and only if their axes are perpendicular. Hence the pair ← and ↑ may be written unambiguously as <img src="APLimg/luarr.bmp">. Moreover, <img src="APLimg/luarr.bmp"> = 
<img src="APLimg/xarr.bmp"> . The remaining two transformations can be denoted by <img src="APLimg/ltarr.bmp"> and
<img src="APLimg/gtarr.bmp"> , with the convention that the operator nearest the operand (i.e., the horizontal) is executed first. & \\
<td valign=top>d.<a name="note1d"></a> & The symbols ∪ and ∩ (and the operations they denote) are commonly called \textit{cup} and \textit{cap}, respectively. & \\
<td valign=top>e.<a name="note1e"></a> & For the purposes of describing algorithms, this notation is superior to the classical ``disjoint cycles'' notation for permutations
<acronym title="Birkhoff, G., and S. MacLane (1941), A Survey of Modern Algebra, Macmillian, New York."> [cf. Birkhoff and MacLane, (1941)]</acronym> because (1) the direction of the transformation (from \vect{a} to \vect{c}) is unequivocally indicated, and (2) the notation directly indicates a straightforward and efficient method for the actual execution, namely, indirect addressing. & \\
<td valign=top>f.<a name="note1f"></a> & If the function \textit{f} is many-to-one, the specification of a unique inverse \textit{g} is achieved by restricting the range of \textit{g} to some set of ``principal'' values, as is done, for example, for the inverse trigonometric functions. & \\
<td valign=top>g.<a name="note1g"></a> & Further levels can, of course, be handled by considering a family of matrices 
^1\mat{M},
^2\mat{M}, ...,
^{\textit{n}}\mat{M}, or families of families 
_{\textit{j}}^{\textit{i}}\mat{M}. & \\
<td valign=top>h.<a name="note1h"></a> & Although such a genealogical tree is not necessarily a tree in the mathematical sense, it will be assumed so for present purposes. & \\
<td valign=top>i.<a name="note1i"></a> & In the 1-origin indexing system used here it would be possible to use the numeric zero to represent the null. In 0-origin indexing, however, zeros occur as components of index vectors and must be distinguishable from the nulls used. & \\
<td valign=top>j.<a name="note1j"></a> & The term \textit{left list} and the notation [\mat{T} are both intended to suggest left justification. & \\
<td valign=top>k.<a name="note1k"></a> & These statements hold only for 1-origin indexing. In 0-origin indexing, \vect{a} = (∘, 0, 1, ..., \textit{δ}(\mat{T}) - 1). & \\
\end{tabularx}

<a name="ex1"></a>
\par \textbf{Exercises}

\par Organize each of the programs according to the method of leading decisions. Except where otherwise indicated, use 1-origin indexing. The conventions of Sec. S.1 of the Summary of Notation will be used in the statement of each of the exercises.

\par \textbf{1.1} Let $\vect{d} =$ (a, 2, 3, 4, 5, 6, 7, 8, 9, 10, j, q, k), $\vect{s} =$ (c, d, h, s), $\vect{u} =$ (1, 0, 1, 0, 1), $\vect{v} =$ (0, 1, 1, 1, 0), $\vect{x} =$ (16, 8, 4, 2, 1), and $\vect{y} =$ (2, 3, 4, 5, 6). Determine

\begin{tabularx}
<td valign=top>(a) & & the dimensions \textit{ν}(\vect{d}), \textit{ν}(\vect{s}), and \textit{ν}(\vect{x}).

<td valign=top>(b) & & the vectors \vect{x} + \vect{y}, \vect{x} - \vect{y}, \vect{x} \times \vect{y}, \vect{x} ÷ \vect{y}, and \vect{u} + \vect{v}.

<td valign=top>(c) & & the logical vectors \vect{u} \wedge \vect{v}, \vect{u} \vee \vect{v}, (\vect{u} \neq \vect{v}), and (\vect{u} = \vect{v}).

<td valign=top>(d) & & the reductions +/\vect{x}, \times/\vect{y}, \wedge/\vect{u}, and \vee/\vect{v}.

<td valign=top>(e) & & the base two values of \vect{u} and of \vect{v}, that is, +/(\vect{x} \times \vect{u}), and +/(\vect{x} \times \vect{v}).

<td valign=top>(f) & & the rotated vectors 2 ↓ \vect{d}, 4 ↑ \vect{s}, and ↑ \vect{y}.

<td valign=top>(g)^{} & & the unit vectors \textbf{ϵ}^1(5) in a 1-origin system, and \textbf{ϵ}^2(5) in a 0-origin system.

<td valign=top>(f)^{} & & the infixes (\mathbf{α}^5(7) \wedge \textbf{ω}^5(7)) and 2 ↓ \mathbf{α}^3(7).

\end{tabularx}

\par \textbf{1.2} Show that
\begin{tabularx}
<td valign=top>(a)^{} & & \times/\textbf{ι}^1(\textit{n}) = \textit{n}! (Include the case \textit{n} = 0.)

<td valign=top>(b)^{} & & +/\textbf{ι}^{ \textit{j}}(\textit{n}) = \textit{n}(\textit{n} + 2\textit{j} - 1) ÷ 2.

<td valign=top>(c) & & \times/(\textit{k} ↑ \vect{x}) = \times/\vect{x}. 

<td valign=top>(d) & & (\textit{k} ↑ \vect{x}) + (\textit{k} ↑ \vect{y}) = \textit{k} ↑ (\vect{x} + \vect{y}).

\end{tabularx}

\par \textbf{1.3} Write detailed (i.e., component-by-component) programs for the following operations. Include tests for compatibility of the operands.

\begin{tabularx}\begin{tabularx}
(a) & & \vect{w} ← \vect{u} \wedge \vect{v}. & \\
(b) & & \mat{W} ← \mat{U} \vee \mat{V}. & \\
(c) & & \vect{b} ← \vect{u}/\vect{a}. & \\
(d) & & \mat{B} ← \vect{u}/\mat{A}. & \\
(e) & & \mat{B} ← \vect{u}/\vect{v}/\!/\mat{A}. & \\
(f) & & \vect{x} ← (\vect{x} > 0)/\vect{x}. & \\
\end{tabularx} & & <td valign=top>\begin{tabularx}
(g) & & \vect{u} ← \mathbf{α}^{\textit{j}}(\textit{k}). & \\
(h) & & \vect{u} ← \textit{i} ↓ \mathbf{α}^{\textit{j}}(\textit{k}). & \\
(i) & & \vect{c} ← \backslash\vect{a}, \vect{u}, \vect{b}\backslash. & \\
(j) & & \vect{c} ← /\vect{a}, \vect{u}, \vect{b}/. & \\
(k) & & \vect{c} ← \vect{u}\backslash\vect{a}. & \\
\end{tabularx} & \\\end{tabularx}

\par \textbf{1.4} Established the identities
\begin{tabularx}
(a) & & /\vect{a}, \vect{u}, \vect{b}/ = \backslash\overbar{\vect{u}}/\vect{a}, \vect{u}, \vect{u}/\vect{b}\backslash. & \\
(b) & & \backslash\vect{a}, \vect{u}, \vect{b}\backslash = /\overbar{\vect{u}}\backslash\vect{a}, \vect{u}, \vect{u}\backslash\vect{b}/. & \\
\end{tabularx}

\par \textbf{1.5} The classic $``rings-o-seven''$ puzzle can be posed as follows: an order collection of $n$ rings is to be placed on (removed from) a bar under the following constraints:

\begin{tabularx}
<td valign=top>(i) & & ring \textit{n} may be placed on or removed at will. & \\
<td valign=top>(ii) & & ring \textit{k} may be placed on or removed only if ring (\textit{k} + 1) is on and all succeeding rings are off. & \\
\end{tabularx}

The state of the rings can be described by a logical vector \vect{u}, with \vect{u}_{\textit{k}} = 1 if ring \textit{k} is on. Write programs on \vect{u} which describe the removal of the rings beginning with

\begin{tabularx}
<td valign=top>(a) & & \vect{u} = \textbf{ϵ} [The successive values of \vect{u} represent a \textit{reflected Gray code}; see
<acronym title="Phister, M. (1958), Logical Design of Digital Computers, Wiley, New York.">Phister (1958)</acronym>.] & \\
<td valign=top>(b) & & \vect{u} arbitrary. & \\
\end{tabularx}

\par \textbf{1.6} The ordered array of variables used to represent a variable $x$ in some coding system may be considered as a \textit{vector representation} of $x$, denoted by $\mathbf{ρ}(x)$. In the 8421 code for decimal digits, for example, $\mathbf{ρ}(0) =$ (0, 0, 0, 0), $\mathbf{ρ}(1) =$ (0, 0, 0, 1), and, in general, $\mathbf{ρ}(x)$ is defined by the relation $+/[\vect{w} \times \mathbf{ρ}(x)] = x$, where $\vect{w} =$ (8, 4, 2, 1). For each of the following coding systems, (see
<acronym title="Richards, R.K. (1955), Arithmetic Operations in Digital Computers, Van Nostrand, New York.">Richards</acronym>, pp. 183-184 for definitions), write a concise expression for \mathbf{ρ}(\textit{x}):

\begin{tabularx}
<td valign=top>(a) & & the excess-three code for decimal digits.

<td valign=top>(b) & & any chosen two-out-of-five code.

<td valign=top>(c) & & any chosen biquinary code.

<td valign=top>(d) & & the semaphore code for the alphabet (see any good dictionary). Denote each code by a two-component vector \mathbf{ρ}(\textit{x}) ⊆ \textbf{ι}^0(8). Use \vect{a}ι\textit{x}, where \vect{a} = (a, b, c, ..., z).

\end{tabularx}

\par \textbf{1.7} Let $\mat{X}$ be a square sparse matrix represented by the logical matrix $\mat{U} = (\mat{X} \neq$ 0) and either or both the vectors $\vect{r} = \mat{U}/\mat{X}$, and $\vect{c} = \mat{U}/\!/\mat{X}$. Write programs to determine the product $\mat{Y} = \mat{X}$ {+ $\atop \times} \mat{X}$, using the arguments

\begin{tabularx}
<td valign=top>(a) & & \vect{r}, \vect{c}, and \mat{U}. & \\
<td valign=top>(b) & & \vect{r} and \mat{U}. & \\
<td valign=top>(c) & & \vect{c} and \mat{U}. & \\
\end{tabularx}

\par \textbf{1.8} Prove that
\begin{tabularx}
<td valign=top>(a) & & ⌈\textit{x}⌉ = -⌊-\textit{x}⌋

<td valign=top>(b) & & ⌊⌊ \textit{a} ÷ \textit{b}⌋ ÷ \textit{c}⌋ = ⌊ \textit{a} ÷ \textit{bc}⌋ for all positive integers \textit{a}, \textit{b}, and \textit{c}.

\end{tabularx}

\par \textbf{1.9} Let $\vect{r} = \mathsf{\mat{E}}/\mat{A}$, and $\vect{c} = \mathsf{\mat{E}}/\!/\mat{A}$ be the row list and column list, respectively, of the matrix $\mat{A}$, and let $\vect{r}_h, \mat{A}_j^i$, and $\vect{c}_k$ be the corresponding elements of the three representations of $\mat{A}$. Determine:
\begin{tabularx}
<td valign=top>(a) & & \textit{h} as a function of \textit{k}, \textit{ν}(\mat{A}), and \textit{μ}(\mat{A}).

<td valign=top>(b) & & \textit{k} as a function of \textit{h}, \textit{ν}(\mat{A}), and \textit{μ}(\mat{A}).

<td valign=top>(c) & & the permutation vector \vect{h} such that \vect{c} = \vect{h} \int \vect{r}.

\end{tabularx}

\par \textbf{1.10} Show that
\begin{tabularx}
<td valign=top>(a) & & \wedge/\vect{u} = <img src="APLimg/ex1x10a.bmp"> (Use De Morgan's law for two variables and induction.)

<td valign=top>(b) & & \neq/\vect{u} = 2|_0 +/\vect{u} (Use induction.)

<td valign=top>(c) & & =/\vect{u} = <img align=top src="APLimg/ex1x10c.bmp">.

<td valign=top>(d) & & \neq/\vect{u} = <img align=top src="APLimg/ex1x10d.bmp">.

<td valign=top>(e) & & \mat{U} <img src="APLimg/neand.bmp"> \vect{v} = (2\textbf{ϵ}) |_0 (\mat{U} {+ \atop \times} \vect{v}).

<td valign=top>(f) & & \mat{U} <img src="APLimg/neand.bmp"> \mat{V} =
<img align=bottom src="APLimg/ex1x10f.bmp">.

<td valign=top>(g) & & (\vect{t} {\circ \atop \wedge} \vect{u}) \wedge (\vect{v} {\circ \atop \wedge} \vect{w}) = (\vect{t} {\circ \atop \wedge} \vect{w}) \wedge (\vect{v} {\circ \atop \wedge} \vect{u}).

\end{tabularx}

\par \textbf{1.11}
\begin{tabularx}
<td valign=top>(a) & & Show that +/\vect{x} = +/(\overbar{\vect{u}}/\vect{x}) +(\vect{u}/\vect{x}). (Include the case \vect{u} = 0.)

<td valign=top>(b) & & What properties are required of an operator
\odot that it satisfy the relation established for + in part (a)?

\end{tabularx}

\par \textbf{1.12} Show that
\begin{tabularx}
<td valign=top>(a) & & \mat{X} {+ \atop \times} \mat{Y} = (\overbar{\vect{u}}/\mat{X})
{+ \atop \times} (\overbar{\vect{u}}/\!/\mat{Y}) + (\vect{u}/\mat{X})
{+ \atop \times} (\vect{u}/\!/\mat{Y}).

<td valign=top>(b) & & \vect{u}/(\mat{X} {+ \atop \times} \mat{Y}) = \mat{X} {+ \atop \times} (\vect{u}/\mat{Y}).

<td valign=top>(c) & & \vect{u}/\!/(\mat{X} {+ \atop \times} \mat{Y}) = (\vect{u}/\!/\mat{X}) 
{+ \atop \times} \mat{Y}.

<td valign=top>(d) & & (\vect{u} \wedge \vect{v})/\vect{a} = (\vect{u}/\vect{v})/(\vect{u}/\vect{a}).

\end{tabularx}

\par \textbf{1.13} Use the result of Exercise 1.11 (b) to extend the results of Exercise 1.12 (a-c) to logical operators.

\par \textbf{1.14} Write programs to determine:
\begin{tabularx}
<td valign=top>(a) & & the value of the polynomial \vect{x} at the point \textit{a}, that is, to evaluate (\textit{y}\textbf{ϵ}) ⊥ \vect{x} for \textit{y} = \textit{a}. Use no more than \textit{ν}(\vect{x}) multiplications.

<td valign=top>(b) & & the derivative of the polynomial \vect{x}, that is, the vector \vect{z} such that\\
 (\textit{y}\textbf{ϵ}) ⊥ \vect{z} =
<img align=top src="APLimg/ddy.bmp"> ((\textit{y}\textbf{ϵ}) ⊥ \vect{x}), and \textit{ν}(\vect{z}) = \textit{ν}(\vect{x}).

<td valign=top>(c) & & the integral \vect{z} of the polynomial \vect{x} satisfying the boundary condition\\
 (\textit{a}\textbf{ϵ}) ⊥ \vect{z} = \textit{b}.

<td valign=top>(d) & & the quotient \vect{q} and remainder \vect{r} obtained in dividing the polynomial \vect{n} by the polynomial \vect{d}, for \textit{ν}(\vect{d}) \leq \textit{ν}(\vect{n}).

<td valign=top>(e) & & the value of the polynomial \vect{n} at the point \textit{a} by using part (d) with \vect{d} = (1, -\textit{a}).

<td valign=top>(f) & & the value of <img align=top src="APLimg/ddy.bmp"> ((\textit{y}\textbf{ϵ}) ⊥ \vect{n}) at the point \textit{a} by two applications of part (e).

<td valign=top>(g) & & an approximate real root of the equation (\textit{y}\textbf{ϵ}) ⊥ \vect{x} = 0 using parts (e) and (f) and the Newton-Raphson formula
<acronym title="Kunz, K.S. (1957), Numerical Analysis, McGraw-Hill, New York.">[Kunz (1957)]</acronym>.

\end{tabularx}

\par \textbf{1.15} Let the components of the vector $\vect{r}$ be the real roots of a polynomial $\vect{x}$. Write a program to
\begin{tabularx}
<td valign=top>(a) & & determine the symmetric functions of \vect{r}. [<acronym title="Dickson, L.E. (1939), New First Course in the Theory of Equations, Wiley, New York.">Dickson (1939)</acronym>, Ch. X.]

<td valign=top>(b) & & determine \vect{x} as a function of \vect{r}.

\end{tabularx}

\par \textbf{1.16} Write a program to determine the polynomial $x$ consisting of the first $n$ terms of the exponential series 1 $+ y + y^2/2! +$ ... .

\par \textbf{1.17} Write a program to determine the moduli of all roots of the polynomial $x$, using the Graeffe method 
<acronym title="Kunz, K.S. (1957), Numerical Analysis, McGraw-Hill, New York.">[Kunz (1957)]</acronym>. Assume that operations for the logarithm and exponential functions are available as subroutines.

\par \textbf{1.18} List all the 1-origin permutation vectors of dimension four which are self-inverse.

\par \textbf{1.19} Using 1-origin indexing, write programs to derive
\begin{tabularx}
<td valign=top>(a) & & the permutation \vect{k} which is inverse to the permutation \vect{j}.

<td valign=top>(b) & & a permutation \vect{j} which transforms a given logical vector \vect{u} to a prefix vector.

\end{tabularx}

\par \textbf{1.20} A square logical matrix $\mat{U}$ such that $+/\mat{U} = +/\!/\mat{U} = \textbf{ϵ}$ is sometimes called a \textit{permutation matrix}, since premultiplication of a numerical vector $\vect{x}$ determines a permutation of $\vect{x}$. Write programs to determine
\begin{tabularx}
<td valign=top>(a) & & the permutation matrix \mat{U} corresponding to the 1-origin permutation vector \vect{k}, that is, determine \mat{U} such that \mat{U} {+ \atop \times} \vect{x} = \vect{k} \int_1 \vect{x}.

<td valign=top>(b) & & the permutation \vect{k} corresponding to a given permutation matrix \mat{U}.

<td valign=top>(c) & & the permutation \mat{V} which is inverse to the permutation \mat{U}.

\end{tabularx}

\par \textbf{1.21} Let $\vect{p}$ be the vector representation of a permutation and let $\vect{c}$ be the standard representation in terms of disjoint cycles, including all cycles of one [<acronym title="Jacobson, N. (1951), Lectures in Abstract Algebra, vol. 1, Van Nostrand, New York.">Jacobson (1951)</acronym>, p. 34]. Each cycle of $\vect{c}$ is enclosed in square brackets, each half-bracket being considered as a component of $\vect{c}$. For example, $\vect{c} =$ ([, 1, 3, 5, ], [, 2, 4, ], [, 6, ]), then $\vect{p} =$ (3, 4, 5, 2, 1, 6), $ν(\vect{p}) =$ 6, and, in general, $ν(\vect{c}) = ν(\vect{p}) + 2k$ where $k$ is the number of disjoint cycles in $\vect{p}$. The usual elision of cycles of one would give $\vect{c} =$ ([, 1, 3, 5, ], [, 2, 4, ]), but this determines a unique correspondent $\vect{p}$ only if the dimension of $\vect{p}$ is otherwise specified, and inclusion of all cycles of one will therefore be assumed. If each infix of numerical components in $\vect{c}$ is preceded by a left bracket and followed by a right bracket, and if $\vect{c}$ determines a legitimate permutation vector $\vect{p}$, then $\vect{c}$ is said to be \textit{well formed}.

\begin{tabularx}
<td valign=top>(a) & & Write a program to determine \vect{p} as a function of a well formed permutation \vect{c}. Include determination of the dimension of \vect{p}. & \\
<td valign=top>(b) & & Modify the program in part (a) to incorporate checks on the well formation of \vect{c}. If \vect{c} is ill formed, the vector \vect{p} is to be defined as the literal ``ill formed''. & \\
<td valign=top>(c) & & Modify part (b) to process a sequence of vectors \vect{c}^1, \vect{c}^2, ..., each pair being separated by a single null element, and the end of the sequence being indicated by a pair of successive null elements, i.e., to process \vect{z} = \vect{c}^1
\oplus (∘) 
\oplus \vect{c}^2
\oplus ... \oplus \vect{c}^{\textit{r}} \oplus (∘,∘). Include checks on the well formation of each permutation. & \\
<td valign=top>(d) & & Write a program to determine the parity [<acronym title="Jacobson, N. (1951), Lectures in Abstract Algebra, vol. 1, Van Nostrand, New York.">Jacobson (1951)</acronym>, p. 36] of a permutation vector \vect{p}. & \\
\end{tabularx}

\par \textbf{1.22} Write detailed programs for the following processes:
\begin{tabularx}
<td valign=top>\begin{tabularx}
<td valign=top>(a) & & \vect{k} ← θ_1/\vect{x} & \\
<td valign=top>(b) & & \vect{y} ← \vect{m} \int_1 \vect{x} & \\
<td valign=top>(c) & & \vect{v} ← \vect{u} ⌈ \vect{x} & \\
<td valign=top>(d) & & \mat{V} ← \vect{u} <img src="APLimg/jotmax.bmp"> \mat{X} & \\
<td valign=top>(e) & & \vect{v} ← α/\vect{u} & \\
<td valign=top>(f) & & \mat{V} ← ω/\!/\mat{U} & \\
<td valign=top>(g) & & \vect{v} ← \textit{σ}/\vect{b} & \\
<td valign=top>(h) & & \mat{V} ← \textit{τ}/\!/\mat{B} & \\
\end{tabularx} & & <td valign=top>\begin{tabularx}
<td valign=top>(i) & & \vect{m} ← \vect{b} ι_0 \vect{a} & \\
<td valign=top>(j) & & \mat{M} ← \mat{B} <img src="APLimg/jotiota.bmp">_0 \vect{a} & \\
<td valign=top>(k)^{} & & \vect{u} ← \textbf{ϵ}_{\vect{b}}^{\vect{a}} & \\
<td valign=top>(l) & & \vect{c} ← \vect{b} ∩ \vect{a} & \\
<td valign=top>(m) & & \vect{c} ← \vect{b} ∆ \vect{a} & \\
<td valign=top>(n) & & \vect{c} ← \vect{b} ∪ \vect{a} & \\
<td valign=top>(o) & & \mat{C} ← \vect{b} <img src="APLimg/circletimes.bmp"> \vect{a} & \\\end{tabularx} & \\
\end{tabularx}

\par \textbf{1.23}
\begin{tabularx}
<td valign=top>(a) & & Copy onto file Φ_1^2 successive groups of items from the row of files Φ^ι in cyclic order, omitting any exhausted files. The end of each group is demarked by a partition λ_2, and the end of each file by a partition λ_3. & \\
<td valign=top>(b) & & A file which is always recorded in the forward direction and read in the backward direction functions as a \textit{stack}. Using file Φ_2^2 as a stack, modify the program of part (a) so as to reverse (in the output file Φ_1^2) the order of the items within each group. & \\
\end{tabularx}

\par \textbf{1.24} The accompanying node vector $\vect{n}$ and connecting matrix $\mat{C}$ together specify a directed graph $(\mat{C}_j^i =$ 1 indicates a branch from node $i$ to node $j)$ which is, in fact, a tree.

\par $\vect{n} =$ (a, b, c, d, e, f, g)

\begin{tabularx}
<td rowspan=7> \mat{C} = & 
<td rowspan=7><img src="APLimg/matrixl7.bmp"> & 
^{}0 0 0 0 1 1 0_{} & 
<td rowspan=7><img src="APLimg/matrixr7.bmp"> & \\
^{}0 0 0 0 0 0 0_{} & \\
^{}1 0 0 1 0 0 0_{} & \\
^{}0 0 0 0 0 0 0_{} & \\
^{}0 0 0 0 0 0 0_{} & \\
^{}0 0 0 0 0 0 0_{} & \\
^{}0 1 0 0 0 0 0_{} & \\
\end{tabularx}

\begin{tabularx}
<td valign=top>(a) & & Draw one of the possible ordered trees represented by \vect{n} and \mat{C}.

<td valign=top>(b) & & For the tree \mat{T} of part (a) show the full left list [\mat{T}.

<td valign=top>(c) & & Show the full right list ]\mat{T}.

\end{tabularx}

\par \textbf{1.25} Write programs which includes tests on compatibility and which determine
\begin{tabularx}
<td valign=top>(a) & & \mat{L} = [\mat{T} from \mat{R} = ]\mat{T}

<td valign=top>(b) & & \mat{S} = ](\vect{u}/\mat{T}) from \mathbf{μ}(\mat{T}), ]\mat{T}, and \vect{u}

<td valign=top>(c) & & \mat{M} = [(\vect{u}/\!/\mat{T}) from \mat{L} = [\mat{T} and \vect{u}

<td valign=top>(d) & & \mat{M} = [(\vect{k} \int_1 \mat{T}) from \mat{L} = [\mat{T} and \vect{k}

\end{tabularx}

\par \textbf{1.26}
\begin{tabularx}
<td valign=top>(a) & & Give an example of a well formed right list which demonstrates that a \textit{prefix} of a right list is not, in general, well formed.

<td valign=top>(b) & & Show clearly where the argument used in establishing the well formation of any suffix of a well formed list breaks down when applied to a prefix.

\end{tabularx}

\par \textbf{1.27} Give formal proofs for the facts that
\begin{tabularx}
<td valign=top>(a) & & a left list groups nodes by subtrees.

<td valign=top>(b) & & a right list groups nodes by levels.

<td valign=top>(c) & & \mathbf{μ}_1(\mat{T}) = \textit{ν}(\vect{d}) - +/\vect{d}, where \vect{d} is the degree vector of \mat{T}.

\end{tabularx}

\par \textbf{1.28} Write programs to determine $\mathbf{μ}(\mat{T})$ as a function of
\begin{tabularx}
<td valign=top>(a) & & the left list degree vector of \mat{T}.

<td valign=top>(b) & & the right list degree vector of \mat{T}.

\end{tabularx}

\par \textbf{1.29} Trace Programs 1.20 and 1.21 for the tree of Exercise 1.24.

\par \textbf{1.30} Show that for a homogeneous tree \mat{H}, $μ(\mat{H}) = \vect{y} ⊥ \vect{y}$, where <img align=top src="APLimg/yrarr.bmp"> $= \mathbf{ν}(\mat{H})$.

\par \textbf{1.31} If \mat{H} is homogeneous, $\mathbf{ν}(\mat{H}) =$ (3, 2, 3, 4), and $\vect{i} =$ (1, 0, 2), determine, in a 0-origin system
\begin{tabularx}
<td valign=top>(a) & & the left list index \textit{l}(\vect{i}).

<td valign=top>(b) & & the right list index \textit{r}(\vect{i}).

<td valign=top>(c) & & the index \vect{j} of the node whose left list index is 27.

\end{tabularx}

\par \textbf{1.32}
\begin{tabularx}
<td valign=top>(a)^{} & & If \mat{K} = \textbf{ι}^0(\textit{n}) ↓ (\textbf{ϵ}(\textit{n}) {\circ \atop \times} \textbf{ι}^0(\textit{n})), show that \mat{K} + 
<img src="APLimg/ktilde.bmp"> = \textit{n}(\mathsf{\mat{E} - \mat{I}).

<td valign=top>(b) & & If \vect{y} is any permutation of \vect{x} and \textit{ν}(\vect{x}) = \textit{n}, show that \vect{x} {+ \atop \times} \mat{K}
{+ \atop \times} \vect{x} = \vect{y} {+ \atop \times} \mat{K}
{+ \atop \times} \vect{y}.

\end{tabularx}

\par \textbf{1.33} Using the Euclidean algorithm, write programs to determine:
\begin{tabularx}
<td valign=top>(a) & & \textit{d} as the greatest common divisor of positive integers \textit{x} and \textit{y}.

<td valign=top>(b) & & \vect{d} as the g.c.d. of \vect{x} and \vect{y}, where \vect{d}, \vect{x}, and \vect{y} represent polynomials in \textit{z} (e.g., ((\textit{z}\textbf{ϵ}) ⊥ \vect{x}).

\end{tabularx}

\par \textbf{1.34} To assure uniqueness, the number of different digits (symbols) used in a base $b$ number system must not exceed $b$. The limitation to the particular range 0 $\leq \vect{a}_i < b$ is, however, not essential. For example, a base three system can be constructed using digits -1, 0, and 1, for which it is convenient to adopt the symbols -, 0, and +, respectively. The positive numbers beginning at zero are then represented by the sequence 0, +, $+$ -, +0, $+$ +, $+ -$ -, $+$ -0, $+ -$ +, $+$ 0 -, +00, etc. The negative numbers beginning at 0 are 0, -, $-$ +, -0, $-$ -, $- +$ +, $-$ +0, $- +$ -, -0 +, -00, etc.

\begin{tabularx}
<td valign=top>(a) & & Construct addition and multiplication tables for this number system and calculate the sum and the product of the numbers 0 - and - -. Use the decimal system to check all results.

<td valign=top>(b) & & Negative numbers are represented in this system without the attachment of a special sign position. Special rules regarding sing are therefore banished except that it is necesssary to formulate a rule for changing the sign of a number, i.e. to multiply by minus one. Formulate such a rule.

\end{tabularx}

\par \textbf{1.35} For any integer $n$, let $x_2 =$ 2 |_0 $n, x_3 =$ 3 |_0 $n, x_5 =$ 5 |_0 $n$, and $x_7 =$ 7 |_0 $n$. As shown by
<acronym title="Garner, Harvey L. (1959), “The Residue Number System”, IRE Transactions, vol. EC-8, pp. 140-147.">Garner (1959)</acronym>, the ordered array (\textit{x}_2, \textit{x}_3, \textit{x}_5, \textit{x}_7) provides a representation of the integer \textit{n} in a so-called \textit{residue} number system.

\begin{tabularx}
<td valign=top>(a) & & Write the residue representations of the first ten nonnegative integers.

<td valign=top>(b) & & For integers \textit{n} in the range 0 \leq \textit{n} < (2 \times 3 \times 5 \times 7) show:
\begin{tabularx}
<td valign=top>(1) & & that the representation is unique.

<td valign=top>(2) & & that an addition algorithm may be defined which treats the several columns independently, i.e., there are no carries. (The sums must also lie within the specified range.)

\end{tabularx}

<td valign=top>(c) & & Discuss the choice of moduli for extending the range of the representation.

<td valign=top>(d) & & Show that the algorithm derived in part (b) is valid for all positive and negative integers in the range -\textit{a}/2 \leq \textit{n} < \textit{a}/2 for \textit{a} = 2 \times 3 \times 5 \times 7.

<td valign=top>(e) & & Derive an algorithm for obtaining -\textit{n} from \textit{n}.

<td valign=top>(f) & & Derive an algorithm for multiplication.

<td valign=top>(g) & & The sign of the number (i.e., its relation to zero) is not displayed directly by this representation. Convince yourself that its determination is nontrivial.

\end{tabularx}

\par \textbf{1.36} Let $\vect{x}, \vect{y}$, and $\vect{z}$ be the positional representations of the numbers $x, y$, and $z$ respectively. Using the floor and residue operations, write programs to determine $\vect{z}$ as a function of $\vect{x}$ and $\vect{y}$, where $z = x + y$ and the representation in use is
\begin{tabularx}
<td valign=top>(a) & & base \textit{b}.

<td valign=top>(b) & & mixed base \textit{b}.

<td valign=top>(c) & & the +, -, 0 base three system (of Exercise 1.34).

<td valign=top>(d) & & the residue number system (of Exercise 1.35).

\end{tabularx}

\par \textbf{1.37} Write programs for the multiplication $z = x \times y$ for each of the cases of Exercise 1.36.

\par \textbf{1.38} Write programs to convert in each direction between the following pairs of number systems:
\begin{tabularx}
<td valign=top>(a) & & base \textit{b}_1 and \textit{b}_2.

<td valign=top>(b)^{} & & base \vect{b}^1 and \vect{b}^2.

<td valign=top>(c) & & base three and the +, -, 0 base three of Exercise 1.34. 

<td valign=top>(d) & & residue and base \textit{b} (Exercise 1.35).

\end{tabularx}

\par \textbf{1.39}
\begin{tabularx}
<td valign=top>(a)^{} & & Show that the superdiagonal matrices satisfy ^{\textit{j}}\mathsf{\mat{I}} 
{+ \atop \times}
^{\textit{k}}\mathsf{\mat{I}} =
^{(\textit{j}+\textit{k})}\mathsf{\mat{I}}.

<td valign=top>(b)^{} & & A matrix of the form \mat{J} = (\textit{x}\mathsf{\mat{I}} + 
^1\mathsf{\mat{I}}) is called a \textit{Jordan box}. Write the expansion of the \textit{n}th power of \mat{J}.

<td valign=top>(c)^{} & & Show that \mat{X} {+ \atop \times} \mat{Y} = \mat{X}_1 {\circ \atop \times} \mat{Y}^1 + \mat{X}_2 {\circ \atop \times} \mat{Y}^2 + ... + \mat{X}_{\textit{ν}(\mat{X})} {\circ \atop \times} \mat{Y}^{ \textit{ν}(\mat{X})}.

<td valign=top>(d) & & Determine an explicit solution to the set of linear equations \mat{A} {+ \atop \times} \vect{x} = \vect{y}, where \vect{u}/\vect{x} = \vect{a} and \vect{v}/\vect{y} = \vect{b} are known and where +/\vect{u} + +/\vect{v} = \textit{ν}(\mat{A}) = \textit{μ}(\mat{A}). State the conditions for the existence of a unique solution.

\end{tabularx}

\par \textbf{1.40} Any nonsingular matrix $\mat{A}$ can be reduced to the identity $\mathsf{\mat{I}}$ by a sequence of \textit{row operations} of the form $\mat{A}^i ← x\mat{A}^i + y\mat{A}^i$, or $\mat{A}^i ← \mat{A}^j$. The process which accomplishes this (using row operations only) by reducing successive column vectors to the successive unit vectors is called \textit{Jordan} or \textit{complete} elimination. If the same sequence of row operations is executed upon the identity matrix, it will be transformed to the matrix $\mat{B}$ such that $\mat{B}$ {+ $\atop \times} \mat{A} = \mathsf{\mat{I}}$. The inverse of $\mat{A}$ can therefore be obtained by performing Jordan elimination on the matrix $\mat{M} = \mat{A} \oplus$
\mathsf{\mat{I}} so as to reduce the first \textit{ν}(\mat{A}) columns to the identity. The last \textit{ν}(\mat{A}) columns are then the inverse of \mat{A}.
\begin{tabularx}
<td valign=top>(a) & & Write a program to determine the inverse of \mat{A} by Jordan elimination.

<td valign=top>(b)^{} & & The sequence of operations which reduce the \textit{i}th column of \mat{A} to \textbf{ϵ}^{\textit{i}} is called the \textit{i}th \textit{step} of the process, and the \textit{i}th diagonal element at the beginning of the \textit{i}th step is called the \textit{i}th pivot element. Modify the program of part (a) so that each step is preceded by a column permutation which yields the largest (in absolute value) pivot element possible. This modification tends to reduce the accumulation of round-off errors.

<td valign=top>(c) & & In the Jordan elimination of part (a), it is unnecessary to store the identity matrix explicitly, and, since the \textit{i}th column is first affected at the \textit{i}th step, only one new column need be brought in at each step. Moreover, the \textit{i}th column of \mat{A} may be discarded after its reduction to \textbf{ϵ}^{\textit{i}} on the \textit{i}th step, and it is therefore necessary to store only a square matrix at all times. Show that by shifting all columns to the left and by moving the rows upward cyclically, a very uniform process results, with the pivot element in the leading position at every step [<acronym title="verson, K.E. (1954), “Machine Solutions of Linear Differential Equations”, Doctoral Thesis, Harvard University.">Iverson (1954)</acronym> or
<acronym title="Rutishauser, H. (1959), “Zur Matrizeninversion nach Gauss-Jordan”, Zeitschrift für Angewandte Mathematik und Physik, vol. X, pp. 281-291.">Rutishauser (1959)</acronym>]. Write a program for the process.

<td valign=top>(d) & & Modify part (c) to allow the choice of pivot elements as in part (b). The effects of the permutation on the not explicitly recorded identity cannot be indicated directly, but the performance of the same set of permutations in reverse order upon the \textit{rows} of the resulting inverse produces the same result. Verify this and program the process.

\end{tabularx}

\par \textbf{1.41}
\begin{tabularx}
<td valign=top>(a) & & Show that a group
<acronym title="Jacobson, N. (1951), Lectures in Abstract Algebra, vol. 1, Van Nostrand, New York.">[Jacobson (1951)]</acronym> can be represented by a square matrix \mat{M} such that each row and each column is a permutation vector.

<td valign=top>(b) & & Show that \mat{M}^{\textit{i}} = \mat{M}_{\textit{i}} = \textbf{ι}^1 for some \textit{i}.

<td valign=top>(c) & & What are the necessary and sufficient conditions that the group represented by \mat{M} be Abelian?

<td valign=top>(d) & & What a program to determine all cyclic subgroups of a group represented by \mat{M}.

\end{tabularx}

\par \textbf{1.42} If $\mat{U}$ is a logical matrix whose rows are each nonzero, mutually disjoint, and collectively exhaustive (that is, $(+/\mat{U} \geq \textbf{ϵ}) = \textbf{ϵ}$, and $+/\!/\mat{U} = \textbf{ϵ})$, then $\mat{U}$ defines an \textit{m-way partition} of $n$, where $m = μ(\mat{U})$, and $n = ν(\mat{U})$. The partition is more commonly represented by the vector $p = +/\mat{U}$ [<acronym title="Riordan, J. (1958), An Introduction to Combinatorial Analysis, Wiley, New York.">Riordan (1958)</acronym>, p. 107]. Clearly $+/\vect{p} = n$. Write a program to generate

\begin{tabularx}
<td valign=top>(a) & & all partitions \mat{U} of a given integer \textit{n}.

<td valign=top>(b) & & all distinct partitions of \textit{n}, where \mat{U} and \mat{V} are considered equivalent if \vect{p} = +/\mat{U} is a permutation of \vect{q} = +/\mat{V}.

\end{tabularx}

\par \textbf{1.43} Let $= \vect{x}$ be a \textit{space} vector (i.e., of dimension three), and let $\mat{R}(\vect{x})$ be the square matrix $\textbf{ι} ↑ (\textbf{ϵ} {\circ \atop \wedge} \vect{x})$. Show that

\begin{tabularx}
<td valign=top>(a) & & +/\mat{R}(\vect{x} \times \vect{y}) = (\vect{x} {+ \atop \times} \vect{y}) \times \textbf{ϵ}

<td valign=top>(b) & & \textbf{ϵ} {+ \atop \times} (\vect{x} \times \vect{y}) = \vect{x} {+ \atop \times} \vect{y}

<td valign=top>(c) & & (+/\mat{R}(\vect{x} \times \vect{y}))
{+ \atop \times} (\vect{w} \times \vect{z}) = (\vect{x} {+ \atop \times} \vect{y}) \times (\vect{w} {+ \atop \times} \vect{z})

<td valign=top>(d) & & (\vect{x} {+ \atop \times} \vect{y}) \times (\vect{x} {+ \atop \times} \vect{y}) = (\vect{x} \times \vect{y}) + (\vect{x} \times \vect{y}) + 2(↓ \vect{x} \times ↑ \vect{y})
{+ \atop \times} (↓ \vect{x} \times ↑ \vect{y}).

\end{tabularx}

\par \textbf{1.44} Let $\vect{x} \cdot \vect{y} = (↑ \vect{x} \times ↓ \vect{y}) - (↓ \vect{x} \times ↑ \vect{y})$ be the \textit{vector product} of $\vect{x}$ and $\vect{y}$ for vectors of dimension three. Show that

\begin{tabularx}
<td valign=top>(a) & & this agrees with the usual definition
<acronym title="Margenau, H., and G.M. Murphy (1943), The Mathematics of Physics and Chemistry, Van Nostrand, New York.">[Margenau and Murphy (1943)]</acronym>.

<td valign=top>(b) & & \vect{x} \cdot \vect{y} = -(\vect{y} \cdot \vect{x})

<td valign=top>(c) & & \vect{x} \cdot \vect{y} is perpendicular to \vect{x}, that is, \vect{x} 
{+ \atop \times} (\vect{x} \cdot \vect{y}) = 0. (Use the fact that ↓ \vect{x} = 2↑\vect{x} for a vector of dimension three.)

\end{tabularx}

\par \textbf{1.45} Let $[\vect{x}] =$ 
<img align=middle src="APLimg/ex1x45a.bmp"> be the \textit{length of} \vect{x}, and let \vect{x} γ \vect{y} = 
<img align=middle src="APLimg/ex1x45b.bmp"> be the cosine of the angle between \vect{x} and \vect{y}, and let \vect{x} \textit{σ} \vect{y} = <img align=middle src="APLimg/ex1x45c.bmp"> be the sine of the angle. Use the results of Exercises 1.43 and 1.44 to show that for space vectors

\begin{tabularx}
<td valign=top>(a) & & [\vect{x} \cdot \vect{y}] = [\vect{x}] \times [\vect{y}] \times (\vect{x} \textit{σ} \vect{y}). Note that [\vect{x} \cdot \vect{y}] is the area enclosed by the parallelogram defined by \vect{x} and \vect{y}.

<td valign=top>(b) & & (\vect{x} \cdot \vect{y}) \cdot \vect{z} = (\vect{x} {+ \atop \times} \vect{z}) \times \vect{y} - (\vect{y} {+ \atop \times} \vect{z}) \times \vect{x}

<td valign=top>(c) & & (\vect{x} \cdot \vect{y}) {+ \atop \times} \vect{z} = \vect{x} {+ \atop \times} (\vect{y} \cdot \vect{z}).

\end{tabularx}

<!------------------------------------------------------------------------------- ------------------------------------------------------------------------------->

<a name="3"></a>
\par $\large{Chapter$ 3 Representation of Variables}

<a name="3.1"></a>
\par \textbf{3.1 Allocation and encoding}

\par Although the abstract description of a program may be presented in any suitable language, its automatic execution must be performed on some specified representation of the relevant operands. The specification of this representation presents two distinct aspects---allocation and encoding.

\par An \textit{allocation} specifies the correspondences between physical devices and the variables represented thereby. An \textit{encoding} specifies the correspondences between the distinct states of the physical devices and the literals which they represent. If, for example, certain numerical data are to be represented by a set of 50 two-state devices, the two-out-of-five coding system of Exercise 1.6 might be chosen, and it would then remain to specify the allocation. The two-digit quantity ``hours worked'' might be allocated as follows: devices 31-35 represent components 1-5, respectively, of the first digit, and devices 29, 16, 17, 24, and 47 represent components 1, 2, 3, 4, 5, respectively, of the second digit.

\par The encoding of a variable will be specified by an \textit{encoding matrix} $\mat{C}$ and associated \textit{format vector} $\vect{f}$ such that the rows of $\overbar{\vect{f}}/\mat{C}$ list the representands and the rows of $\vect{f}/\mat{C}$ list the corresponding representations. The encoding is normally fixed and normally concerns the programmer only in the translation of input or output data. Even this translation is usually handled in a routine manner, and attention will therefore be restricted primarily to the problem of allocation.

\par However, the encoding of numeric quantities warrants special comment. It includes the representation of the sign and of the scale, as well as the representation of the significant digits. Small numbers, such as indices, admit not only of the usual positional representation but also of the use of the unit vector $\textbf{ϵ}^j$ to represent the number $j$ (i.e., a $one-out-of-n$ coding system), or of the use of a logical vector of weight $j$ (i.e., a base 1 number system).

\par Allocation will be described in terms of the \textit{physical vector} $\mathbf{π}$, which denotes the physical storage elements of the computer. Each component of $\mathbf{π}$ corresponds to one of the $ν(\mathbf{π})$ similar physical devices available, its range of values is the set of physical states achievable by each device, and its index is the address of the device. Each component of $\mathbf{π}$ may correspond to a computer register, an individual character position in a register, or an individual binary digit within a character, depending on the degree of resolution appropriate to the allocation problem considered. The 0-origin indexing normally used for computer addresses will be used for the physical vector, but 1-origin indexing will, throughout this chapter, normally be employed for all other structured operands.

\par An index of the physical vector will be called an \textit{address} and will itself be represented in the (perhaps mixed) radix appropriate to the given computer. The Univac, for example, employs base ten addressing for the registers, and (because of the use of 12-character words) a radix of twelve for finer resolution. The address of the fourth character of register 675 might therefore be written as 675.3. In computers which have two or more independent addressing systems (e.g., the independent addressing systems for main memory and for auxiliary storage in the IBM 705), superscripts may be used to identify the several physical vectors $\mathbf{π}^j$.

\par In general, the \textit{representation} of a quantity $x$ is a vector (to be denoted by $\mathbf{ρ}(x))$ whose components are chosen from the physical vector $\mathbf{π}$. Thus $\mathbf{ρ}(x) = \vect{k}\int\mathbf{π}$, where $\vect{k}$ is a mapping vector associated with $x$. The dimension of the representation (that is, $ν(\mathbf{ρ}(x)))$ is called the \textit{dimension of} $x in \mathbf{π}$. If, for example, $\mathbf{ρ}(x) = (\mathbf{π}_{10}, \mathbf{π}_9, \mathbf{π}_{17}, \mathbf{π}_{18})$, then $\vect{k} =$ (10, 9, 17, 18), and the dimension of $x$ in $\mathbf{π}$ is four. If $\mathbf{ρ}(x)$ is an infix of $\mathbf{π}$, then the representation of $x$ is said to be \textit{solid}. A solid representation can be characterized by two parameters, its dimension $d$ and its \textit{leading address} $f$, that is, the index in $\mathbf{π}$ of its first component. Then $\mathbf{ρ}(x) = (f ↓ \mathbf{α}^d)/\mathbf{π}$.

<a name="3.2"></a>
\par \textbf{3.2 Representation of structured operands}

<a name="3.2.1"></a>
\par \textbf{The grid matrix}

\par If each component of a vector $\vect{x}$ has a solid representation, then the representation of the entire vector is said to be solid and may be characterized by the \textit{grid matrix} $\tree{Γ}(\vect{x})$, of dimension $ν(\vect{x}) \times$ 2, defined as follows: $\tree{Γ}_1^i(\vect{x})$ is the leading address of $\mathbf{ρ}(\vect{x}_i)$, and $\tree{Γ}_2^i(\vect{x})$ is the dimension of $\vect{x}_i$ in $\mathbf{π}$. If, for example, the vector $\vect{x}$ is represented as shown in Fig. $3.1a$, then

\begin{tabularx}
<td rowspan=5> & <td rowspan=5> \tree{Γ}(\vect{x}) = & 
<td rowspan=5><img src="APLimg/matrixl5.bmp"> & 
17 2 & 
<td rowspan=5><img src="APLimg/matrixr5.bmp"> & 
<td rowspan=5>. & \\
19 4 & \\
27 5 & \\
23 1 & \\
32 3 & \\
\end{tabularx}

\par Any structured operand can first be reduced to an equivalent vector, and the grid matrix therefore suffices for describing the representation of any construct, providing only that the representation of each of its elements is solid. Thus a matrix $\mat{X}$ may be represented by either the row-by-row list $\vect{r} = \mat{E}/\mat{X}$ or the column-by-column list $\vect{c} = \mat{E}/\!/\mat{X}$, and a tree \mat{T} may be represented by the left list matrix [\mat{T} or the right list matrix ]\mat{T}, either of which may be represented, in turn, by a vector.

\par If a process involves only a small number of variables, it is practical to make their allocation implicit in the algorithm, i.e., to incorporate in the algorithm the selection operations on the vector $\mathbf{π}$ necessary to extract the appropriate variables. This is the procedure usually employed, for example, in simple computer programs. In processes involving numerous variables, implicit allocation may become too cumbersome and confusing, and more systematic procedures are needed.

<img src="APLimg/fig3x1.bmp">
\par \textbf{Figure 3.1}

<img src="APLimg/fig3x2.bmp">
\par \textbf{Figure 3.2} Linear Representation of a matrix $\mat{X}$

<a name="3.2.2"></a>
\par \textbf{Linear representations}

\par The representation of a structured operand is said to be \textit{linear} if each component is represented by an infix of the form $(l ↓ \mathbf{α}^d)/\mathbf{π}$, where $l$ is a linear function of the indices of the component. For example, the representation of the matrix $\mat{X}$ indicated by Fig. 3.2 is linear, with $d =$ 2 and $l =$ -11 $+ 5i + 8j$.

\par A linear representation is solid and can clearly be characterized by a small number of parameters---the dimension $d$ of each component and the coefficients in the linear expression $l$. The representation of a vector $\vect{x}$ is linear if and only if $\tree{Γ}_2(\vect{x}) = d\textbf{ϵ}$ and the difference $δ = \tree{Γ}_1^i(\vect{x}) - \tree{Γ}_1^{i-1}(\vect{x})$ is constant for $i =$ 2, 3, ... $, ν(\vect{x})$.

\par If $l = p + qi + rj$ is the function defining a linear representation of a matrix $\vect{x}$ and if $a$ is the leading address of a given element, then the leading address of the succeeding element in the row (or column) is simply $a + r$ (or $a + q)$. Frequently, the succession must be cyclic, and the resulting sum must be reduced modulo $ν(\vect{x}) \times r$ (or $μ(\vect{x}) \times q)$. The inherent convenience of linear representations is further enhanced by index registers, which provide efficient incrementation and comparison of addresses.

\par Linear representation of a structured operand requires that all components be of the same dimension in $\mathbf{π}$. This common dimension may, however, be achieved by appending null elements to the shorter components. The convenience of the linear representation must then be weighed against the waste occasioned by the null elements. Moreover, if several vectors or matrices are to be represented and if each is of unspecified total dimension in $\mathbf{π}$, it may be impossible to allot to each an infix sufficiently large to permit linear representation. Consequently, a linear representation is not always practicable.

<a name="3.2.3"></a>
\par \textbf{Nonlinear representations}

\par Since the use of the grid matrix imposes only the condition of solidity for each component, it permits an allocation which is sufficiently general for most purposes. The grid matrix serves in two distinct capacities: (1) as a useful conceptual device for describing an allocation even when the actual allocation is implicit in the program, and (2) as a parameter which enters directly into an algorithm and explicitly specifies the allocation.

\par If the grid matrix is used in a program as an explicit specification of the allocation, then the grid matrix must itself be represented by the physical vector. There remains, therefore, the problem of choosing a suitable allocation for the grid matrix itself; a linear allocation is illustrated by Fig.3.lb.

\par If the grid matrix $\tree{Γ}(\vect{x})$ itself employs a linear representation, its use offers advantages over the direct use of a linear representation of $\vect{x}$ only if the total dimension of $\tree{Γ}$ in $\mathbf{π}$ is much less than the total dimension of $\vect{x}$ in $\mathbf{π}$ when linear representations are employed for both. This is frequently the case, since each element of a grid matrix belongs to the index set of $\mathbf{π}$ (that is, to $\textbf{ι}^0(ν(\mathbf{π})))$, and the dimension of each element in $\mathbf{π}$ is therefore both uniform and relatively small. Program 3.3 shows the use of the grid matrix $\tree{Γ}(\vect{x})$ and the encoding matrix $\mat{C}$ in determining the $k$th component of the vector $\vect{x}$.

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
<td nowrap>^{}\textit{l} ← \textit{p} + \textit{qk} + \textit{r} \times 1 _{} & \\
<td nowrap>^{}\textit{f} ← \vect{b} ⊥ ((\textit{l} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) _{} & \\
^{}\textit{l} ← \textit{l} + \textit{r} _{} & \\
^{}\textit{d} ← \vect{b} ⊥ ((\textit{l} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π} _{} & \\
^{}\vect{z} ← (\textit{ f} ↓ \mathbf{α}^{\textit{d}})/\mathbf{π} _{} & \\
^{}\textit{h} ← \textit{μ}(\mat{C}) + 1_{} & \\
^{}\textit{h} ← \textit{h} - 1 _{} & \\
^{}\vect{z} : \vect{f}/\mat{C}^{\textit{ h}} _{} & \\
^{}\vect{z} ← \overbar{\vect{f}}/\mat{C}^{\textit{ h}}_{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}
<img src="APLimg/prog3x3.bmp"> \\

<td nowrap> & 
<table border=1 cellspacing=0 cellpadding=0>
<td colspan=2 align=center>0-origin for \mathbf{π} only & \\
\begin{tabularx}
<td valign=top nowrap> \textit{p, q, r} & 
Constant, coefficient of row index, and coefficient of column index in the linear function for the representation of \tree{Γ}(\vect{x}). & \\
<td valign=top> \vect{b} & 
Based used in representing elements of \tree{Γ}(\vect{x}). & \\
<td valign=top> \textit{g} & 
Dimension of \mathbf{π} of each element of \tree{Γ}(\vect{x}). & \\
<td valign=top> \textit{f} & 
Leading address of \mathbf{ρ}(\vect{x}_{\textit{k}}). & \\
<td valign=top> \textit{d} & 
Dimension of \mathbf{ρ}(\vect{x}_{\textit{k}}) in \mathbf{π}. & \\
<td valign=top> \vect{z} & 
\mathbf{ρ}(\vect{x}_{\textit{k}}). & \\
<td valign=top> \mat{C} & 
Encoding matrix for components of \vect{x}. & \\
<td valign=top> \vect{f} & 
Format vector for \mat{C}. & \\
 \textit{z} & 
Character encoded by \vect{x}_{\textit{k}}. & \\
\end{tabularx} & \\
\end{tabularx}
\par \textbf{Legend} $&$ 
\\\end{tabularx}

\par \textbf{Program 3.3} Determination of $\vect{z} = \mathbf{ρ}(\vect{x}_k)$ and $z = \vect{x}_k\\$
 from a linear representation of the grid matrix \tree{Γ}(\vect{x})

\begin{tabularx} & 
\par \textbf{Program 3.3}. A linear representation is assumed for $\tree{Γ}(\vect{x})$, with element $\tree{Γ}_j^i(\vect{x})$ represented by the infix $((p + qi + rj) ↓ \mathbf{α}^g)/\mathbf{π}$. Moreover, each element of $\tree{Γ}(\vect{x})$ is assumed to be represented in a base $\vect{b}$ number system. Step 1 determines the leading address of the representation of $\tree{Γ}_1^k(\vect{x})$. Step 2 specifies $f$ as the base $\vect{b}$ value of this representation, i.e., as the leading address of $\mathbf{ρ}(\vect{x}_k)$. Steps 3 and 4 specify $d$ as the dimension of $\vect{x}_k$ in $\mathbf{π}$, and step 5 therefore specifies $\vect{z}$ as the representation of $\vect{x}_k$.

\par Steps 7-9 perform the decoding of $\vect{z} = \mathbf{ρ}(\vect{x}_k)$ to obtain $\vect{z}$ as the actual value of $\vect{x}_k$. Since this process is normally performed by human or mechanical means (e.g., a printer) outside the purview of the programmer, it is here expressed directly in terms of the encoding matrix $\mat{C}$ rather than in terms of its representation. The left-pointing exit on step 7 is followed only if $\vect{z}$ does not occur as an entry in the encoding matrix.
 & & \\\end{tabularx}

\par The form chosen for the grid matrix is one of several possible. The two columns could, for example, represent the leading and final addresses of the corresponding representations or the dimensions and final addresses. The present choice of leading address $f$ and dimension $d$ is, however, the most convenient for use in conjunction with the notation adopted for infixes; the logical vector $(f ↓ \mathbf{α}^d)$ selects the appropriate infix.

<a name="3.2.4"></a>
\par \textbf{Chained representations}^{<a href="#note3a">[a]</a>}

\par If a linear representation is used for a vector, then the deletion of a component (as in a compress operation) necessitates the moving (i.e., respecification) of the representations of each of the subsequent components. Similarly, mesh operations (insertion) and permutations necessitate extensive respecification. The use of a grid matrix $\tree{Γ}(\vect{x})$ obviates such respecification in $\vect{x}$, since appropriate changes can instead be made in $\tree{Γ}(\vect{x})$, where they may be much simpler to effect. If, for example, $\vect{x}$ is the vector represented as in Fig. 3.1a, and $z$ is a quantity of dimension six in $\mathbf{π}$, then the mesh operation

\begin{tabularx}<td width=75> & 
\vect{x} ← \backslash\vect{x}, \textbf{ϵ}^3, \textit{z}\backslash & 
\\\end{tabularx}

\par may be effected by specifying the physical infix (70 $↓ \mathbf{α}^6)/\mathbf{π}$ by $\mathbf{ρ}(z)$ and by respecifying $\tree{Γ}(\vect{x})$ as follows:

\begin{tabularx}<td rowspan=6> & 
<td rowspan=6>\tree{Γ}(\vect{x}) = & 
<td rowspan=6><img src="APLimg/matrixl6.bmp"> & 
17 2 & 
<td rowspan=6><img src="APLimg/matrixr6.bmp"> & 
<td rowspan=6>. & \\
19 4 & \\
70 6 & \\
27 5 & \\
23 1 & \\
32 3 & \\
\end{tabularx}

\par However, if the representation of $\tree{Γ}(\vect{x})$ is itself linear, then insertions, deletions, and permutations in $\vect{x}$ will occasion changes in all components of $\tree{Γ}(\vect{x})$ whose indices are affected. The need for a linear representation of the grid matrix (and hence for all linear representations) can be obviated by the use of a chained representation defined as follows.

\par Consider a vector $\vect{y}$, each of whose components $\vect{y}_k$ has a solid representation $\mathbf{ρ}(\vect{y}_k)$ whose infixes $(g ↓ \mathbf{α}^g)/\mathbf{ρ}(\vect{y}_k)$ and $\mathbf{α}^g/\mathbf{ρ}(\vect{y}_k)$ are, respectively, the dimension of $\mathbf{ρ}(\vect{y}_k)$ in $\mathbf{π}$ and the leading address of the representation of the (cyclically) succeeding component of $\vect{y}$ (both in a base $\vect{b}$ system), and whose suffix 
\overbar{\vect{α}}^{ 2\textit{g}}/\mathbf{ρ}(\vect{y}_{\textit{k}}) is the representation of the \textit{k}th component of some vector \vect{x}. Then (the representation of) \vect{y} is called a \textit{chained representation of} \vect{x}. In other words, the representation of \vect{y} incorporates its own grid matrix (with the address column \tree{Γ}_1(\vect{y}) rotated upward by one place) as well as the representation of the vector \vect{x}.

\par For example, if $g =$ 2, $\vect{b} = 10\textbf{ϵ}$, and $\vect{x} =$ (365, 7, 24), then

\begin{tabularx}
<td width=75> & 
\mathbf{ρ}(\vect{y}_1) = (\mathbf{π}_{17}, \mathbf{π}_{18}, \mathbf{π}_{19}, \mathbf{π}_{20}, \mathbf{π}_{21}, \mathbf{π}_{22}, \mathbf{π}_{23}) = (6, 8, 0, 7, 3, 6, 5), & \\

\mathbf{ρ}(\vect{y}_2) = (\mathbf{π}_{68}, \mathbf{π}_{69}, \mathbf{π}_{70}, \mathbf{π}_{71}, \mathbf{π}_{72}) = (2, 6, 0, 5, 7), & \\
and & \\

\mathbf{ρ}(\vect{y}_3) = (\mathbf{π}_{26}, \mathbf{π}_{27}, \mathbf{π}_{28}, \mathbf{π}_{29}, \mathbf{π}_{30}, \mathbf{π}_{31}) = (1, 7, 0, 6, 2, 4), & \\
\end{tabularx}

\par is a suitable chained representation of $\vect{x}$.

\par The parameters required in executing an algorithm on a chained representation $\vect{y}$ are $g$, the common dimension in $\mathbf{π}$ of the elements of the grid matrix $\tree{Γ}_1(\vect{y}); \vect{b}$, the base of the number system employed in their representation; and $f$ and $h$, the leading address and index, respectively, of the representation of some one component of $\vect{y}$. The parameters $g$ and $\vect{b}$ are usually common to the entire set of chained representations in use. Program 3.4 illustrates the type of algorithm required to determine $\mathbf{ρ}(\vect{x}_k)$ from a given chained representation of $\vect{x}$.

<!--
<table border=1 cellspacing=0 cellpadding=0>\begin{tabularx}
^{} \textit{h} ← \textit{ν}(\vect{x}) |_1(\textit{h} + 1) _{} & \\
^{} \textit{f} ← \vect{b} ⊥ (( \textit{f} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) _{} & \\
^{} \textit{h} : \textit{k} _{} & \\
<td nowrap>^{} \textit{d} ← \vect{b} ⊥ (( \textit{f} + \textit{g}) ↓ \mathbf{α}^{\textit{g}})/\mathbf{π} _{} & \\
^{} \vect{z} ← ( \textit{f} ↓ \mathbf{α}^{\textit{d}})/\mathbf{π} _{} & \\
^{} \mathbf{ρ}(\vect{x}_{\textit{k}}) ← \overbar{\vect{α}}^{ 2\textit{g}}/\mathbf{π} _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}<img src="APLimg/prog3x4.bmp"> \\

<td nowrap> & 
<table border=1 cellspacing=0 cellpadding=0>
<td align=center>0-origin indexing for \mathbf{π} only & \\
\begin{tabularx}
<td valign=top nowrap> \textit{h}, \textit{f} & \textit{f} is the leading address of the \textit{h}th component of the chained representation of \vect{x}. & \\
<td valign=top> \textit{b} & Base used for representation of the elements of the grid matrix. & \\
<td valign=top> \textit{g} & Dimension in \mathbf{π} of elements of the grid matrix. & \\
<td valign=top> \textit{d} & Dimension in \mathbf{π} of \textit{k}th component of the chained representation of \vect{x}. & \\
<td valign=top> \textit{z} & \textit{k}th component of the chained representation of \vect{x}. & \\
\end{tabularx} & \\
\end{tabularx}
\par \textbf{Legend}

\end{tabularx}

\par \textbf{Program 3.4} Determination of $\mathbf{ρ}(\vect{x}_k)$ from a chained representation of $\vect{x}$

\begin{tabularx} & 
\par \textbf{Program 3.4.} The loop (1-3) is executed $ν(\vect{x})|_0(k - h)$ times, with the result that at step 4 the parameter $f$ is the leading address of $\mathbf{ρ}(\vect{y}_k)$. Step 4 therefore specifies $d$ as the dimension of $\mathbf{ρ}(\vect{x}_k)$, that is, as the base $\vect{b}$ value of $\tree{Γ}_2^k(\vect{y})$. Step 5 then specifies $\vect{z}$ as $\mathbf{ρ}(\vect{y}_k)$. Step 6 deletes those components of $\vect{z}$ which represent the elements of the grid matrix, leaving $\mathbf{ρ}(\vect{x}_k)$.

\par The parameters $f$ and $h$ are themselves respecified in the execution of the algorithm so that $h$ becomes $k$ and $f$ becomes, appropriately, the leading address of $\mathbf{ρ}(\vect{y}_k)$. A subsequent execution then begins from this new initial condition.
 & & \\\end{tabularx}

\par The chained representation used thus far is cyclic and contains no internal identification of the first or the last components. Such an identification can be incorporated by adding a null component between the last and first components of $\vect{x}$. Alternatively the identification may be achieved without augmenting the dimension but by sacrificing the end-around chaining, i.e., by replacing the last component of $↑\tree{Γ}_1(\vect{y})$ by a null element. Moreover, a chained representation may be entered (i.e., the scan may be begun) at anyone of several points, provided only that the index $h$ and corresponding leading address $f$ are known for each of the points.

\par The number of components of a chained representation scanned (steps 1-3 of Program 3.4) in selecting the $k$th component of $\vect{x}$ is given by $ν(\vect{x})|_0(k - h)$, where $h$ is the index of the component last selected. The selection operation is therefore most efficient when the components are selected in ascending order on the index. The chaining is effective in the forward direction only, and the component $(h -$ 1) would be obtained only by a complete cyclic forward scan of $ν(\vect{x}) -$ 1 components. The representation is therefore called a \textit{forward chain}. A \textit{backward chain} can be formed by incorporating the vector $↓\tree{Γ}_1(\vect{y})$ instead of $↑\tree{Γ}_1(\vect{y})$, and a double chain results from incorporating both.

\par A vector $\vect{x}$ which is respecified only by either deleting the final component or by adding a new final component (i.e., by operations of the form $\vect{x} ← \overbar{\vect{\omega}}^1/\vect{x}$, or $\vect{x} ← \vect{x} \oplus (z))$ behaves as a stack (cf. Exercise 2.6). A backward-chained representation is clearly convenient for such a stack.

\par A simple example of the use of a chained stack occurs in representing the available (i.e., unused) segments of the physical vector $\mathbf{π}$. This will be illustrated by a program for the vector compression

\par $\vect{x} ← \vect{v}/\vect{x}$

\par executed on a forward-chained representation of $\vect{x}$. The unused segments representing the components of $\overbar{\vect{v}}/\vect{x}$ are returned to a backward-chained stack or \textit{pool} of available components. A linear representation can usually be used for logical control vectors such as $\vect{v}$; in any case the problems involved in their representation are relatively trivial and will be subordinated by expressing each operation directly in terms of the logical vectors and not in terms of the physical components representing them.

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
 1^{} \textit{x} : ∘ _{} & \\
 2^{} \textit{i} ← \textit{x} _{} & \\
 3^{} \textit{t} ← \textit{x} _{} & \\
 4^{} \textit{x} ← ∘ _{} & \\
 5^{} \textit{k} ← 1 _{} & \\
 6^{} \vect{b} ⊥ ((\textit{r} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) ← \textit{s} _{} & \\
 7^{} \textit{k} ← \textit{k} + 1_{} & \\
 8^{} \textit{i} ← \textit{j} _{} & \\
 9^{} \textit{i} : \textit{t} _{} & \\
10^{} \textit{x} : ∘ _{} & \\
<td nowrap>11^{} \vect{b} ⊥ ((\textit{h} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) ← \textit{x} _{} & \\
12^{} \textit{j} ← \vect{b} ⊥ ((\textit{i} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) _{} & \\
13^{} \vect{v}_{\textit{k}} : 0 _{} & \\
14^{} \textit{x} : ∘ _{} & \\
15^{} \textit{x} ← \textit{i} _{} & \\
16^{} \textit{h} ← \textit{i}_{} & \\
17^{} \vect{v}_{\textit{k}-1} : 0 _{} & \\
18^{} \textit{r} ← \textit{h} _{} & \\
19^{} \textit{s} ← \textit{i} _{} & \\
20^{} \textit{h} ← \textit{i} _{} & \\
21^{} \textit{r} ← \textit{i} _{} & \\
22^{} \textit{s} ← \textit{p} _{} & \\
23^{} \textit{p} ← \textit{i} _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}
<img src="APLimg/prog3x5.bmp"> & & <td valign=top>
<table border=1 cellspacing=0 cellpadding=0>
<td align=center nowrap>0-origin indexing for \mathbf{π} only & \\
\begin{tabularx}
<td valign=top> \textit{x} & Leading address of \mathbf{ρ}(\vect{x}_1) if \textit{ν}(\vect{x}) > 0; otherwise \textit{x} = ∘ & \\
<td valign=top nowrap> \vect{v} & Logical vector. & \\
<td valign=top nowrap> \textit{k} & Index of \vect{v}. & \\
<td valign=top> \textit{i} & Leading address of \mathbf{ρ}(\vect{x}_{\textit{k}}) & \\
<td valign=top> \textit{j} & Leading address of \mathbf{ρ}(\vect{x}_{\textit{k}+1}) & \\
<td valign=top> \textit{h} & Leading address of last preceding component of \vect{v}/\vect{x}. & \\
<td valign=top> \textit{p} & Leading address of last preceding component of pool of available segments. & \\
<td valign=top> \textit{g} & Dimension in \mathbf{π} of elements of grid matrices. & \\
<td valign=top> \vect{b} & Base of representation of elements of grid matrices. & \\
\end{tabularx} & \\
\end{tabularx}
\par \textbf{Legend} $& \\$

\end{tabularx}

\par \textbf{Program 3.5} Program for $\vect{x} ← \vect{v}/\vect{x}$ on a forward chained representation of $\vect{x}\\$
 and a backward chained stack of available segments

\begin{tabularx} & 
\par \textbf{Program 3.5}. In the major loop (6-23), $k$ determines the index of the current component $\vect{v}_k$, and $i$ and $j$ determine the leading addresses of $\mathbf{ρ}(\vect{x}_k)$ and $\mathbf{ρ}(\vect{x}_{k+1})$, respectively. These three parameters are cycled through successive values by steps 7, 8, and 12 and are initialized by steps 2,5, and 12. If $\vect{v}_k =$ 0, the infix $\mathbf{ρ}(\vect{x}_k)$ is returned to the pool by steps 21, 22, 23, and 6 so as to construct a backward chain.

\par The parameter $x$ specifies the leading address of $\mathbf{ρ}(\vect{x}_1)$ unless $ν(\vect{x}) =$ 0, in which case $x$ is null. Step 1 terminates the process if $ν(\vect{x}) =$ 0, and otherwise step 4 respecifies $x$ as the null element. If $\vect{v} =$ 0, this null value of $x$ remains; if not, the first nonzero component of $\vect{v}$ causes a branch to step 14. Since $x = ∘$, step 15 is executed to respecify $x$ as the leading address of $\mathbf{ρ}((\vect{v}/\vect{x})_1)$. Step 16 then specifies $h$, the leading address of the last completed component of $\vect{v}/\vect{x}$. Step 15 is never again executed.

\par Components of $\vect{v}/\vect{x}$ other than the first must each be chained (in a forward chain) to the preceding one. Hence the leading address $i$ of a newly added component must be inserted in the last preceding component (whose leading address is $h)$. This is normally done by steps 18, 19, and 6; step 20 respecifies $h$. If, however, the component $\vect{x}_{k-1}$ were also included, it would appear as the last completed component of $\vect{v}/\vect{x}$ and would already be chained to the new component $\vect{x}_k$. This situation is recognized by step 17 and occasions a branch to step 16. Step 16 then respecifies $h$ and repeats the loop without executing steps 18, 19, and 6.

\par The process terminates when the cycle through the chained representation of $\vect{x}$ is completed, that is, when $i$ returns to the original value of $x$, preserved as $t$ by step 3. Step 10 is then executed, terminating the process directly if $ν(\vect{v}/\vect{x}) =$ 0. Otherwise, step 11 is executed to close the chain of $\vect{v}/\vect{x}$, that is, to insert $x$, the leading address of $\mathbf{ρ}((\vect{v}/\vect{x})_1)$, in the representation of the last component of $\vect{v}/\vect{x}$.
 & & \\\end{tabularx}

\par A chained representation can be generalized to allow the direct representation of more complex constructs, such as trees, by incorporating the address of each of the successor components associated with a given component. This notion is formalized in the chain list matrix of Sec. 3.4. The same scheme can also be employed to produce an efficient combined representation of two or more vectors which share certain common components. If, for example, $\vect{x}_j = \vect{x}_k$, and chained representations are used for both $\vect{x}$ and $\vect{z}$, then $\vect{x}$ may be represented in standard form except that component $\vect{x}_j$ incorporates a secondary address, which is the leading address of $\vect{z}_{k+1}$. Moreover $\vect{z}$ has a standard representation except that $\vect{z}_{k-1}$ is chained to $\vect{x}_j$ with an indicator to show that the secondary address of the succeeding component is to be used. Deletion of any vector component in such a shared system must occasion only the corresponding change in the address chain of the vector, the actual representation of the component being deleted only when no associated address remains.

<a name="3.2.5"></a>
\par \textbf{Partitions}

\par If the set $\vect{a}$ is the range of the components of the physical vector $\mathbf{π}$, and if some element, say $\vect{a}_1$ is reserved as a \textit{partition symbol} and is excluded from use in the normal representation of quantities, it can be inserted to demark the end (or beginning) of an infix of $\mathbf{π}$. If the vector $\vect{y}$ is represented by a single infix of $\mathbf{π}$ such that the beginning of component $\vect{y}_{j+1}$ follows immediately after the terminal partition of $\vect{y}_j$, then the structure of $\vect{y}$ is completely represented by the partitions, and $\vect{y}$ is called a \textit{partitioned representation}. A partitioned representation can be used for more complex operands, such as matrices, if a set of two or more distinct partition symbols are provided, one for each level of structure. The distinct partition symbols can, of course, be represented by multiple occurrences of a single symbol $\vect{a}_1$ rather than by distinct members of $\vect{a}$.

\par A partitioned representation is similar to a double-chained representation without end-around chaining in the following particular: beginning from component $\vect{y}_i$, the component $\vect{y}_j$ can be reached only by scanning all intervening components between $i$ and $j$ in increasing or decreasing order according as $i < j$ or $i > j$. The file notation introduced in Sec. 1.22 clearly provides the operations appropriate to a partitioned representation of a vector, with conventions which suppress all inessential references to the partitions themselves.

\par The use of a partition to demark the end of an infix is particularly convenient when the infix must be processed component by component for other reasons, as in the use of magnetic tape or other serial storage. The partition also appears to be more economical than the grid matrix, which it replaces. This apparent economy is, however, somewhat illusory, since the reservation of a special partition symbol reduces the information content of each nonpartition component by the factor $log_2(ν(\vect{a}) -$ 1) $÷$ log_2 $ν(\vect{a})$, where $\vect{a}$ is the range of the components of $\mathbf{π}$.

\par Partitions can be employed in chained representations. For example, the dimension in $\mathbf{π}$ of each component of a chained representation $\vect{y}$ can be specified implicitly by terminal partitions instead of explicitly by the vector $\tree{Γ}_2(\vect{y})$ of the grid matrix. Thus if the elements of $\tree{Γ}_1(\vect{y})$ are of dimension $g$ in $\mathbf{π}$, then $\textbf{ω}^1/\mathbf{ρ}(\vect{y}_j) = \vect{a}_1$, and $(\overbar{\vect{α}}^{$ g} $\wedge$
\overbar{\vect{\omega}}^1)/\mathbf{ρ}(\vect{y}_{\textit{j}}) = \mathbf{ρ}(\vect{x}_{\textit{j}}), where \vect{x} is the vector represented by \vect{y}. Program 3.6 shows the determination of \mathbf{ρ}(\vect{x}_{\textit{k}}) from a chained representation \vect{y} with terminal partitions \vect{a}_1.

<!--
<table border=1 cellspacing=0 cellpadding=0>\begin{tabularx}
1^{}_{} & ^{} \textit{h} ← \textit{ν}(\vect{x}) |_1 (\textit{h} + 1) _{} & \\
2^{}_{} & <td nowrap>^{} \textit{f} ← \vect{b} ⊥ ((\textit{f} ↓ \mathbf{α}^{\textit{g}})/\mathbf{π}) _{} & \\
3^{}_{} & ^{} \textit{h} : \textit{k} _{} & \\
4a^{}_{} & ^{} \textit{j} ← \textit{f} + \textit{g} _{} & \\
4b^{}_{} & ^{} \mathbf{π}_{\textit{j}} : \vect{a}_1 _{} & \\
4c^{}_{} & ^{} \textit{j} ← \textit{j} + 1 _{} & \\
4d^{}_{} & ^{} \textit{d} ← \textit{j} - \textit{f} _{} & \\
5^{}_{} & ^{} \vect{z} ← ( \textit{f} ↓ \mathbf{α}^{\textit{d}})/\mathbf{π} _{} & \\
6^{}_{} & ^{} \mathbf{ρ}(\vect{x}_{\textit{k}}) ← \overbar{\vect{α}}^{\textit{ g}}/\vect{z} _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}<img src="APLimg/prog3x6.bmp"> & 

<table border=1 cellspacing=0 cellpadding=0>
<td align=center>0-origin indexing for \mathbf{π} only & \\
\begin{tabularx}
<td valign=top nowrap> \textit{h}, \textit{f} & \textit{f} is the leading address of the \textit{h}th component of the chained representation of \vect{x}.
<td valign=top nowrap> \vect{b} & Base used for representation of the elements of the grid matrix. & \\
<td valign=top nowrap> \textit{g} & Dimension in \mathbf{π} of the elements of the grid matrix. & \\
<td valign=top nowrap> \vect{a}_1 & Partition symbol. & \\
<td valign=top nowrap> \vect{z} & \textit{k}th component of the chained representation of \vect{x} exclusive of the terminal partition symbol. & \\
<td valign=top nowrap> \textit{d} & Dimension of \vect{z} in \mathbf{π}. & \\
\end{tabularx} & \\\end{tabularx}
\par \textbf{Legend}
 & \\\end{tabularx}

\par \textbf{Program 3.6} Determination of $\mathbf{ρ}(\vect{x}_k)$ from a chained representation of $\vect{x} \\$
 with terminal partitions \vect{a}_1

\begin{tabularx} & 
\par \textbf{Program 3.6}. The program is similar to Program 3.4 and the step numbering indicates the correspondences. The dimension $d$ is so determined (steps 4a-d) as to exclude the terminal partition itself from the quantity $\vect{z}$ specified by step 5. Since only the first column of the grid matrix is incorporated in the partitioned representation, step 6 excises a prefix of dimension $g$ rather than $2g$ as in Program 3.4.
 & & \\\end{tabularx}

<a name="3.2.6"></a>
\par \textbf{Pools}

\par Components of the physical vector $\mathbf{π}$ in use for the representation of one quantity must not be allocated to the representation of some other quantity. The \textit{construction} of a chained representation therefore poses one problem not encountered in its \textit{use}, namely, the specification and observation of restrictions on the availability of components of $\mathbf{π}$. The restrictions can conveniently be specified as a \textit{pool}, consisting of the available components of $\mathbf{π}$. Each allocation made must then be reflected in a corresponding change in the pool. Moreover, as each piece of data is deleted, the components allocated to it are returned to the pool.

\par If, as in Program 3.5, a pool is treated as a stack, then the component next taken from the pool is the component last added to it. The queue of components in the pool thus obeys a so-called \textit{last in first out}, or LIFO discipline. The dimension in $\mathbf{π}$ of the last component of a pool will not, in general, agree with the dimension required for the next quantity it is called on to represent. If it exceeds the requirements, the extra segment may be left in the pool, and the pool therefore tends to accrue more and more components of smaller and smaller dimension. Hence it may be wise, or even essential, to revise the pool occasionally so as to coalesce the segments into the smallest possible number of infixes. This process can even be extended to allow substitutions in other vectors in order to return to the pool short segments which may unite existing segments of the pool. This, however, will require a systematic scan of the chained vectors.

\par If the dimension of the last component (or perhaps of all components) of the pool falls short of the requirements for representing a new quantity, segments of the pool can be chained together. This requires the use of a special partition symbol or other indication to distinguish two types of links, one which marks the end of a given representation and one which does not. More generally, it may be convenient to use multilevel partition symbols to distinguish several levels of links, as was suggested for the representation of a matrix.

\par Queue disciplines other than LIFO may be used. Three other types of primary interest in allocation queues are the FIFO (first in first out), the \textit{dimension-ordered}, and the \textit{address-ordered} disciplines. FIFO uses a forward chain and may be preferred over LIFO because it uses the entire original pool before using any returned (and usually shorter) segments.

\par The components of a dimension-ordered pool are maintained in ascending (or descending) order on their dimensions in $\mathbf{π}$. This arrangement is convenient in selecting a pool element according to the dimension required. The components of an address-ordered pool are arranged in ascending order on their leading addresses. This arrangement facilitates the fusion of components which together form an infix of $\mathbf{π}$.

\par If each of the available components of $\mathbf{π}$ is set to a special value which is used for no other purpose, then the available components can be determined by a scan of $\mathbf{π}$. Such a pool has no structure imposed by chaining and will be called a \textit{marked pool}.

\par A marked pool requires little maintenance, since components returned to it are simply marked, but selection from it requires a scan of $\mathbf{π}$ and is therefore relatively slow. The use of marked and chained pools may also be combined---all returned components go to a marked pool which is left undisturbed until the chained pool is exhausted, at which time the entire marked pool is organized into a chained pool.

<a name="3.2.7"></a>
\par \textbf{Summary}

\par Since any structured operand can first be reduced to an equivalent vector, the problems of representation can be discussed in terms of vectors alone. The characteristics of the linear, chained, and partitioned representations of a vector may be summarized as follows. A linear representation permits the address of any component to be computed directly as a linear function of its indices and hence requires no scanning of the vector. However, the strict limitations which it imposes on allocation may engender: (1) conflicts with allocations for other operands, (2) waste of storage due to the imposition of a common dimension in $\mathbf{π}$ for all components, or (3) uneconomical execution due to the extensive reallocations occasioned by the insertion or deletion of other than terminal components.

\par The concept of the grid matrix is helpful even when the corresponding allocation is implicit in the program. The explicit use of a grid matrix which is itself in a linear representation removes the restrictions on the allocation of the vector itself while retaining the advantage of direct address computation. The address computation differs from the linear case only in the addition of a single reference to the grid matrix and hence requires no scanning. The difficulties enumerated for the direct linear representation are not eliminated but merely shifted to the linearly represented grid matrix itself, where they may, however, prove much less serious.

\par A chained representation allows virtually arbitrary allocation, relatively simple operations for the insertion and deletion of components, the direct representation of more complex structures such as trees, and economical joint representations of vectors which have one or more components in common. However, a chained representation requires extra storage for the grid matrix which it incorporates and occasions additional operations for scanning when the components are selected in other than serial order. The required scanning can be reduced by the retention of auxiliary information which allows the chained representation to be entered at several points.

\par A partitioned representation requires the allocation of a single infix of $\mathbf{π}$, and selection requires a fine scan, i.e., a component-by-component scan of $\mathbf{π}$ to detect partition symbols. Partitioning removes the need to incorporate the grid matrix explicitly and does not impose a common dimension in $\mathbf{π}$ for all components.

\par Mixed systems employing combinations of linear, chained, and partitioned representations are frequently advantageous. Block chaining, for example, involves the chaining of blocks, each consisting of an infix of $\mathbf{π}$ and each serving as a linear representation of some infix of the represented vector. Alternatively, each chained block may be a partitioned representation of some infix.

<a name="3.3"></a>
\par \textbf{3.3 Representation of matrices}

\par Structured operands other than vectors may be represented by first reducing them to equivalent vectors which can, by employing the techniques of the preceding section, be represented, in turn, in the physical vector $\mathbf{π}$. In the case of a matrix $\mat{A}$, two alternative reductions are of interest, the row list $\vect{r} = \mat{E}/\mat{A} = \mat{A}^1 \oplus \mat{A}^2 \oplus$ ...
\oplus \mat{A}^{\textit{μ}} and the column list \vect{c} = \mat{E}/\!/\mat{A}. If \vect{r}_{\textit{h}}, \mat{A}_{\textit{j}}^{\textit{i}}, and \vect{c}_{\textit{k}} are corresponding elements of the three alternative representations, then in a 0-origin system:

\begin{tabularx}
<td width=75> & \textit{h} = \textit{vi} + \textit{j}, & \\
 & \textit{k} = \textit{i} + \textit{μj}. & \\
<td colspan=2>Consequently, & \\
 & \textit{i} = ⌊\textit{h} ÷ \textit{ν}\rfloor = \textit{μ} |_0 \textit{k}, & \\
and & \textit{j} = \textit{ν} |_0 h = ⌊\textit{k} ÷ \textit{μ}\rfloor. & \\
\end{tabularx}

\par The dependence of $h$ on $k$ can be obtained directly by substituting the foregoing expressions in the identity

\begin{tabularx}
<td width=75> & \textit{h} = \textit{ν} \times ⌊\textit{h} ÷ \textit{ν}\rfloor + \textit{ν} |_0 \textit{h} & \\
to yield & \textit{h} = \textit{ν} \times (\textit{μ} |_0 \textit{k}) + ⌊\textit{k} ÷ \textit{μ}\rfloor. & \\
Similarly, & \textit{k} = \textit{μ} \times (\textit{ν} |_0 \textit{h}) + ⌊\textit{h} ÷ \textit{ν}\rfloor. & \\
\end{tabularx}

\par The permutation $\vect{h}$ which carries the row list $\vect{r}$ into the column list $\vect{c}$ (that is, $\vect{c} = \vect{h}\int_0\vect{r})$ can be obtained directly from the foregoing expression for $\vect{h}$ as follows:

\begin{tabularx}
<td width=75> & \vect{h} = \textit{ν} \times (\textit{μ}\textbf{ϵ} |_0 \textbf{ι}^0) + ⌊\textbf{ι}^0 ÷ \textit{μ}\textbf{ϵ}\rfloor.

\end{tabularx}

\par The expression for the $k$th component of $\vect{h}$ is identical with the expression for $\vect{h}$ above. Hence, if $\vect{c} = \vect{h}\int_0\vect{r}$, then $\vect{c}_k = \vect{r}_{\vect{h}_k} = \vect{r}_h$ as required.

\par If the row list (or column list) is itself represented linearly, then the address of any component $\mat{A}_j^i$ is obtained as a linear function of the indices $i$ and $j$. If either a file or a chained representation is to be used for the list vector, then the components are processed most efficiently in serial order, and the use of column list or row list is dictated by the particular processes to be effected.

\par If a large proportion of the elements of a matrix are null elements, it is called a \textit{sparse} matrix. Sparse matrices occur frequently in numerical work (where zero serves as the null element), particularly in the treatment of partial difference equations. A sparse matrix $\mat{A}$ can be represented compactly by the row list $\vect{r} = \mat{U}/\mat{A}$, and the logical matrix $\mat{U}$, where $\mat{U} = (\mat{A} \neq$ 0). The matrix $\mat{A}$ may then be obtained by expansion: $\mat{A} = \mat{U}\backslash\vect{r}$.

\par Alternatively, the column list $\vect{c} = (\mat{A} \neq 0)/\!/\mat{A}$ may be used. The transformation between the column list $\vect{c}$ and row list $\vect{r}$ must, in general, be performed as a sequential operation on the elements of $\mat{U}$. Since it is frequently necessary to scan a given matrix in both row and column order (e.g., as either pre- or post-multiplier in a matrix multiplication), neither the row list nor the column list alone is satisfactory. A chaining system can, however, be devised to provide both row and column scanning.

\par Let $\mat{L}$ be a matrix such that $\mat{L}_1$ is a list of the nonzero elements of a matrix $\mat{A}$ in arbitrary order, $\mat{L}_2^i$ is the column index in $\mat{A}$ of element $\mat{L}_1^i$, and $\mat{L}_3^i$ is the row index in $\mat{L}$ of the next nonzero element following $\mat{L}_1^i$ in its row of $\mat{A}$. If $\mat{L}_1^i$ is the last nonzero element in its row, $\mat{L}_3^i = ∘$. Let $\vect{f}_j$ be the row index in $\mat{L}$ of the first nonzero element of row $\mat{A}^j$, and let $\vect{f}_j = ∘$ if $\mat{A}^j =$ 0. The following example shows corresponding values of $\vect{f}, \mat{L}$, and $\vect{f}$ :

\begin{tabularx}<td width=75> & 
\begin{tabularx}
<td rowspan=5>\mat{A} = & 
<td rowspan=5><img src="APLimg/matrixl5.bmp"> & 
6 0 0 9 & 
<td rowspan=5><img src="APLimg/matrixr5.bmp"> & 

0 3 0 0 & \\
0 0 0 0 & \\
7 8 0 4 & \\
0 0 5 0 & \\
\end{tabularx} & & 
\begin{tabularx}
<td rowspan=7>\mat{L} = & 
<td rowspan=7><img src="APLimg/matrixl7.bmp"> & 
8 2 7 & 
<td rowspan=7><img src="APLimg/matrixr7.bmp"> & 

5 3 ∘ & \\
6 1 5 & \\
3 2 ∘ & \\
9 4 ∘ & \\
7 1 1 & \\
4 4 ∘ & \\
\end{tabularx} & & 
\begin{tabularx}
<td rowspan=5>\vect{f} = & 
<td rowspan=5><img src="APLimg/matrixl5.bmp"> & 
3 & 
<td rowspan=5><img src="APLimg/matrixr5.bmp"> & 
<td rowspan=5>. & 

4 & \\
∘ & \\
6 & \\
2 & \\
\end{tabularx} & \\
\end{tabularx}

\par The matrix $\mat{L}$ will be called a \textit{row-chained} representation of $\mat{A}$ and may be used, together with the vector $\vect{f}$, for the efficient scanning of any row $\mat{A}^i$ as illustrated by Program 3.7. The vector $\mat{L}_3$ can be modified so as to give the address in $\mathbf{π}$ directly rather than the row index in $\mat{L}$ of the next element in the row, and Program 3.7 can then be easily re-expressed in terms of the physical vector $\mathbf{π}$.

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
<td nowrap>1^{}_{} & ^{} \mat{A}^{\textit{i}} ← 0 _{} & \\
2^{}_{} & ^{} \textit{k} ← \vect{f}_{\textit{i}} _{} & \\
3^{}_{} & ^{} \textit{k} : ∘ _{} & \\
4^{}_{} & ^{} \textit{j} ← \mat{L}_2^{\textit{k}} _{} & \\
5^{}_{} & <td nowrap>^{} \mat{A}_{\textit{j}}^{\textit{i}} ← \mat{L}_1^{\textit{k}} _{} & \\
6^{}_{} & ^{} \textit{k} ← \mat{L}_3^{\textit{k}} _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}<img src="APLimg/prog3x7.bmp"> & 
 & <table border=1 cellspacing=0 cellpadding=0>
<td align=center>1-origin indexing & \\
\begin{tabularx}
<td valign=top nowrap> \vect{f}_{\textit{i}} ^{} & Row index in \mat{L} of first nonzero element of row \mat{A}^{\textit{i}}. \vect{f}_{\textit{i}} = ∘ if \mat{A}^{\textit{i}} = 0. & \\
<td valign=top nowrap> \textit{k} & Row index in \mat{L} of next element. & \\
<td valign=top nowrap> \mat{L}_1 & List of nonzero elements of \mat{A}. & \\
<td valign=top nowrap> \mat{L}_2^{\textit{k}} & Column index in \mat{A} of \mat{L}_1^{\textit{k}}. & \\
<td valign=top nowrap> \mat{L}_3^{\textit{k}} & Row index in_{}\mat{L}^{}of next nonzero element following \mat{L}_1^{\textit{k}} in its row in \mat{A}. \mat{L}_3^{\textit{k}} = ∘ if no such element exists. & \\
\end{tabularx} & \\\end{tabularx}
\par \textbf{Legend} $& \\$
\end{tabularx}

\par \textbf{Program 3.7} Determination of the row vector $\mat{A}^i\\$
 from a row-chained representation of \mat{A}

\begin{tabularx} & 
\par \textbf{Program 3.7}. Step 2 yields the index in $\mat{L}$ of the first element of the $i$th row of $\mat{A}$. Step 4 determines its column index $j$, and step 6 determines the index of the succeeding component. The process terminates at step 3 when the scan of the row is completed.
 & & \\\end{tabularx}

\par If $\mat{L}_1$ is chosen as a row list, the vector $\mat{L}_3$ reduces to the form $\mat{L}_3^k = k +$ 1 or $\mat{L}_3^k = ∘$. Its function can then be served instead by incrementation of the index $k$ and by the use of the logical vector $\vect{u} = (\mat{L}_3^k = ∘\textbf{ϵ})$ for determining the end of each row.

\par The construction of a column-chained representation is analogous to that of a row-chained representation, and the two representations can be combined in a single matrix $\mat{L}$ which gives both row and column chaining employing but a single representation (that is, $\mat{L}_1)$ of the nonzero elements of $\mat{A}$.

<a name="3.4"></a>
\par \textbf{3.4 Representation of trees}^{<a href="#note3b">[b]</a>}

\par A tree $\mat{T}$ may be represented by a matrix and hence, in turn, by a vector in a number of useful ways as follows:

\begin{tabularx}
1. & by a full right list matrix ]\mat{T} or by any column permutation thereof (Sec. 1.23), & \\
2. & by a full left list matrix [\mat{T} or by any column permutation thereof, & \\
3. ^{} & by a right list matrix \mathbf{α}^2/]\mat{T}, & \\
4. ^{} & by a left list matrix \mathbf{α}^2/[\mat{T}, & \\
5. & by various chain list matrices. & \\
\end{tabularx}

\par The full left and right lists seldom prove more convenient than the more concise left and right lists. Except for the special case of a homogeneous tree, both the right list and the left list are awkward to use for path tracing. This function is better served by the chain list matrix, to be defined as a formalization of the chaining scheme suggested in Sec. 3.2.

<a name="3.4.1"></a>
\par \textbf{Simplified list matrices}

\par In certain important special cases, the various list representations of trees may be simplified. If the degree of each node is a known function $δ$ of the value of the node, then for any list matrix $\mat{M}, \mat{M}_1^i = δ(\mat{M}_2^i)$, and the degree vector $\mat{M}_1$ may be eliminated without loss. The node vector alone then represents the tree and may be referred to as a \textit{right} or \textit{left list vector} as the case may be.

\par <img src="APLimg/fig3x8.bmp">

\par \textbf{Figure 3.8} The compound logical statement 
\overbar{x} ∧ (\textit{y} \vee \textit{z})

\par For example, in the tree of Fig. 3.8 (which represents the compound logical statement $\overbar{x} ∧ (y \vee z))$, a fixed degree is associated with each of the logical operators \textit{and}, $or$, and \textit{not} (namely, 2, 2, and 1), and the degree zero is associated with each of the variables. The statement can therefore be represented unambiguously by the left list vector

\begin{tabularx}
<td width=75> & 
\vect{v} = (\wedge, ¯, \textit{x}, \vee, \textit{y}, \textit{z}). & \\
\end{tabularx}

\par This is the so-called \textit{Lukasiewicz}, \textit{Polish}, or \textit{parenthesis-free} form of the compound statement [<acronym title="Lukasiewicz, Jan, (1951), $Aristotle’s$ Syllogistic from the Standpoint of Modern Formal Logic, Clarendon Press, Oxford, England, p. 78.">Lukasiewicz (1951)</acronym> and 
<acronym title="Burks, A.W., D.W. Warren, and J.B. Wright, (1954), “An Analysis of a Logical Machine Using Parenthesis-free Notation”, Mathematical Tables and Other Aids to Computation, vol. VIII, pp. 53-57. ">Burks et al. (1954)</acronym>]. Frequently, the only significant nodes of a tree \mat{T} are its leaves (e.g., in Example 3.2 and in a certain key transformation of Fig. 4.7) and all other nodes may be considered as nulls. Hence if \mat{M} is any list matrix, the significant portions of \mat{M}_1 and \mat{M}_2 are (\mat{M}_1 \neq 0)/\mat{M}_1 and (\mat{M}_1 = 0)/\mat{M}_2, respectively. These significant portions may then be coalesced to form the single vector

\begin{tabularx}
<td width=75> & 
\vect{v} = /\mat{M}_1, (\mat{M}_1 = 0), \mat{M}_2/, & \\
\end{tabularx}

\par which, together with the logical vector $(\mat{M}_1 =$ 0), forms a \textit{leaf list matrix} that describes the tree. Moreover, if the values of the leaves are distinguishable from the components of the degree vector, the logical vector $(\mat{M}_1 =$ 0) may also be dropped.

<a name="3.4.2"></a>
\par \textbf{The use of left lists}

\par The use of the right list matrix is illustrated by the repeated selection sort treated in Sec. 6.4. The use of left lists will be illustrated here by two examples, each of interest in its own right: the partitioning of the left list of an $n-tuply$ rooted tree to yield the left lists of the component singular subtrees and the construction of a Huffman minimum-redundancy prefix code.

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
<td nowrap>1^{}_{} & ^{} \vect{p} ← \textbf{ϵ}(0) _{} & \\
2^{}_{} & ^{} \textit{i} ← 0 _{} & \\
3^{}_{} & ^{} \textit{i} : \textit{μ}(\mat{Z}) _{} & \\
4^{}_{} & ^{} \textit{m} ← 0 _{} & \\
5^{}_{} & ^{} \textit{r} ← 0 _{} & \\
6^{}_{} & ^{} \textit{i} : \textit{μ}(\mat{Z}) _{} & \\
7^{}_{} & ^{} \textit{i} ← \textit{i} + 1 _{} & \\
8^{}_{} & ^{} \textit{m} ← \textit{m} + 1 _{} & \\
9^{}_{} & <td nowrap>^{} \textit{r} ← \textit{r} + 1 - \mat{Z}_1^{\textit{i}} _{} & \\
10^{}_{} & ^{} \textit{r} : 1 _{} & \\
11^{}_{} & ^{} \vect{p} ← \vect{p} \oplus (\textit{m}) _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}<img src="APLimg/prog3x9.bmp"> & & 
<table border=1 cellspacing=0 cellpadding=0>
<td align=center>1-origin indexing & \\
\begin{tabularx}
<td valign=top nowrap>\mat{Z} & Given left list of \mat{T}. & \\
<td valign=top>\textit{i} & Row index of \mat{Z} in ascending scan. & \\
<td valign=top>\textit{r} & Indicated number of roots of current rooted subtree. & \\
<td valign=top nowrap>\textit{m} & Moment of current rooted subtree. & \\
<td valign=top>\vect{p} & Partition vector of \mat{Z}, that is, \vect{p}_{\textit{j}} = \textit{μ}(\mat{T}_{\textit{j}}). & \\
\end{tabularx} & \\
\end{tabularx}
\par \textbf{Legend}

\end{tabularx}

\par \textbf{Program 3.9} Partitioning of the left list of an $n-tuply$ rooted tree

\begin{tabularx} & 
\par \textbf{Example 3.1. Partitioning of an $n-tuply$ rooted tree.} Program 3.9 shows a scheme for partitioning a left list $\mat{Z}$ of a tree \mat{T} into component subtrees, i.e., for determining the vector $\vect{p}$ such that $\vect{p}_j$ is the moment of the singular subtree $\mat{T}_j$. Thus $ν(\vect{p}) = \mathbf{μ}_1(\mat{T}), \vect{p}_j = μ(\mat{T}_j)$, and the infix $((\vect{p}$ {+ $\atop \times} \vect{a}^{$ j-1}) $↓ \vect{a}^{\vect{ p}_j})/\!/\mat{Z}$ is the left list of $\mat{T}_j$.

\par The loop 6-10 scans successive components of the degree vector $\mat{Z}_1$ (in ascending order) and computes $\vect{r}$, the indicated number of roots. The value of $\vect{r}$ increases by, at most, one per iteration, and when $\vect{r}$ becomes unity, the end of a singly rooted tree has been reached. Its moment $m$ is then appended (step 11) as a new final component of the partition vector $\vect{p}$, the parameters $m$ and $r$ are reset, and the scan of the next rooted tree is begun. Normal termination occurs at step 3; termination at step 6 indicates ill formation of $\mat{Z}$.
 & & \\\end{tabularx}

<table align=center>
<img src="APLimg/fig3x10.bmp"> & \\
\end{tabularx}

\par \textbf{Figure 3.10} Construction of a Huffman prefix code

\begin{tabularx} & 
\par \textbf{Example 3.2. Huffman minimum redundancy prefix code.} If $\vect{b}$ is any set such that $ν(\vect{b}) >$ 1, then any other finite set $\vect{a}$ can be encoded in $\vect{b}$, that is, represented by $\vect{b}$. (The sets $\vect{a}$ and $\vect{b}$ may be called the ``alphabet'' and ``basic alphabet'', respectively.) If $ν(\vect{a}) ≤ ν(\vect{b})$, the encoding may be described by a mapping vector $\vect{k}$ such that $\mathbf{ρ}(\vect{a}_i) = \vect{b}_{\vect{k}_i}$. If $ν(\vect{a}) > ν(\vect{b})$, then each $\vect{a}_i$ must be represented by a vector $\vect{x}^i ⊆ \vect{b}$. For example, if $\vect{a} = \textbf{ι}^0(10)$ and $\vect{b} = \textbf{ι}^0(2)$, then the decimal digits $\vect{a}$ may be encoded in the so-called 8421 system:

\begin{tabularx}
<td width=75> & (2 \textbf{ϵ}(4)) ⊥ \vect{x}^{\textit{i}} = \vect{a}_{\textit{i}}

\end{tabularx}

\par In so-called fixed length coding the vectors $\vect{x}^i$ have a common dimension $d$, and the decoding of a message $m$ (consisting of the catenation of vectors $\vect{x}^i)$ involves the selection of successive infixes of dimension $d$. If the probability distribution of the characters $\vect{a}_1$ occurring in messages is not uniform, more compact encoding may be achieved by using variable length codes and assigning the shorter codes to the more frequent characters. Decoding of a message in variable length coding can be performed only if the boundaries between the successive $\vect{x}^i$ are indicated in some way.

\par The boundaries between characters in a message in variable length code may be demarked by special partition symbols (which is inefficient) or by using a \textit{prefix} code in which no legitimate \textit{code point} $\vect{x}^i$ is the prefix of any other legitimate code point, including itself. The index vectors of the leaves of any tree possess this property; conversely, any set of prefix codes can be arrayed as the leaves of some tree. Hence if each character of the set to be encoded is assigned as the leaf of a common tree, and if each character is encoded by the associated index vector, a so-called prefix code is attained. Figure 3.10 furnishes an example of a binary code (i.e., the branching ratios do not exceed two) constructed in this manner. 0-origin indexing is used. The discussion will be limited to binary trees.

\par If $\vect{f}_i$ is the frequency of the $i$th character and $\vect{l}_i$ is the length of the assigned code (i.e., the length of path to the root), then the most efficient code is attained by minimizing the scalar product $\vect{f}$ {+ $\atop \times} \vect{l}$. This may be achieved by the following construction, shown to be optimal by 
<acronym title="Huffman, D.A., (1952), “A Method for the Construction of Minimum Redundancy Codes”, Proc. IRE, vol. 40, pp. 1098-1101.">Huffman (1952)</acronym>. First, the characters to be encoded are all considered as roots, and the two roots of lowest frequency are rooted to an auxiliary node (shown as a null element in Fig. 3.10), which is then assigned their combined frequency. The process is repeated until only two roots remain. The tree of Fig. 3.10 is optimal with respect to the frequencies shown to the left of the leaves. The appropriate combined frequencies are shown to the left of each of the nonleaves.

\par Programs 3.11 and 3.12 show the construction of the tree \mat{T} representing a Huffman code for a set of characters $\vect{c}_i$ with frequencies $\vect{f}_i$ the former in terms of the tree itself and the latter in terms of its left list.
 & & \\\end{tabularx}

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
<td nowrap>1^{}_{} & ^{} \mat{T} ← \vect{c} _{} & \\
2^{}_{} & ^{} \mathbf{μ}_1(\mat{T}) : 2 _{} & \\
3^{}_{} & ^{} \mat{T} ← (\textit{θ}/\vect{f} )\int\mat{T} _{} & \\
4^{}_{} & ^{} \mat{T} ← \backslash\backslash(∘) \oplus \mathbf{α}^2/\!/\mat{T}, \overbar{\vect{α}}^1, \overbar{\vect{α}}^2/\!/\mat{T}\backslash\backslash _{} & \\
5^{}_{} & <td nowrap>^{} \vect{f} ← (\textit{θ}/\vect{f}) \int \vect{f} _{} & \\
6^{}_{} & ^{} \vect{f} ← \backslash+/\mathbf{α}^2/\vect{f}, \overbar{\vect{α}}^1, \overbar{\vect{α}}^2/\vect{f} \backslash _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

<table align=center>
<img src="APLimg/prog3x11.bmp">
 & \\\end{tabularx}

\par \textbf{Program 3.11} Construction of the binary Huffman code $\mat{T}\\$
 for characters \vect{c} with frequency \vect{f}

\begin{tabularx} & 
\par \textbf{Program 3.11}. The frequency vector $\vect{f}$ is permuted (step 5) to bring it to ascending order, and the tree is subjected (step 3) to the same permutation. Step 4 replaces the first two rooted subtrees of \mat{T} by the single subtree obtained by rooting them in a null, and step 6 makes the corresponding alterations in the frequency vector. The tree is initialized (step 1) as a one-level tree whose roots are the given characters, and the process terminates when the number of roots of \mat{T} has been reduced to two.
 & & \\\end{tabularx}

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
<td nowrap>1^{}_{} & ^{} \vect{z} ← \vect{c} _{} & \\
2^{}_{} & <td nowrap>^{} \vect{p} ← \textbf{ϵ}(\textit{ν}(\vect{z})) _{} & \\
3^{}_{} & ^{} \textit{ν}( \vect{f} ) : 2 _{} & \\
4^{}_{} & ^{} \textit{i} ← 1 _{} & \\
5^{}_{} & <td nowrap>^{} \vect{x} ← \textbf{ϵ}(0) _{} & \\
6^{}_{} & ^{} \textit{j} ← (\textit{θ}/\vect{f} )_{\textit{i}} _{} & \\
7^{}_{} & <td nowrap>^{} \vect{x} ← \vect{x} \oplus ((( \vect{p} {+ \atop \times} \mathbf{α}^{\textit{ j}-1}) ↓ \mathbf{α}^{ \vect{p}_{\textit{j}}})/\vect{z}) _{} & \\
8^{}_{} & ^{} \textit{i} ← \textit{i} + 1 _{} & \\
9^{}_{} & ^{} \textit{i} : \textit{ν}( \vect{f} ) _{} & \\
10^{}_{} & ^{} \vect{z} ← (∘) \oplus \vect{x} _{} & \\
11^{}_{} & ^{} \vect{p} ← (\textit{θ}/\vect{f} ) \int \vect{p} _{} & \\
12^{}_{} & ^{} \vect{p} ← (1 + ( \vect{p} {+ \atop \times} \mathbf{α}^2)) \oplus \overbar{\vect{α}}^2/\vect{p} _{} & \\
13^{}_{} & ^{} \vect{f} ← (\textit{θ}/\vect{f} ) \int \vect{f} _{} & \\
<td nowrap>14^{}_{} & ^{} \vect{f} ← (+/\mathbf{α}^2/\vect{f} ) \oplus \overbar{\vect{α}}^{ 2}/\vect{f} _{} & \\
\end{tabularx}
 & \\\end{tabularx} -->

\begin{tabularx}
<img src="APLimg/prog3x12.bmp"> & & 
<table border=1 cellspacing=0 cellpadding=0>
<td align=center>1-origin indexing & \\
\begin{tabularx}
<td valign=top>\vect{c} & & Given character set. & \\
<td valign=top>\vect{z} & & Left list of Huffman tree. & \\
<td valign=top>\vect{f}_{\textit{i}} & & Frequency of \textit{i}th subtree of \vect{z}. & \\
<td valign=top>\vect{p} & & Partition vector \vect{p}_{\textit{i}} is the moment of the \textit{i}th subtree of \vect{z}. & \\
<td valign=top>\vect{x} & & Reordered left list with subtrees in ascending order on frequency. & \\
\end{tabularx} & \\\end{tabularx}
\par \textbf{Legend}
\par \textbf{Program 3.12} Construction of the left list $\vect{z}\\$
 of the binary Huffman code for characters \vect{c} with frequency \vect{f}

\end{tabularx}

\begin{tabularx} & 
\par \textbf{Program 3.12}. The tree \mat{T} of Program 3.11 is represented by the left list node vector $\vect{z}$, in conjunction with the implicit degree vector $\vect{d} =$ 2 $\times (\vect{z} = ∘\textbf{ϵ})$. The algorithm differs from Program 3.11 primarily in the reordering of the subtrees (steps 6-9). Step 7 appends to $\vect{x}$ the left list of the ith subtree (of the reordered tree) selected by the partition vector $\vect{p}$ according to the conventions of Program 3.9. Step 1a prefixes $\vect{x}$ by the new null root, and steps 11-12 redefine $\vect{p}$ appropriately.

\par Program 1.21 can be applied to the left list produced by Program 3.12 to determine the associated index matrix (in a 0-origin system), and hence the actual codes assigned.

\par It is not essential that the characters be assigned to leaves in precisely the order specified by Programs 3.11 and 3.12, and it is sufficient that the dimension of the leaf index increase monotonically with decreasing frequency of the character. It is therefore unnecessary to carry the characters themselves through the process; it suffices to determine the structure of the tree, sort the corresponding index matrix to right list order (which is ordered on dimension of the index vectors), and assign the characters (in decreasing order by frequency) to successive leaves. Since the structure of such a tree (whose nodes have a common irrelevant value and whose nonleaves all have a common branching ratio equal to the number of roots) is sufficiently determined by the moment vector $\mathbf{μ}(\mat{T})$, the process of Program 3.12 can be simplified.
 & & \\\end{tabularx}

<a name="3.4.3"></a>
\par \textbf{Chain list matrices}

\par The full chain list matrix of a tree \mat{T} is a matrix $\mat{P}$ of dimension $μ(\mat{T}) \times (δ(\mat{T}) +$ 2) defined as follows: $\mat{P}_2$ is some node vector of \mat{T}, $\mat{P}_1$ is the associated degree vector, $\mat{P}_{j+2}^i$ is null if $j$ exceeds the associated degree $\mat{P}_1^i$ and is otherwise the row index in $\mat{P}$ of the $j$th node emanating from node $\mat{P}_2^i$. Table 3.13 shows a full chain list matrix for the tree of Fig. 1.16. A full chain list matrix is called a full \textit{right} (\textit{left}) chain list matrix if the nodes occur in right (left) list order.

<table align=center>
<td valign=top>\begin{tabularx} & \\ <td align=center>1 & \\ <td align=center>2 & \\ <td align=center>3 & \\ <td align=center>4 & \\ <td align=center>5 & \\ <td align=center>6 & \\ <td align=center>7 & \\ <td align=center>8 & \\ <td align=center>9 & \\ 10 & \\ 11 & \\ 12 & \\ 13 & \\ 14 & \\ 15 & \\ 16 & \\ 17 & \\ 18 & \\ 19 & \\ 20 & \\ 21 & \\ 22 & \\ 23 & \\ 24 & \\ 25 & \\ 26 & \\ \end{tabularx} & 

<td valign=top align=center><table border=1 cellspacing=0 cellpadding=0>
<td align=center> \vect{d}' & <td align=center> \vect{n}' & <td colspan=4 align=center>\mat{Q} & \\
 \begin{tabularx} <td align=center>1 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>1 & \\ <td align=center>4 & \\ <td align=center>0 & \\ <td align=center>0 & \\ \end{tabularx} & \begin{tabularx} <td align=center>n & \\ <td align=center>g & \\ <td align=center>u & \\ <td align=center>t & \\ <td align=center>a & \\ <td align=center>b & \\ <td align=center>v & \\ <td align=center>k & \\ <td align=center>z & \\ <td align=center>o & \\ <td align=center>f & \\ <td align=center>r & \\ <td align=center>y & \\ <td align=center>s & \\ <td align=center>d & \\ <td align=center>j & \\ <td align=center>m & \\ <td align=center>i & \\ <td align=center>w & \\ <td align=center>h & \\ <td align=center>e & \\ <td align=center>p & \\ <td align=center>x & \\ <td align=center>c & \\ <td align=center>q & \\ <td align=center>l & \\ \end{tabularx} & \begin{tabularx} 18 & \\ 16 & \\ ∘ & \\ ∘ & \\ 24 & \\ <td align=center>8 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ <td align=center>4 & \\ ∘ & \\ ∘ & \\ <td align=center>3 & \\ 22 & \\ ∘ & \\ 10 & \\ ∘ & \\ ∘ & \\ 13 & \\ 11 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & \begin{tabularx} ∘ & \\ 26 & \\ ∘ & \\ ∘ & \\ <td align=center>9 & \\ 20 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 19 & \\ ∘ & \\ ∘ & \\ 14 & \\ 25 & \\ ∘ & \\ 17 & \\ ∘ & \\ ∘ & \\ ∘ & \\ 15 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & \begin{tabularx} ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ <td align=center>1 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 23 & \\ ∘ & \\ ∘ & \\ <td align=center>7 & \\ ∘ & \\ ∘ & \\ ∘ & \\ 12 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & \begin{tabularx} ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 21 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & 
\\\end{tabularx} \par (a)\\
A full chain list matrix & 

<td valign=top align=center><table border=1 cellspacing=0 cellpadding=0>
 \vect{d}' & \vect{n}' & <td align=center> \vect{p} & \\
 \begin{tabularx} 3 & \\ 2 & \\ 2 & \\ 4 & \\ 0 & \\ 1 & \\ 0 & \\ 3 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 2 & \\ 0 & \\ 3 & \\ 0 & \\ 0 & \\ 0 & \\ 0 & \\ 2 & \\ 1 & \\ 0 & \\ 0 & \\ 0 & \\ \end{tabularx} & \begin{tabularx} <td align=center>a & \\ <td align=center>b & \\ <td align=center>g & \\ <td align=center>c & \\ <td align=center>z & \\ <td align=center>n & \\ <td align=center>k & \\ <td align=center>h & \\ <td align=center>j & \\ <td align=center>l & \\ <td align=center>f & \\ <td align=center>d & \\ <td align=center>r & \\ <td align=center>e & \\ <td align=center>i & \\ <td align=center>o & \\ <td align=center>m & \\ <td align=center>v & \\ <td align=center>p & \\ <td align=center>q & \\ <td align=center>u & \\ <td align=center>s & \\ <td align=center>x & \\ <td align=center>t & \\ <td align=center>w & \\ <td align=center>y & \\ \end{tabularx} & \begin{tabularx} <td align=center>4 & \\ <td align=center>7 & \\ <td align=center>9 & \\ 11 & \\ ∘ & \\ 15 & \\ ∘ & \\ 16 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 19 & \\ ∘ & \\ 21 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 24 & \\ 26 & \\ ∘ & \\ ∘ & \\ ∘ & \\ \end{tabularx} & 
\\\end{tabularx} \par (b)\\
The right chain\\
list matrix & 

<td valign=top align=center><table border=1 cellspacing=0 cellpadding=0>
 \vect{d}' & \vect{n}' & \vect{f} & \vect{h} & \\
 \begin{tabularx} <td align=center>1 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>2 & \\ <td align=center>0 & \\ <td align=center>3 & \\ <td align=center>0 & \\ <td align=center>0 & \\ <td align=center>1 & \\ <td align=center>4 & \\ <td align=center>0 & \\ <td align=center>0 & \\ \end{tabularx} & \begin{tabularx} <td align=center>n & \\ <td align=center>g & \\ <td align=center>u & \\ <td align=center>t & \\ <td align=center>a & \\ <td align=center>b & \\ <td align=center>v & \\ <td align=center>k & \\ <td align=center>z & \\ <td align=center>o & \\ <td align=center>f & \\ <td align=center>r & \\ <td align=center>y & \\ <td align=center>s & \\ <td align=center>d & \\ <td align=center>j & \\ <td align=center>m & \\ <td align=center>i & \\ <td align=center>w & \\ <td align=center>h & \\ <td align=center>e & \\ <td align=center>p & \\ <td align=center>x & \\ <td align=center>c & \\ <td align=center>q & \\ <td align=center>l & \\ \end{tabularx} & \begin{tabularx} ∘ & \\ ∘ & \\ 14 & \\ 19 & \\ <td align=center>6 & \\ <td align=center>2 & \\ ∘ & \\ 20 & \\ <td align=center>1 & \\ 17 & \\ 15 & \\ 21 & \\ ∘ & \\ 23 & \\ 12 & \\ 26 & \\ <td align=center>7 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ 25 & \\ ∘ & \\ <td align=center>9 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & \begin{tabularx} 18 & \\ 16 & \\ ∘ & \\ ∘ & \\ 24 & \\ <td align=center>8 & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ ∘ & \\ <td align=center>4 & \\ ∘ & \\ ∘ & \\ <td align=center>3 & \\ 22 & \\ ∘ & \\ 10 & \\ ∘ & \\ ∘ & \\ 13 & \\ 11 & \\ ∘ & \\ ∘ & \\ \end{tabularx} & 
\\\end{tabularx} \par (c)\\
Filial-heir chain list & 

<td valign=top>\begin{tabularx} & \\ <td align=center>1 & \\ <td align=center>2 & \\ <td align=center>3 & \\ <td align=center>4 & \\ <td align=center>5 & \\ <td align=center>6 & \\ <td align=center>7 & \\ <td align=center>8 & \\ <td align=center>9 & \\ 10 & \\ 11 & \\ 12 & \\ 13 & \\ 14 & \\ 15 & \\ 16 & \\ 17 & \\ 18 & \\ 19 & \\ 20 & \\ 21 & \\ 22 & \\ 23 & \\ 24 & \\ 25 & \\ 26 & \\ \end{tabularx} & 

\\\end{tabularx}

\par \textbf{Table 3.13} Chain lists of the tree of Fig. 1.16

\par The full chain list matrix is a formalization of the scheme suggested in the discussion of chained representations (Sec. 3.2). Its convenience in forward path tracing is obvious. Since it does not identify the roots of the tree, an auxiliary vector must be provided for this purpose. However, if the ordering chosen for the nodes is that of a right list, the roots occur first in the list, their number $r = ν(\mat{P}_1) - (+/\mat{P}_1)$ is specified by the degree vector $\mat{P}_1$, and the need for the auxiliary vector vanishes. Moreover, since a right list groups all nodes emanating from a given node, each row of $\overbar{\vect{α}}^2/\mat{P}$ is simply a sequence of integers followed by null elements, and the information necessary to path tracing is provided by the column $\mat{P}_3$ alone.

\par The \textit{right chain list matrix} of a tree \mat{T} is therefore defined as $\overbar{\vect{α}}^3/\mat{P}$, where $\mat{P}$ is the full right chain list matrix of \mat{T}. It is illustrated by Table 3.13b. Program 3.14 shows its use in path tracing. Although the degree vector $\mat{P}_1$ is redundant (that is, $\mat{P}_1$ and $\mat{P}_3$ can be determined one from the other), it provides a direct check (step 6) on the legitimacy of the index vector $\vect{r}$ which would be difficult to obtain from $\mat{P}_3$ alone.

\par For a search of the type described by Program 3.14, it is necessary to scan down a level until agreement is reached and then across to the next level. For this type of scan, the \textit{filial-heir chain list} is compact and convenient.

<!--
<table border=1 cellspacing=0 cellpadding=0>
\begin{tabularx}
1 & & _{} \textit{k} ← 1 ^{} & \\
2 & & _{} \textit{j} ← 0 ^{} & \\
3 & & <td nowrap>_{} \textit{d} ← \textit{ν}(\mat{P}_1) - +/\mat{P}_1 ^{} & \\
4 & & _{} \textit{j} : \textit{ν}(\vect{r}) ^{} & \\
5 & & _{} \textit{j} ← \textit{j} + 1 ^{} & \\
6 & & _{} \vect{r}_{\textit{ j}} : \textit{d} ^{} & \\
7 & & _{} \textit{i} ← \textit{k} + \vect{r}_{\textit{ j}} - 1 ^{} & \\
8 & & _{} \vect{p}_{\textit{ j}} ← \mat{P}_2^{\textit{i}} ^{} & \\
9 & & _{} \textit{d} ← \mat{P}_1^{\textit{i}} ^{} & \\
10 & & _{} \textit{k} ← \mat{P}_3^{\textit{i}} ^{} & \\
\end{tabularx} & \\
\end{tabularx} -->

\begin{tabularx}
<td valign=top><img src="APLimg/prog3x14.bmp"> & 

<table border=1 cellspacing=0 cellpadding=0>
<td align=center>1-origin indexing & \\
\begin{tabularx}
<td valign=top> \vect{r} & & Given index vector. & \\
<td valign=top> \mat{P} & & Right chain list matrix of \mat{T}. & \\
<td valign=top> \mat{P}_1 & & Degree vector of \mat{T}. & \\
<td valign=top> \mat{P}_2 & & Node vector of \mat{T}. & \\
<td valign=top> \mat{P}_3 & & Chaining vector of \mat{T}. & \\
<td valign=top> \vect{p}^{} & & Path vector \mat{T}^{\vect{r}}. & \\
<td valign=top> \textit{d} & & Degree of current node. & \\
<td valign=top> \textit{k} & & Base address of the infix containing the current node. & \\
<td valign=top> \textit{i} & & Index of succeeding node in the path \mat{T}^{\vect{r}}. & \\
<td valign=top> \textit{j} & & Current index of index vector \vect{r}. & \\
\end{tabularx} & \\\end{tabularx}
\par \textbf{Legend}
 & \\\end{tabularx}

\par \textbf{Program 3.14} Determination of the path $\vect{p} = \mat{T}^{\vect{r}}\\$
 from the right chain list matrix \mat{P}

\par The set of $(j +$ 1)th level nodes of the subtree $\mat{T}_{\vect{i}}$ are collectively called the \textit{jth filial vector} of node $\vect{i}$, and the first member of the first filial vector of node $\vect{i}$ is called the \textit{heir} of node $\vect{i}$. (For brevity, the first filial vector of a node will also be called its filial vector.) If each node is chained only to its successor in the filial vector containing it and to its heir, the resulting representation is called a \textit{filial-heir chain list}. Formally, the filial-heir representation of a tree \mat{T} is a matrix $\mat{F}$ of dimension $μ(\mat{T}) \times$ 4 such that $\mat{F}_2$ is a node vector of \mat{T}, $\mat{F}_1$ is the associated degree vector, $\mat{F}_3$ is a \textit{filial chain} such that $\mat{F}_3^i = j$ if node $\mat{F}_2^j$ is the successor of node $\mat{F}_2^i$ in the smallest filial set containing it and $\mat{F}_3^i = ∘$ if node $\mat{F}_2^i$ has no such successor, and $\mat{F}_4$ is an \textit{heir chain} such that $\mat{F}_4^i = h$ if node $\mat{F}_2^h$ is the heir of node $\mat{F}_2^i$ and $\mat{F}_4^i = ∘$ if $\mat{F}_2^i$ is a leaf. The filial-heir chain list is illustrated in Table 3.13c.

<a name="ref3"></a>
\par \textbf{References}

\begin{tabularx}
<td valign=top nowrap>• & Blaauw, G. A., (1959), ``Indexing and Control-Word Techniques'', \textit{IBM Journal of Research and Development}, vol. 3, pp. 288-301. & \\
<td valign=top>• & Brooks, F.P., Jr., and K.E. Iverson, (1962), (in press) \textit{Automatic Data Processing}, Wiley, New York. & \\
<td valign=top>• & Burks, A.W., D.W. Warren, and J.B. Wright, (1954), ``An Analysis of a Logical Machine Using Parenthesis-free Notation'', \textit{Mathematical Tables and Other Aids to Computation}, vol. VIII, pp. 53-57. & \\
<td valign=top>• & Dewey, Godfrey, (1923), \textit{Relative Frequency of English Speech Sounds}, Cambridge University Press, p. 185. & \\
<td valign=top>• & Huffman, D.A., (1952), ``A Method for the Construction of Minimum Redundancy Codes'', \textit{Proc. IRE}, vol. 40, pp. 1098-1101. & \\
<td valign=top>• & Iverson, K.E., (1955), ``Report by the Staff of the Computation Laboratory to the American Gas Association and Edison Electric Institute'', Section III, Report No.1, Harvard Computation Laboratory. & \\
<td valign=top>• & Johnson, L.R., (1962), ``On Operand Structure, Representation, Storage, and Search'', Research Report # RC-603, IBM Corp. & \\
<td valign=top>• & Lukasiewicz, Jan, (1951), \textit{Aristotle's Syllogistic from the Standpoint of Modern Formal Logic}, Clarendon Press, Oxford, England, p. 78. & \\
<td valign=top>• & Marimont, R.B., (1959), ``A New Method of Checking the Consistency of Precedence Matrices'', \textit{J. ACM}, vol. 6, pp. 164-171. & \\
<td valign=top>• & Ross, I.C., and F. Harary, (1960), ``The Square of a Tree'', \textit{Bell System Tech. J.}, vol. XXXIX, pp. 641-8. & \\
<td valign=top>• & Shaw, J.C., A. Newell, H.A. Simon, and T.O. Ellis, (1958), ``A Command Structure for Complex Information Processing'', \textit{Proc. Western Joint Computer Conference}, pp. 119-128. & \\
\end{tabularx}

<a name="notes3"></a>
\par \textbf{Notes}

\begin{tabularx}
<td valign=top nowrap>a.<a name="note3a"></a> & Chained representations have received extensive treatment, frequently under the name ``lists''. See, for example, 
<acronym title="Shaw, J.C., A. Newell, H.A. Simon, and T.O. Ellis, (1958), “A Command Structure for Complex Information Processing”, Proc. Western Joint Computer Conference, pp. 119-128.">Shaw et al. (1958)</acronym> and
<acronym title="Blaauw, G. A., (1959), “Indexing and Control-Word Techniques”, IBM Journal of Research and Development, vol. 3, pp. 288-301.">Blaauw (1959)</acronym>. & \\
<td valign=top>b.<a name="note3b"></a> & 
<acronym title="Johnson, L.R., (1962), “On Operand Structure, Representation, Storage, and Search”, Research Report # RC-603, IBM Corp.">Johnson (1962)</acronym> provides a comprehensive treatment of the representations of trees and discusses the suitability of each representation for a variety of search procedures. & \\
\end{tabularx}

<a name="ex3"></a>
\par \textbf{Exercises}

\par The symbols $\vect{a}$ and $\vect{c}$ will be used exclusively to denote lower case and capital alphabets defined as follows:

\begin{tabularx}
<td width=75> & \vect{a} = & (0, a, b, c, ... , z, \textbf{.}, \textbf{,}, #, *, +). & \\
 & \vect{c} = & (0, A, B, C, ... , Z, \textbf{.}, \textbf{,}, #, *, +). & \\
\end{tabularx}

\par The expression $\mathbf{π} ⊆ \vect{x}$ will be used to specify the set $\vect{x}$ as the range of the components of $\mathbf{π}$.

\par \textbf{3.1} For each of the following cases, specify a suitable encoding matrix and format vector and show the explicit value of the infix of $\mathbf{π}$ which (in a solid representation) represents the given example vector $\vect{x}$:

\begin{tabularx}
<td valign=top nowrap>(a) ^{} & the decimal digits \vect{d} = \textbf{ι}^0(10) in a ranked fixed-length code for \mathbf{π} ⊆ \textbf{ι}^0(2).\\
 Example: \vect{x} = (6, 8, 9). & \\
<td valign=top nowrap>(b) ^{} & the set \vect{a} in a ranked fixed-length code for \mathbf{π} ⊆ \textbf{ι}^0(2).\\
 Example: \vect{x} = (c, a, t). & \\
<td valign=top>(c) ^{} & the set \vect{a} ∪ \vect{c} ∪ \textbf{ι}^0(10) in a fixed-length code for \mathbf{π} ⊆ \textbf{ι}^0(10).\\
 Example: \vect{x} = (M, a, y, ∘, 3, \textbf{,}, 1,9,6,0, \textbf{.}). & \\
<td valign=top>(d) & the set \vect{a} ∪ \vect{c} in a two-case code (with single-character shift) for \mathbf{π} ⊆ \vect{a}. (See <acronym title="Brooks, F.P., Jr., and K.E. Iverson, (1962), (in press) Automatic Data Processing, Wiley, New York.">Brooks and Iverson, 1962.</acronym>)\\
 Example: x = (T, r, o, y, \textbf{,}, N, \textbf{.}, Y, \textbf{.}). & \\
<td valign=top>(e) ^{} & the set \vect{a} in a Huffman prefix code for \mathbf{π} ⊆ \textbf{ι}^0(2). Assume the frequency distribution given in 
<acronym title="Dewey, Godfrey, (1923), Relative Frequency of English Speech Sounds, Cambridge University Press, p. 185.">Dewey (1923)</acronym>.\\
 Example: \vect{x} = (t, r, e, e). & \\
\end{tabularx}

\par \textbf{3.2} For each of the cases of Exercise 3.1 write a program which decodes the infix $(i ↓ \mathbf{α}^j)/\mathbf{π}$, that is, which produces the vector $\vect{z}$ represented by the infix. The auxiliary physical vector $\mathbf{π}^1 ⊆ \vect{s}$ may be employed to represent the first column of the encoding matrix, where $\vect{s}$ is the set encoded. Perform a partial trace of each program for the example value used in Exercise 3.1.

\par \textbf{3.3} The ordered set of months $\vect{m} =$ (JANUARY, FEBRUARY, ... , DECEMBER) is to be represented by the physical vector $\mathbf{π} ⊆ \vect{c} ∪ \textbf{ι}^0(10)$. For each of the following types of representation, specify a particular representation and show the values of the relevant components of $\mathbf{π}$:

\begin{tabularx}
<td valign=top nowrap>(a) & a linear representation (employing null elements for filling to a common dimension in \mathbf{π}). & \\
<td valign=top nowrap>(b) & a solid representation for each element of \vect{m} and an appropriate grid matrix itself represented linearly. & \\
<td valign=top nowrap>(c) & a chained representation. & \\
<td valign=top nowrap>(d) & a double chained representation. & \\
\end{tabularx}

\par \textbf{3.4} 
\begin{tabularx}<td valign=top nowrap>(a) & For each of the cases of Exercise 3.3, write a program which selects month \vect{m}_{\textit{k}}. & \\
<td valign=top nowrap>(b) & Trace each program for the case \textit{k} = 2. & \\
<td valign=top nowrap>(c) & For case (d) of Exercise 3.3, write a program which selects \vect{m}_{\textit{k}} by forward chaining if \textit{k} ≤ \textit{ν}(\vect{m})÷2, and by backward chaining if \textit{k} > \textit{ν}(\vect{m})÷2. & \\
\end{tabularx}

\par \textbf{3.5} For each of the cases of Exercise 3.3, write a program which ``prints out'' the set of months in a minimum number of $n-character$ lines, inserting a single null between successive months except where (i) further nulls must be added to prevent the continuation of a single word from one line to the next, or (ii) no null is needed between two successive words, the first of which is coterminous with the line. In other words, produce a matrix $\mat{Z}$ of row dimension $n$ and of minimum column dimension such that $(\mat{Z} \neq ∘\mat{E})/\mat{Z} = \mathbf{ρ}(\vect{m}_1) \oplus \mathbf{ρ}(\vect{m}_2) \oplus$ ...
\oplus \mathbf{ρ}(\vect{m}_{12}), and such that each row \mat{Z}^{\textit{i}} may be partitioned into one or more vectors of the form \mathbf{ρ}(\vect{m}_{\textit{k}}) 
\oplus ∘\textbf{ϵ}, all but the last of which must be of dimension \textit{ν}[\mathbf{ρ}(\vect{m}_{\textit{k}})] + 1.

\par \textbf{3.6} Assuming a linear representation for each of the logical vectors involved, and a forward-chained representation for each of the remaining operands, write programs for the following operations. Assume in each case that the arguments $\vect{x}$ and $\vect{y}$ need not be retained, and assume the use of a backward-chained pool where necessary.
\begin{tabularx}
(a) & \vect{z} ← \backslash\vect{x}, \vect{u}, \vect{y}\backslash & \\
(b) & \vect{z} ← /\vect{x}, \vect{u}, \vect{y}/ & \\
(c) & \vect{z} ← \textit{k} ↑ \vect{x} & \\
(d) & \vect{z} ← \textit{k} ↓ \vect{x} & \\
\end{tabularx}

\par \textbf{3.7} Repeat Exercise 3.6(a), using separate grid matrices for $\vect{x}, \vect{y}$, and $\vect{z}$ instead of chained representations. Specify a suitable linear representation for each of the grid matrices.

\par \textbf{3.8} 
\begin{tabularx}
<td valign=top nowrap>(a) & If a chained representation is used for a vector \vect{x}, then the selection of a specified component can be made faster by providing a number of alternative starting points for the required scan. State precisely the quantities required in such a process and write a program showing its use. & \\
<td valign=top nowrap>(b) & If provision is made for starting the scan at any component of \vect{x}, the chained representation may itself be simplified. Show precisely what the simplified form is and identify the type of representation to which it is equivalent. & \\
\end{tabularx}

\par \textbf{3.9} Frequently a vector $\vect{x}$ kept in a partitioned representation (for efficient use of storage) must be ``unpacked'' to a linear or other more accessible form for efficient processing. The converse operation of ``packing'' is also required. Let the partitioned representation be a file $\tree{Φ}$ employing an intercomponent partition $\mathbf{λ}_1$, and a terminal partition $\mathbf{λ}_2$, and write both packing and unpacking programs for each of the following cases. Assume that the maximum dimension in $\mathbf{π}$ of any component is $n$.

\begin{tabularx}
<td valign=top nowrap>(a) & A solid linear representation employing null fill. & \\
<td valign=top nowrap>(b) & An allocation prescribed by a grid matrix \mat{G} with \mat{G}_2 = \textit{n}\textbf{ϵ}. & \\
\end{tabularx}

\par \textbf{3.10} Let $\mathbf{π} ⊆ \textbf{ι}^0(2)$, let the set a be encoded in a five-bit code such that $(2\textbf{ϵ}) ⊥ \mathbf{ρ}(\vect{a}_i) = i$, and let each component of the vector $\vect{x}$ be an (uncapitalized) English word. Using 0-origin indexing throughout, specify a suitable partitioned representation in $\mathbf{π}$ for the vector $\vect{x}$, and repeat Exercises 3.9(a) and 3.9(b), using it in lieu of the files.

\par \textbf{3.11} For each of the following pool organizations, write a program to convert a given marked pool into a backward-chained pool:
\begin{tabularx}
(a) & dimension-ordered. & \\
(b) & address-ordered. & \\
\end{tabularx}

\par \textbf{3.12} For each of the following queue disciplines, write programs which take from and return to the pool an infix of length $n$. Use secondary linking and relegate to a marked pool any infix which is too short for linking. In each case choose the type of chaining best suited to the particular queue discipline.
\begin{tabularx}
<td valign=top nowrap>(a) & LIFO (last-in-first-out). & \\
<td valign=top nowrap>(b) & FIFO (first-in-first-out). & \\
<td valign=top nowrap>(c) & Dimension ordered. & \\
<td valign=top nowrap>(d) & Address-ordered (utilize the possibility of fusing adjacent infixes). & \\
\end{tabularx}

\par \textbf{3.13} Give a complete specification of a scheme for representing a tree \mat{T} by a full chain list matrix which is not in right list order. Write a program (expressed in terms of the physical vector $\mathbf{π})$ which determines the path vector $\mat{T}^{\vect{i}}$ for a given index vector $\vect{i}$.

\par \textbf{3.14} Give a complete specification of a scheme allowing joint representation of those components shared by two or more of a family of vectors $\vect{x}^1, \vect{x}^2$, ..., $\vect{x}^n$ as suggested in Sec. 3.2. Write programs to (i) select component $\vect{x}_j^i$, and (ii) delete component $\vect{x}_j^i$.

\par \textbf{3.15} Let $\mathbf{π} ⊆ \vect{a} ∪ \textbf{ι}^0(10)$, and let $\vect{x}^1, \vect{x}^2$, ..., $\vect{x}^n$ be a family of vectors whose components belong to the set $\overbar{\vect{α}}^1/[\vect{a} ∪ \textbf{ι}^0(10)]$. Let the average and the maximum dimensions of the vectors $\vect{x}^i$ be $a$ and $m$, respectively. Assume that the chaining index is represented in decimal, with each digit represented by one component of $\mathbf{π}$. Determine (as a function of $m$ and $n)$ the value of $a$ below which a chained representation provides more compact storage than a linear representation with null fill.

\par \textbf{3.16} Write a program which uses the minimization operation $\vect{u} ← v ⌊ x$ to determine the ordering permutation vector $\vect{p} ← θ_1/(\vect{a} ι_1 \vect{b})$.

\par \textbf{3.17} Let $\mat{U} = (\mat{X} \neq$ 0) and $\vect{r} = \mat{U}/\mat{X}$ jointly represent the sparse matrix $\mat{X}$.

\begin{tabularx}
<td valign=top nowrap>(a) & Write a program which determines (as a function of \mat{U} and \vect{r}) a suitable row-chained and column-chained representation of \mat{X}. & \\
<td valign=top nowrap>(b) & Write a program defined on the representation produced in part (a) to compute the product \mat{Y} = \mat{X} 
{+ \atop \times} \mat{X}, itself represented in the form \mat{V} = (\mat{Y} \neq 0) and \vect{p} = \mat{V}/\mat{Y}. & \\
<td valign=top nowrap>(c) & Write a program to determine the trace (that is, +/\mat{I}/\mat{X}) of \mat{X} from the representation produced in part (a). & \\
\end{tabularx}

\par \textbf{3.18} The unique assignment of Huffman codes produced by Program 3.12 is, in general, only one of many equally efficient assignments, since the symbols to be coded need only be assigned, in decreasing order on frequency, to the leaves of the code tree in increasing order on their levels. Show that the structure of the tree produced can be sufficiently described by its moment vector alone, and write a program for the construction of a Huffman code based on this fact.

\par \textbf{3.19} Following the notation and terminology used in Program 3.9 for the analogous case of a left list, write a program which determines from the right list $\mat{R}$ of a tree $\mat{T}$, the partition vector $\vect{p}$ which partitions it by levels.

\par \textbf{3.20} Write a program which determines the right list $\mat{R} = \mathbf{α}^2/]\mat{T}$ as a function of the left list $\mat{L} = \mathbf{α}^2/[\mat{T}$. Incorporate tests of well formation.

\par \textbf{3.21} Let $[\mat{X}{\odot_1 \atop \odot_2}]^p$ denote the $p$th power of the square matrix $\mat{X}$ with respect to the operators 
\odot_1 and 
\odot_2, that is, [\mat{X}{\odot_1 \atop \odot_2}]^{\textit{p}} = \mat{X} {\odot_1 \atop \odot_2} \mat{X} {\odot_1 \atop \odot_2} ... 
{\odot_1 \atop \odot_2} \mat{X} to \textit{p} factors.

\begin{tabularx}
<td valign=top nowrap>(a) ^{} & Show that ([\mat{C}{\vee \atop \wedge}]^{\textit{p}})_{\textit{j}}^{\textit{i}} = 1 if and only if there is a path of length \textit{p} from node \textit{i} to node \textit{j} in the graph (\vect{n}, \mat{C}). & \\
<td valign=top nowrap>(b) ^{} & Show that [\mat{C}{\vee \atop \wedge}]^{\textit{p}} = 0 for some \textit{p} < \textit{ν}(\mat{C}) if and only if (\vect{n}, \mat{C}) contains no circuits. & \\
<td valign=top nowrap>(c) & If (\vect{n}, \mat{C}) contains no circuits, the connection matrix \mat{C} is said to be ``consistent''. The result of part (a) can be used to check consistency. Program the alternative method of 
<acronym title="Marimont, R.B., (1959), “A New Method of Checking the Consistency of Precedence Matrices”, J. ACM, vol. 6, pp. 164-171.">Marimont (1959)</acronym>. & \\
<td valign=top nowrap>(d) & If \mat{H} = \mat{C} \vee \mat{I}, then ([\mat{H}{\vee \atop \wedge}]^{\textit{p}})_{\textit{j}}^{\textit{i}} = 1 if and only if \textit{i} = \textit{j} or there exists a path from node \textit{i} to node \textit{j} of length \textit{n} ≤ \textit{p} + 1. Show that for any connection matrix \mat{C}, [\mat{H}{\vee \atop \wedge}]^{\textit{p}} converges to a limit. & \\
\end{tabularx}

\par \textbf{3.22} Devise programs to determine

\begin{tabularx}
<td valign=top nowrap>(a) & whether a given connection matrix \mat{C} represents a tree. & \\
<td valign=top nowrap>(b) & the left list of the tree (\vect{n}, \mat{C}). & \\
<td valign=top nowrap>(c) & the right list of the tree (\vect{n}, \mat{C}). & \\
<td valign=top nowrap>(d) & a node list \vect{n} and connection matrix \mat{C} as a function of
\begin{tabularx}
(i) & a left list \mat{L} & \\
(ii) & a right list \mat{R}. & \\
\end{tabularx} & \\
\end{tabularx}

\par \textbf{3.23} Show that $(\vect{n}, \mat{C})$ and $(\vect{n}_{\vect{p}}, \mat{C}_{\vect{p}}^{\vect{p}})$ represent the same graph for any permutation $\vect{p}$.

\par \textbf{3.24} If $(\vect{n}, \mat{C})$ is a tree and if $\mat{K} = \mat{C} {\vee \atop \wedge} \mat{C}$, then $\mat{C}$ can be determined as a function of $\mat{K}$ (see <acronym title="Ross, I.C., and F. Harary, (1960), $“The$ Square of a $Tree”$, Bell System Tech. J., vol. XXXIX, pp. 641-8.">Ross and Harary, 1960</acronym>). Write a program for determining $\mat{C}$ from $\mat{K}$.

<!------------------------------------------------------------------------------- ------------------------------------------------------------------------------->

<a name="summary"></a>
\par $\large{Summary$ of Notation}

<a name="s.1"></a>
\par \textbf{S.1 Conventions}

<a name="s.1.1"></a>
\par \textbf{Basic conventions}

\begin{tabularx}
<td valign=top>(a) & & 1-origin indexing assumed in this summary.

<td valign=top>(b) & & Controlling variables appear to the left, e.g., \vect{u}/\vect{x}, \vect{u} ⊥ \vect{y}, \textit{k} ↑ \vect{x}, and \vect{u} ⌈ \vect{x}.

<td valign=top>(c)^{} & & Dimension \textit{n} may be elided (if determined by compatibility) from \textbf{ϵ}(\textit{n}), \textbf{ϵ}^{\textit{k}}(\textit{n}), \mathbf{α}^{\textit{k}}(\textit{n}), \textbf{ω}^{\textit{k}}(\textit{n}), and \textbf{ι}^{ \textit{j}}(\textit{n}).

<td valign=top>(d) & & The parameter \textit{j} may be elided from operators |_{ \textit{j}}, θ_{ \textit{j}}, \int_{ \textit{j}}, and ι_{ \textit{j}}, and from the vector \textbf{ι}^{ \textit{j}} if \textit{j} is the index origin in use.

<td valign=top>(e) & & The parameter \textit{k} may be elided from \textit{k} ↑ \vect{x} if \textit{k} = 1.

\end{tabularx}

<a name="s.1.2"></a>
\par \textbf{Branching conventions}

\begin{tabularx}
<td valign=top>(a) & & 
\par $│ x : y │<img$ src="APLimg/APLbranch.bmp">
\par The statement to which the arrow leads is executed if $(x \mathcal{R} y) =$ 1; otherwise the listed successor is executed next. An unlabeled arrow is always followed.

<td valign=top>(b) & & 
\par $x : y, \vect{r} → \vect{s}$
\par The statement numbered $\vect{s}_{ i}$ is executed next if $(x\vect{r}_iy) =$ 1. The null symbol $∘$ occurring as a component of $\vect{r}$ denotes the relation which complements the disjunction of the remaining relations in $\vect{r}$.

<td valign=top>(c) & & 
\par $→$ Program $a, b$
\par Program $a$ branches to its statement $b$. The symbol $a$ may be elided if the statement occurs in Program \textit{a\textit{ itself.

\end{tabularx}

<a name="s.1.3"></a>
\par \textbf{Operand conventions used in summary}

\begin{tabularx}
 & & <td align=center>Scalar & & <td align=center>Vector & & <td align=center>Matrix & & <td align=center>Tree & \\
Logical & & \textit{u}, \textit{v}, \textit{w} & & \vect{u}, \vect{v}, \vect{w} & & \mat{U}, \mat{V}, \mat{W} & & \mat{U}, \mat{V}, \mat{W} & \\
Integral & & \textit{h}, \textit{i}, \textit{j}, \textit{k} & & \vect{h}, \vect{i}, \vect{j}, \vect{k} & & \mat{H}, \mat{I}, \mat{J}, \mat{K} & & \mat{H}, \mat{I}, \mat{J}, \mat{K} & \\
Numerical & & \textit{x}, \textit{y}, \textit{z} & & \vect{x}, \vect{y}, \vect{z} & & \mat{X}, \mat{Y}, \mat{Z} & & \mat{X}, \mat{Y}, \mat{Z} & \\
Arbitrary & & \textit{a}, \textit{b}, \textit{c} & & \mat{A}, \mat{B}, \mat{C} & & \mat{A}, \mat{B}, \mat{C} & & \mat{A}, \mat{B}, \mat{C} & \\
\end{tabularx}

<a name="s.2"></a>
\par \textbf{S.2 Structural Parameters, Null}

\begin{tabularx}
Dimension & & \textit{ν}(\vect{a}) & & Number of components in vector \vect{a} & <td align=right valign=top><a href="#nu_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Row dimension & & <td valign=top>\textit{ν}(\mat{A}) & & Number of components in each\\
row vector \mat{A}^{\textit{i}} & <td align=right valign=top><a href="#nu_matrix"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Column dimension & & <td valign=top>\textit{μ}(\mat{A}) & & Number of components in each\\
column vector \mat{A}_{\textit{j}} & <td align=right valign=top><a href="#mu_matrix"><img src="APLimg/link.bmp"></a> & \\
Height & & \textit{ν}(\mat{A}) & & Length of longest path in \mat{A} & <td align=right valign=top><a href="#nu_tree"><img src="APLimg/link.bmp"></a> & \\
Moment & & \textit{μ}(\mat{A}) & & Number of nodes in \mat{A} & <td align=right valign=top><a href="#mu_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Dispersion vector^{} & & <td valign=top>\mathbf{ν}(\mat{A})^{} & & \begin{tabularx} <td valign=top>\mathbf{ν}_1(\mat{A}) & <td valign=top>= & number of roots of \mat{A} & \\ <td valign=top>\mathbf{ν}_{\textit{j}}(\mat{A}) & <td valign=top>= & maximum degree of nodes on level \textit{j} - 1; & \\ <td valign=top nowrap>\textit{ν}(\mathbf{ν}(\mat{A})) & <td valign=top>= & \textit{ν}(\mat{A}) & \\ \end{tabularx} & <td align=right valign=top>^{}<a href="#dispersion_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Moment vector & & <td valign=top>\mathbf{μ}(\mat{A}) & & \mathbf{μ}_{ j}(\mat{A}) = number of nodes on level \textit{j} of \mat{A}; \textit{ν}(\mathbf{μ}(\mat{A})) = \textit{ν}(\mat{A}) & <td align=right valign=top><a href="#mu_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Degree of node \textit{i} & & <td valign=top nowrap>\textit{δ}(\vect{i}, \mat{A}) & & Degree of node \textit{i} of tree \mat{A} & <td align=right valign=top><a href="#delta2"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Degree & & <td valign=top nowrap>\textit{δ}(\mat{A}) & & \textit{δ}(\mat{A}) = max \textit{δ}(\vect{i}, \mat{A})\\
 ^{\vect{i}} & <td align=right valign=top><a href="#delta_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Leaf count & & <td valign=top nowrap>\textit{λ}(\mat{A}) & & \textit{λ}(\mat{A}) is the number of leaves in \mat{A} & <td align=right valign=top><a href="#lambda_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Row dimension of file & & <td valign=top>\textit{ν}(\tree{Φ}) & & Number of files in each row\\
of a file array & <td align=right valign=top><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Column dimension of file & & <td valign=top>\textit{μ}(\tree{Φ}) & & Number of files in each column\\
of a file array & <td align=right valign=top><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Null character & & <td valign=top>∘ & & Null character of a set (e.g. space in the alphabet) or null reduction operator & <td align=right valign=top><a href="#null_element"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.3"></a>
\par \textbf{S.3 Relations}

\begin{tabularx}
<td nowrap>Equality & <td nowrap> & <td nowrap>\textit{a} = \textit{b} & <td nowrap> & \textit{a} and \textit{b} are identical & & <td align=right><a href="#relation"><img src="APLimg/link.bmp"></a> & \\
Membership & & \textit{a} ϵ \vect{b} & & \textit{a} = \vect{b}_{\textit{i}} for some \textit{i} & & <td align=right><a href="#membership"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Inclusion & & \vect{b} \supseteq \vect{a} \\
\vect{a} \subseteq \vect{b} & & <td valign=top>\vect{a}_{\textit{j}}} = \vect{b} for all \textit{j} & & <td valign=top align=right>^{}<a href="#inclusion"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Strict inclusion & & \vect{b} ⊃ \vect{a} \\
\vect{a} ⊂ \vect{b} & & <td valign=top>\vect{b} \supseteq \vect{a} and \vect{a} <img src="APLimg/notsupe.bmp"> \vect{b} & & <td valign=top align=right>^{}<a href="#strict_inclusion"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Similarity & & \vect{b} ≡ \vect{a} & & <td valign=top>\vect{b} \supseteq \vect{a} and \vect{a} \supseteq \vect{b} & & <td align=right><a href="#similarity"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Complementary relations & & <td align=center valign=top><img src="APLimg/notrel.bmp"> & & The relation which holds if and only if \mathcal{R} does not. Examples of complementary pairs: ϵ, <img src="APLimg/noteps.bmp">; ⊃, <img src="APLimg/notsup.bmp">; >, <img src="APLimg/notgt.bmp">. & \\
<td valign=top>Combined (\textit{or}ed) relations & & & & A list of relations between two variables is construed as the \textit{or} of the relations. Thus \vect{x} ⊂ ⊃ \vect{y} is equivalent to \vect{x} ≢ \vect{y}. When equality occurs as one of the \textit{or}ed relations, it is indicated by a single inferior line, e.g. \leq and ⊆ . & \\
\end{tabularx}

<a name="s.4"></a>
\par \textbf{S.4 Elementary Operations}

\begin{tabularx}
Negation & & \textit{w} ← \overbar{u} & <td nowrap> & \textit{w} = 1 ←→ \textit{u} = 0 & <td align=right>^{}<a href="#not"><img src="APLimg/link.bmp"></a> & \\
And & & \textit{w} ← \textit{u} \wedge \textit{v} & & \textit{w} = 1 ←→ \textit{u} = 1 and \textit{v} = 1 & <td align=right>^{}<a href="#and"><img src="APLimg/link.bmp"></a> & \\
Or & & \textit{w} ← \textit{u} \vee \textit{v} & & \textit{w} = 1 ←→ \textit{u} = 1 or \textit{v} = 1 & <td align=right>^{}<a href="#or"><img src="APLimg/link.bmp"></a> & \\
Relational statement & & <td valign=top nowrap>\textit{w} ← (\textit{a} \mathcal{R} \textit{b}) & & <td valign=top>\textit{w} = 1 ←→ the relation \textit{a} \mathcal{R} \textit{b} holds & <td valign=top align=right>^{}<a href="#relation"><img src="APLimg/link.bmp"></a> & \\

Sum & & \textit{z} ← \textit{x} + \textit{y} & & \textit{z} is the algebraic sum of \textit{x} and \textit{y} & <td align=right>^{}<a href="#plus"><img src="APLimg/link.bmp"></a> & \\
Difference & & \textit{z} ← \textit{x} - \textit{y} & & \textit{z} is the algebraic difference of \textit{x} and \textit{y} & <td align=right>^{}<a href="#minus"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Product & & <td valign=top> \textit{z} ← \textit{x} \times \textit{y}\\
 \textit{z} ← \textit{xy}\\
 \textit{c} ← \textit{a} \times \textit{u}\\
 \textit{c} ← \textit{au} & & <td valign=top>\textit{z} is the algebraic product of numbers \textit{x} and \textit{y}, and \textit{c} is the arbitrary character \textit{a} or zero according to whether the logical variable \textit{u} is one or zero. & <td valign=top align=right>^{}<a href="#times"><img src="APLimg/link.bmp"></a> & \\
Quotient & & \textit{z} ← \textit{x} ÷ \textit{y} & & \textit{z} is the quotient of \textit{x} and \textit{y} & <td align=right>^{}<a href="#divide"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Absolute value & & <td valign=top>\textit{z} ← |\textit{x}| & & <td valign=top>\textit{z} = \textit{x} \times [(\textit{x} > 0) - (\textit{x} < 0)] & <td align=right>^{}<a href="#abs"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Floor & & <td valign=top>\textit{k} ← ⌊\textit{x}⌋ & & <td valign=top>\textit{k} \leq \textit{x} < \textit{k} + 1 & <td align=right>^{}<a href="#floor"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Ceiling & & <td valign=top>\textit{k} ← ⌈\textit{x}⌉ & & <td valign=top>\textit{k} \geq \textit{x} > \textit{k} - 1 & <td align=right>^{}<a href="#ceiling"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\textit{j}-Residue mod \textit{h} & & <td valign=top>\textit{k} ← \textit{h}|_{\textit{j}}\textit{i} & & <td valign=top>\textit{i} = \textit{hq} + \textit{k}; \textit{j} \leq \textit{k} < \textit{j} + \textit{h}; and \textit{q} is integral. & <td align=right>^{}<a href="#residue"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.5"></a>
\par \textbf{S.5 Vector Operations}

\begin{tabularx}
Component-by-component\\
extension of basic operation & & <td valign=top nowrap>\vect{c} ← \vect{a} \odot \vect{b} & <td nowrap> </d> <td colspan=3 valign=top>\vect{c}_{\textit{i}} ← \vect{a}_{\textit{i}} \odot \vect{b}_{\textit{i}} . Examples: \vect{x} \times \vect{y}, (\vect{a} \neq \vect{b}), \vect{h}|_{\textit{j}}\vect{i}, \vect{u} \wedge \overbar{\vect{v}}, ⌈\vect{x}⌉. & <td valign=top align=right><a href="#1.5"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Scalar multiple & & <td nowrap> \vect{z} ← \textit{x} \times \vect{y}\\
 \vect{z} ← \textit{x}\vect{y}\\
 \vect{c} ← \textit{a} \times \vect{u}\\
 \vect{c} ← \textit{a}\vect{u} & & <td colspan=3 valign=top>\vect{z}_{\textit{i}} ← \textit{x} \times \vect{y}_{\textit{i}}, and \vect{c}_{\textit{i}} ← \textit{a} \times \vect{u}_{\textit{i}} & <td valign=top align=right><a href="#scalar_multiple"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Reduction & & <td valign=top>\textit{c} ← \odot/\vect{a} & & <td colspan=3>\textit{c} = (...((\vect{a}_1 \odot \vect{a}_2) \odot \vect{a}_3)...) \odot \vect{a}_{\textit{ν}}), <td valign=top align=right><a href="#1.8"><img src="APLimg/link.bmp"></a> & \\
<td colspan=4> & <td colspan=4> where \odot is a binary operator or relation with a suitable domain. Examples: +/\vect{x}, \times/\vect{x}, \neq/\vect{u}. Reduction of the null vector \textbf{ϵ}(0) is defined as the identity element of the operator \odot. Examples: +/\textbf{ϵ}(0) = 0, \times/\textbf{ϵ}(0) = 1, \vee/\textbf{ϵ}(0) = 0, \wedge/\textbf{ϵ}(0) = 1. & \\
<td valign=top>Ranking & <td colspan=6> & <td align=right><a href="#1.16"><img src="APLimg/link.bmp"></a> & \\
<td nowrap valign=top> \textit{j}-origin \vect{b}-index of \textit{a} & & <td nowrap valign=top>\textit{c} ← \vect{b} ι_{\textit{j}} \textit{a} & & <td valign=top colspan=4>\textit{c} = ∘ if \textit{a} <img src="APLimg/noteps.bmp"> \vect{b}; otherwise \textit{c} is the \textit{j}-origin index of the first occurrence of \textit{a} in \vect{b}. & \\
<td nowrap valign=top> \textit{j}-origin \vect{b}-index of \vect{a} & & <td nowrap valign=top>\vect{c} ← \vect{b} ι_{\textit{j}} \vect{a} & & <td valign=top colspan=4>\vect{c}_{\textit{i}} = \vect{b} ι_{\textit{j}} \vect{a}_{\textit{i}} & \\
<td valign=top>Left rotation & & <td valign=top>\vect{c} ← \textit{k} ↑ \vect{a} & & <td nowrap colspan=3>\vect{c}_{\textit{i}} = \vect{a}_{\textit{j}}, where \textit{j} = \textit{ν}(\vect{a}) |_1 (\textit{i} + \textit{k}) & <td valign=top align=right><a href="#1.6"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Right rotation & & <td valign=top>\vect{c} ← \textit{k} ↓ \vect{a} & & <td colspan=3>\vect{c}_{\textit{i}} = \vect{a}_{\textit{j}}, where \textit{j} = \textit{ν}(\vect{a}) |_1 (\textit{i} - \textit{k}) & <td valign=top align=right><a href="#right_rotation"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Base \vect{y} value of \vect{x} & & <td valign=top>\textit{z} ← \vect{y} ⊥ \vect{x} & & <td colspan=3>\textit{z} = +/(\vect{p} \times \vect{x}), where \vect{p}_{\textit{ν}} = 1, and \vect{p}_{\textit{i}-1} = \\
\vect{p}_{\textit{i}} \times \vect{y}_{\textit{i}} <td valign=top align=right><a href="#base_value"><img src="APLimg/link.bmp"></a> & \\

<td valign=top>Compression & & <td valign=top>\vect{c} ← \vect{u}/\vect{b} & & <td colspan=3>\vect{c} is obtained from \vect{b} by suppressing each \vect{b}_{\textit{i}} for which \vect{u}_{\textit{i}} = 0 & <td valign=top align=right><a href="#1.9"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Expansion & & <td valign=top>\vect{c} ← \vect{u}\backslash\vect{b} & & <td colspan=3>\overbar{\vect{u}}/\vect{c} = 0, \vect{u}/\vect{c} = \vect{b} & <td valign=top align=right><a href="#expansion"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Mask & & <td nowrap valign=top>\vect{c} ← /\vect{a},\vect{u}</b\textbf{,\textit{b}/ & & <td colspan=3>\overbar{\vect{u}}/\vect{c} = \overbar{\vect{u}}/\vect{a}, \vect{u}/\vect{c} = \vect{u}/\vect{b} & <td valign=top align=right><a href="#mask"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Mesh & & <td nowrap valign=top>\vect{c} ← \backslash\vect{a},\vect{u},\vect{b}\backslash & & <td colspan=3>\overbar{\vect{u}}/\vect{c} = \vect{a}, \vect{u}/\vect{c} = \vect{b} & <td valign=top align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Catenation & & <td nowrap valign=top>\vect{c} ← \vect{a} \oplus \vect{b} & & <td colspan=3>\vect{c} = (\vect{a}_1, \vect{a}_2,... \vect{a}_{\textit{ν}(\vect{a})}, \vect{b}_1, \vect{b}_2,... \vect{b}_{\textit{ν}(\vect{b})})\\
 = \backslash\vect{a}, \textbf{ω}^{\textit{ν}(\vect{b})}, \vect{b}\backslash & <td valign=top align=right><a href="#catenation"><img src="APLimg/link.bmp"></a> & \\

<td valign=top>Characteristic of\\
 \vect{x} on \vect{y} & & <td valign=top>\vect{w} ← \textbf{ϵ}_{\vect{y}}^{\vect{x}} & & <td valign=top colspan=3>\vect{w}_{\textit{i}} = (\vect{y}_{\textit{i}} ϵ \vect{x}); \textit{ν}(\vect{w}) = \textit{ν}(\vect{y}) & <td align=right><a href="#characteristic_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>\textit{j}th unit vector & & <td valign=top>\vect{w} ← \textbf{ϵ}^{\textit{j}}(\textit{h}) & & <td valign=top>\vect{w}_{\textit{i}} = (\textit{i} = \textit{j}) & <td rowspan=5><img src="APLimg/matrixr8.bmp"> & & <td valign=top align=right><a href="#unit_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Full vector & & <td valign=top>\vect{w} ← \textbf{ϵ}(\textit{h}) & & <td valign=top>\vect{w}_{\textit{i}} = 1 & & <td valign=top align=right><a href="#full_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Zero vector & & <td valign=top>\vect{w} ← \overbar{\textbf{ϵ}}(\textit{h})\\
 \vect{w} ← 0 & & <td valign=top>\vect{w}_{\textit{i}} = 0 & \textit{ν}(\vect{w})=\textit{h} & <td valign=top align=right><a href="#zero_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Prefix of weight \textit{j} & & <td valign=top>\vect{w} ← \mathbf{α}^{\textit{j}}(\textit{h}) & & <td valign=top>First \textit{k} of \vect{w}_{\textit{i}} are unity,\\
 where \textit{k} = min(\textit{ j},\textit{h}). & & <td valign=top align=right><a href="#prefix_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Suffix of weight \textit{j} & & <td valign=top>\vect{w} ← \textbf{ω}^{\textit{j}}(\textit{h}) & & <td valign=top>Last \textit{k} of \vect{w}_{\textit{i}} are unity,\\
 where \textit{k} = min(\textit{ j},\textit{h}). & & <td valign=top align=right><a href="#suffix_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximum prefix & & <td valign=top>\vect{w} ← α/\vect{u} & & <td valign=top colspan=3>\vect{w} is the max length prefix in \vect{u}. & <td align=right><a href="#maximum_prefix"><img src="APLimg/link.bmp"></a> & \\
<td colspan=4> <td colspan=4 valign=top> e.g. α/(1, 1, 0, 1, 0, 1) = (1, 1, 0, 0, 0, 0). & \\
<td valign=top>Maximum suffix & & <td valign=top>\vect{w} ← ω/\vect{u} & & <td colspan=3 valign=top>\vect{w} is the max length suffix in \vect{u}. & <td align=right><a href="#maximum_suffix"><img src="APLimg/link.bmp"></a> & \\
<td colspan=4> <td colspan=4 valign=top> e.g. ω/(1, 1, 0, 1, 0, 1) = (0, 0, 0, 0, 0, 1). & \\
<td valign=top>Forward set\\
selector & & <td valign=top>\vect{w} ← \textit{σ}/\vect{a} & & <td colspan=3 valign=top>\vect{w}_{\textit{i}} = 1 if \vect{a}_{\textit{j}} \neq \vect{a}_{\textit{i}} for all \textit{j} < \textit{i} & <td align=right><a href="#forward_set_selector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Backward set\\
selector & & <td valign=top>\vect{w} ← \textit{τ}/\vect{a} & & <td colspan=3 valign=top>\vect{w}_{\textit{i}} = 1 if \vect{a}_{\textit{j}} \neq \vect{a}_{\textit{i}} for all \textit{j} > \textit{i} & <td align=right><a href="#backward_set_selector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximum selector & & <td valign=top>\vect{w} ← \vect{u}⌈\vect{x} & & <td valign=top colspan=3>\vect{w}_{\textit{i}} = \vect{u}_{\textit{i}} \wedge (\vect{x}_{\textit{i}} = \textit{m}) where \textit{m} = max(\vect{u}/\vect{x})_{\textit{j}}\\
 ^{\textit{j}} & <td valign=top align=right><a href="#maximum_selector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Minimum selector & & <td valign=top>\vect{w} ← \vect{u}⌊\vect{x} & & <td colspan=3 valign=top>\vect{w}_{\textit{i}} = \vect{u}_{\textit{i}} \wedge (\vect{x}_{\textit{i}} = \textit{m}) where \textit{m} = min(\vect{u}/\vect{x})_{\textit{j}}\\
 ^{\textit{j}} & <td valign=top align=right><a href="#minimum_selector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Interval or \textit{j}-origin identity permutation vector & & <td valign=top>\vect{k} ← \textbf{ι}^{\textit{j}}(\textit{h}) & & <td valign=top colspan=3>\vect{k} = (\textit{j}, \textit{j}+1, ..., \textit{j}+\textit{h}-1) & <td valign=top align=right><a href="#interval_vector"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>\textit{j}-origin identity permutation vector & & <td valign=top>\vect{k} & & <td valign=top colspan=3>\vect{k} = \textbf{ι}^{\textit{j}}(\textit{ν}(\vect{k})) & <td valign=top align=right><a href="#identity_permutation"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>\textit{j}-origin mapping & & <td valign=top>\vect{c} ← \vect{a}_{\vect{b}}\\
 \vect{c} ← \vect{b}\int_{\textit{j}}\vect{a} & & <td colspan=3 valign=top>\vect{c}_{\textit{i}} = ∘ if \vect{b}_{\textit{i}} <img src="APLimg/noteps.bmp"> \textbf{ι}^{\textit{j}}(\textit{ν}(\vect{a})); otherwise \vect{c}_{\textit{i}} = \vect{a}_{\vect{b}_{\textit{i}}} in a \textit{j}-origin system for \vect{a}. In the first form (that is, \vect{c} ← \vect{a}_{\vect{b}}), the origin cannot be specified directly. & <td valign=top align=right><a href="#1.17.2"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>\textit{j}-origin ordering & & <td valign=top>\vect{k} ← θ_{\textit{j}}/\vect{x} & & <td colspan=3>\vect{y} = \vect{k} \int_{\textit{j}}\vect{x} is in ascending order and original relative ordering is maintained among equal components, that is, either \vect{y}_{\textit{i}} < \vect{y}_{\textit{i}+1} or \vect{y}_{\textit{i}} = \vect{y}_{\textit{i}+1} and \vect{k}_{\textit{i}} < \vect{k}_{\textit{i}+1} & <td valign=top align=right><a href="#1.17.4"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.6a"></a>
\par \textbf{S.6a Row Generalizations of Vector Operations}

\begin{tabularx}
<td nowrap>\mat{Z} ← \mat{X} \odot \mat{Y}^{}_{} & <td nowrap> & <td nowrap colspan=2>\mat{Z}_{\textit{j}}^{\textit{i}} = \mat{X}_{\textit{j}}^{\textit{i}} \odot \mat{Y}_{\textit{j}}^{\textit{i}} & <td width=300> & <td align=right><a href="#1.5.2"><img src="APLimg/link.bmp"></a> & \\
\vect{z} ← \odot/\mat{X} _{}^{} & & <td colspan=2>\vect{z}_{\textit{i}} = \odot/\mat{X}^{\textit{i}} & & <td align=right><a href="#reduce_row"><img src="APLimg/link.bmp"></a> & \\
\mat{C} ← <img src="APLimg/circletimes.bmp">/\mat{A} ^{}_{} & & <td nowrap colspan=2>\mat{C} = \mat{A}_{\textit{1}} <img src="APLimg/circletimes.bmp"> \mat{A}_{\textit{2}} <img src="APLimg/circletimes.bmp"> ... <img src="APLimg/circletimes.bmp"> \mat{A}_{\textit{ν}} & & <td align=right><a href="#cp_reduce"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{M} ← \mat{B} ι_{\textit{h}} \mat{A}^{}_{} & & <td colspan=2>\mat{M}^{\textit{i}} = \mat{B}^{\textit{i}} ι_{\textit{h}} \mat{A}^{\textit{i}} & & <td align=right><a href="#iota_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{k} ↑ \mat{A}^{}_{} & & <td colspan=2>\mat{C}^{\textit{i}} = \vect{k}_{\textit{i}} ↑ \mat{A}^{\textit{i}} & & <td align=right><a href="#rotate_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{k} ↓ \mat{A}^{}_{} & & <td colspan=2>\mat{C}^{\textit{i}} = \vect{k}_{\textit{i}} ↓ \mat{A}^{\textit{i}} & & <td align=right><a href="#rotate_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\vect{z} ← \mat{Y} ⊥ \mat{X}^{}_{} & & <td colspan=2>\vect{z}_{\textit{i}} = \mat{Y}^{\textit{i}} ⊥ \mat{X}^{\textit{i}} & & <td align=right><a href="#1.14"><img src="APLimg/link.bmp"> & \\

<td nowrap>\mat{C} ← \mat{A}_{\vect{b}} & & <td colspan=2>\mat{C}_{\textit{j}} = \mat{A}_{\vect{b}_{\textit{j}}} & & <td align=right><a href="#sub_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \mat{B} \int_{\textit{h}} \mat{A}^{} & & <td colspan=2>\mat{C}^{\textit{i}} = \mat{B}^{\textit{i}} \int_{\textit{h}} \mat{A}^{\textit{i}} & & <td align=right><a href="#index_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{K} ← \textit{θ}_{\textit{h}}/\mat{X}^{} & & <td colspan=2>\mat{K}^{\textit{i}} = \textit{θ}_{\textit{h}}/\mat{X}^{\textit{i}} & & <td align=right><a href="#1.17.4"><img src="APLimg/link.bmp"></a> & \\

<td nowrap>\mat{C} ← \mat{A} \oplus \mat{B}^{} & & <td colspan=2>\mat{C}^{\textit{i}} = \mat{A}^{\textit{i}} \oplus \mat{B}^{\textit{i}} & & <td align=right><a href="#catenation"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{u}/\mat{B}^{} & & <td colspan=2>\mat{C}^{\textit{i}} = \vect{u}/\mat{B}^{\textit{i}} & & <td align=right><a href="#compress_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\vect{c} ← \mat{U}/\mat{B}^{} & & <td nowrap colspan=2>\vect{c} = \mat{U}^1/\mat{B}^1 \oplus ... \oplus \mat{U}^{\textit{μ}}/\mat{B}^{\textit{μ}} & & <td align=right><a href="#compress_rowrow"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{u}\backslash\mat{B}^{} & & \overbar{\vect{u}}/\mat{C} = 0, & \vect{u}/\mat{C} = \mat{B} & & <td align=right><a href="#expand_row"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \mat{U}\backslash\vect{b}^{} & & <img src="APLimg/ucapboscore.bmp">/\mat{C} = 0, & \mat{U}/\mat{C} = \vect{b} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \backslash\mat{A}, \vect{u}, \mat{B}\backslash & & \overbar{\vect{u}}/\mat{C} = \mat{A}, & \vect{u}/\mat{C} = \mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \backslash\vect{a}, \mat{U}, \vect{b}\backslash & & <img src="APLimg/ucapboscore.bmp">/\mat{C} = \vect{a}, & \mat{U}/\mat{C} = \vect{b} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\mat{A}, \vect{u}, \mat{B}/ & & \overbar{\vect{u}}/\mat{C} = \overbar{\vect{u}}/\mat{A}, & \vect{u}/\mat{C} = \vect{u}/\mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\mat{A}, \mat{U}, \mat{B}/ & & <img src="APLimg/ucapboscore.bmp">/\mat{C} = <img src="APLimg/ucapboscore.bmp">/\mat{A}, & \mat{U}/\mat{C} = \mat{U}/\mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\vect{a}, \mat{U}, \vect{b}/ & & <td colspan=2>\mat{C} = /\mathsf{\mat{E}}\backslash\vect{a}, \mat{U}, \mathsf{\mat{E}}\backslash\vect{b}/ & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\

\mat{W} ← α/\mat{U}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = α/\mat{U}^{\textit{i}} & & <td align=right><a href="#prefix_row"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← ω/\mat{U}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = ω/\mat{U}^{\textit{i}} & & <td align=right><a href="#prefix_row"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \textit{σ}/\mat{U}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = \textit{σ}/\mat{U}^{\textit{i}} & & <td align=right><a href="#set_selector_row"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \textit{τ}/\mat{U}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = \textit{τ}/\mat{U}^{\textit{i}} & & <td align=right><a href="#set_selector_row"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \mat{U}⌈\mat{X}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = \mat{U}^{\textit{i}}⌈\mat{X}^{\textit{i}} & & <td align=right><a href="#max_row"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \mat{U}⌊\mat{X}^{} & & <td colspan=2>\mat{W}^{\textit{i}} = \mat{U}^{\textit{i}}⌊\mat{X}^{\textit{i}} & & <td align=right><a href="#max_row"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.6b"></a>
\par \textbf{S.6b Column Generalizations of Vector Operations}

\begin{tabularx}
<td nowrap>\mat{Z} ← \mat{X} \odot \mat{Y}^{}_{} & <td nowrap> & <td nowrap colspan=2>\mat{Z}_{\textit{j}}^{\textit{i}} = \mat{X}_{\textit{j}}^{\textit{i}} \odot \mat{Y}_{\textit{j}}^{\textit{i}} & <td width=300> & <td align=right><a href="#1.5.2"><img src="APLimg/link.bmp"></a> & \\
\vect{z} ← \odot/\!/\mat{X} _{} & & <td colspan=2>\vect{z}_{\textit{j}} = \odot/\mat{X}_{\textit{j}} & & <td align=right><a href="#reduce_col"><img src="APLimg/link.bmp"></a> & \\
\mat{C} ← <img src="APLimg/circletimes.bmp">/\!/\mat{A} ^{}_{} & & <td nowrap colspan=2>\mat{C} = \mat{A}^{\textit{1}} <img src="APLimg/circletimes.bmp"> \mat{A}^{\textit{2}} <img src="APLimg/circletimes.bmp"> ... <img src="APLimg/circletimes.bmp"> \mat{A}^{\textit{μ}} & & <td align=right><a href="#cp_reduce"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{M} ← \mat{B} ιι_{\textit{h}} \mat{A}_{} & & <td colspan=2>\mat{M}_{\textit{j}} = \mat{B}_{\textit{j}} ι_{\textit{h}} \mat{A}_{\textit{j}} & & <td align=right><a href="#iota_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{k} \Uparrow \mat{A}_{} & & <td colspan=2>\mat{C}_{\textit{j}} = \vect{k}_{\textit{j}} ↑ \mat{A}_{\textit{j}} & & <td align=right><a href="#rotate_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{k} <img src="APLimg/darr.bmp"> \mat{A}_{} & & <td colspan=2>\mat{C}_{\textit{j}} = \vect{k}_{\textit{j}} ↓ \mat{A}_{\textit{j}} & & <td align=right><a href="#rotate_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\vect{z} ← \mat{Y} <img src="APLimg/decode2.bmp"> \mat{X}_{} & & <td colspan=2>\vect{z}_{\textit{j}} = \mat{Y}_{\textit{j}} ⊥ \mat{X}_{\textit{j}} & & <td align=right><a href="#1.20"><img src="APLimg/link.bmp"></a> & \\

<td nowrap>\mat{C} ← \mat{A}^{\vect{b}} & & <td colspan=2>\mat{C}^{\textit{i}} = \mat{A}^{\vect{b}_{\textit{i}}} & & <td align=right><a href="#sub_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \mat{B} \int\int_{\textit{h}} \mat{A}^{} & <td colspan=2>\mat{C}_{\textit{j}} = \mat{B}_{\textit{j}} \int_{\textit{h}} \mat{A}_{\textit{j}} & & <td align=right><a href="#index_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{K} ← \textit{θ}_{\textit{h}}/\!/\mat{X}^{} & & <td colspan=2>\mat{K}_{\textit{j}} = \textit{θ}_{\textit{h}}/\mat{X}_{\textit{j}} & & <td align=right><a href="#theta_col"><img src="APLimg/link.bmp"></a> & \\

<td nowrap>\mat{C} ← \mat{A} \oplus\oplus \mat{B}_{} & & <td colspan=2>\mat{C}_{\textit{j}} = \mat{A}_{\textit{j}} \oplus \mat{B}_{\textit{j}} & \\
<td nowrap>\mat{C} ← \vect{u}/\!/\mat{B}_{} & & <td colspan=2>\mat{C}_{\textit{j}} = \vect{u}/\mat{B}_{\textit{j}} & & <td align=right><a href="#compress_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\vect{c} ← \mat{U}/\!/\mat{B}_{} & & <td nowrap colspan=2>\vect{c} = \mat{U}_1/\mat{B}_1 \oplus ... \oplus \mat{U}_{\textit{ν}}/\mat{B}_{\textit{ν}} & & <td align=right><a href="#compress_colcol"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \vect{u}\backslash\backslash\mat{B}^{} & & \overbar{\vect{u}}/\!/\mat{C} = 0, & \vect{u}/\!/\mat{C} = \mat{B} & & <td align=right><a href="#expand_col"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \mat{U}\backslash\backslash\vect{b}^{} & & <img src="APLimg/ucapboscore.bmp">/\!/\mat{C} = 0, & \mat{U}/\!/\mat{C} = \vect{b} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \backslash\backslash\mat{A}, \vect{u}, \mat{B}\backslash\backslash & & \overbar{\vect{u}}/\!/\mat{C} = \mat{A}, & \vect{u}/\!/\mat{C} = \mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← \backslash\backslash\vect{a}, \mat{U}, \vect{b}\backslash\backslash & & <img src="APLimg/ucapboscore.bmp">/\!/\mat{C} = \vect{a}, & \mat{U}/\!/\mat{C} = \vect{b} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\!/\mat{A}, \vect{u}, \mat{B}/\!/ & & <td nowrap>\overbar{\vect{u}}/\!/\mat{C} = \overbar{\vect{u}}/\!/\mat{A}, & \vect{u}/\!/\mat{C} = \vect{u}/\!/\mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\!/\mat{A}, \mat{U}, \mat{B}/\!/ & & <td nowrap><img src="APLimg/ucapboscore.bmp">/\!/\mat{C} = <img src="APLimg/ucapboscore.bmp">/\!/\mat{A}, & <td nowrap>\mat{U}/\!/\mat{C} = \mat{U}/\!/\mat{B} & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\
<td nowrap>\mat{C} ← /\!/\vect{a}, \mat{U}, \vect{b}/\!/ & & <td colspan=2>\mat{C} = /\mathsf{\mat{E}}\backslash\backslash\vect{a}, \mat{U}, \mathsf{\mat{E}}\backslash\backslash\vect{b}/ & & <td align=right><a href="#1.9.2"><img src="APLimg/link.bmp"></a> & \\

\mat{W} ← α/\!/\mat{U}_{} & & <td colspan=2>\mat{W}_{\textit{j}} = α/\mat{U}_{\textit{j}} & & <td align=right><a href="#prefix_col"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← ω/\!/\mat{U}_{} & & <td colspan=2>\mat{W}_{\textit{j}} = ω/\mat{U}_{\textit{j}} & & <td align=right><a href="#prefix_col"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \textit{σ}/\!/\mat{U}_{} & & <td colspan=2>\mat{W}_{\textit{j}} = \textit{σ}/\mat{U}_{\textit{j}} & & <td align=right><a href="#set_selector_col"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \textit{τ}/\!/\mat{U}_{} & & <td colspan=2>\mat{W}_{\textit{j}} = \textit{τ}/\mat{U}_{\textit{j}} & & <td align=right><a href="#set_selector_col"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \mat{U}⌈⌈\mat{X}_{} & & <td colspan=2>\mat{W}_{\textit{j}} = \mat{U}_{\textit{j}}⌈\mat{X}_{\textit{j}} & & <td align=right><a href="#max_col"><img src="APLimg/link.bmp"></a> & \\
\mat{W} ← \mat{U}⌊⌊\mat{X}^{} & & <td colspan=2>\mat{W}_{\textit{j}} = \mat{U}_{\textit{j}}⌊\mat{X}_{\textit{j}} & & <td align=right><a href="#max_col"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.7"></a>
\par \textbf{S.7 Special Matrices}

\begin{tabularx}
Full matrix & \mat{W} ← \mathsf{\mat{E}}(\textit{p}, \textit{q}) & & <td colspan=3>\mat{W}_{\textit{j}}^{\textit{i}} = 1 & <td rowspan=9><img src="APLimg/bracketsx7a.bmp"> & <td rowspan=9> \textit{μ}(\mat{W}) = \textit{p}\\
 \textit{ν}(\mat{W}) = \textit{q},\\
 for \textit{p} and \textit{q} integers. Elision of \textit{p} and \textit{q} if dimensions determined by compatibility. & <td valign=top align=right>^{}<a href="#full_matrix"><img src="APLimg/link.bmp"></a> & \\
Zero matrix & \mat{W} ← \overbar{\mathsf{\mat{E}}}(\textit{p}, \textit{q}) & & <td colspan=3>\mat{W}_{\textit{j}}^{\textit{i}} = 0 & <td valign=top align=right>^{}<a href="#zero_matrix"><img src="APLimg/link.bmp"></a> & \\
 & \mat{W} ← 0 & \\
Superdiagonal & <td nowrap>\mat{W} ← ^{\textit{k}}\mathsf{\mat{I}}(\textit{p}, \textit{q}) & & <td colspan=3 nowrap>\mat{W}_{\textit{j}}^{\textit{i}} = (\textit{i} + \textit{k} = \textit{j}) & <td valign=top align=right>^{}<a href="#superdiagonal_matrix"><img src="APLimg/link.bmp"></a> & \\
Identity & <td nowrap>\mat{W} ← \mathsf{\mat{I}}(\textit{p}, \textit{q}) & & <td colspan=3 nowrap>\mat{W} = ^0\mathsf{\mat{I}}(\textit{p}, \textit{q})_{} & <td valign=top align=right>^{}<a href="#identity_matrix"><img src="APLimg/link.bmp"></a> & \\
Upper left (triangle) & <td valign=top nowrap>\mat{W} ← <img src="APLimg/quadnw.bmp">(\textit{p}, \textit{q}) & & <td valign=top>\mat{W}_{\textit{j}}^{\textit{i}} & <td rowspan=4><img src="APLimg/bracketsx7b.bmp"> & <td rowspan=4>= (\textit{i} + \textit{j} \leq \textit{m}),\\
\textit{m} = min(\textit{p}, \textit{q}) & <td valign=top align=right>^{}<a href="#triangular_matrix"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Upper right & <td valign=top>\mat{W} ← <img src="APLimg/quadne.bmp">(\textit{p}, \textit{q}) & & <img src="APLimg/sx7a.bmp"> & <td valign=top align=right>^{}<a href="#triangular_matrix"><img src="APLimg/link.bmp"></a> & \\
Lower left & <td valign=top>\mat{W} ← <img src="APLimg/quadsw.bmp">(\textit{p}, \textit{q}) & & <img src="APLimg/sx7b.bmp"> & <td valign=top align=right>^{}<a href="#triangular_matrix"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Lower right & <td valign=top>\mat{W} ← <img src="APLimg/quadse.bmp">(\textit{p}, \textit{q}) & & <img src="APLimg/sx7c.bmp"> & <td valign=top align=right>^{}<a href="#triangular_matrix"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.8"></a>
\par \textbf{S.8 Transposition}

<table cellpadding=5>
<td valign=top>Diagonal & & \mat{C} ← <img src="APLimg/btilde.bmp"> & & <td valign=top nowrap>\mat{C}_{\textit{i}}^{\textit{j}} = & <td rowspan=5><img align=middle src="APLimg/bracketsx8.bmp"> \mat{B}_{\textit{j}}^{\textit{i}} & <td width=135 align=right valign=top><a href="#b_tilde"><img src="APLimg/link.bmp"></a> & \\
 & & \mat{C} ← <img src="APLimg/bnwarr.bmp"> & & & <td align=right valign=top><a href="#1.12"><img src="APLimg/link.bmp"></a> & \\
Counter diagonal & & \mat{C} ← <img src="APLimg/bnearr.bmp"> & & <img align=middle src="APLimg/sx8a.bmp"> & <td align=right valign=top><a href="#1.12"><img src="APLimg/link.bmp"></a> & \\
Horizontal & & \mat{C} ← <img src="APLimg/brarr.bmp"> & & <img align=middle src="APLimg/sx8b.bmp"> & <td align=right valign=top><a href="#1.12"><img src="APLimg/link.bmp"></a> & \\
Vertical & & \mat{C} ← <img src="APLimg/buarr.bmp"> & & <img align=middle src="APLimg/sx8c.bmp"> & <td align=right valign=top><a href="#1.12"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Vector & & \vect{y} ← <img src="APLimg/xrarr.bmp">\\
 \vect{y} ← <img src="APLimg/xuarr.bmp"> & & <td valign=top>\vect{y} ← \vect{x}_{\textit{ν} + 1 - \textit{i}} & & <td align=right valign=top><a href="#reversal"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.9"></a>
\par \textbf{S.9 Set Operations}

\begin{tabularx}
<td nowrap>Intersection ^{}_{} & & <td valign=top>\vect{c} ← \vect{b} ∩ \vect{a} ^{}_{} & & <td valign=top>\vect{c} = \textbf{ϵ}_{\vect{b}}^{\vect{a}}/\vect{b} & & <td valign=top align=right>^{}_{}<a href="#intersection"><img src="APLimg/link.bmp"></a> & \\
Difference ^{}_{} & & \vect{c} ← \vect{b} ∆ \vect{a} ^{}_{} & & \vect{c} = \overbar{\textbf{ϵ}}_{\vect{b}}^{\vect{a}}/\vect{b} & & <td valign=top align=right>^{}_{}<a href="#difference"><img src="APLimg/link.bmp"></a> & \\
Union ^{}_{} & & \vect{c} ← \vect{b} ∪ \vect{a} ^{}_{} & & \vect{c} = \vect{b} \oplus (\vect{a} ∆ \vect{b}) & & <td valign=top align=right>^{}_{}<a href="#union"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Cartesian\\
product & & <td valign=top nowrap>\mat{C} ← \vect{b}^1 <img src="APLimg/circletimes.bmp"> ... <img src="APLimg/circletimes.bmp"> \vect{b}^{\textit{n}} _{} & & \mat{C}^{ 1 + \vect{d} ⊥ (\vect{k} - \textbf{ϵ})} = (\vect{b}_{\vect{k}_1}^1, \vect{b}_{\vect{k}_2}^2, ..., \vect{b}_{\vect{k}_{\textit{n}}}^{\textit{n}}); \vect{d}_{\textit{j}} = \textit{ν}(\vect{b}^{\textit{j}}); 1 \leq \vect{k}_{\textit{j}} \leq \vect{d}_{\textit{j}} \\
 Clearly, \textit{ν}(\mat{C}) = \textit{n}, and \textit{μ}(\mat{C}) = \times/\vect{d} & & <td valign=top align=right>^{}_{}<a href="#cartesian"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.10"></a>
\par \textbf{S.10 Generalized Matrix Product}

\begin{tabularx}
<td valign=top nowrap>\mat{C} ← \mat{A} {\odot_1 \atop \odot_2} \mat{B} ^{}_{} & & \mat{C}_{\textit{j}}^{\textit{i}} = \odot_1/(\mat{A}^{\textit{i}} \odot_2 \mat{B}_{\textit{j}}), where \odot_2 produces a vector (i.e., is not the operator ⊥), and \odot_1 is a reduction operator (and hence \mat{C}_{\textit{j}}^{\textit{i}} is a scalar). & & <td valign=top align=right>^{}_{}<a href="#1.11"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\mat{C} ← \mat{A} {\circ \atop \odot_2} \vect{b} ^{}_{} & & \mat{C}^{\textit{i}} = (\mat{A}^{\textit{i}} \odot \vect{b}), where \odot is any operator which produces a vector of dimension \textit{ν}(\vect{b}). & & <td valign=top align=right>^{}<a href="#gmp_mv"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\mat{C} ← \vect{a} {\circ \atop \odot_2} \mat{B} ^{}_{} & & \mat{C}_{\textit{j}} = (\vect{a} \odot \mat{B}_{\textit{j}}),^{} where \odot is any operator which produces a vector of dimension \textit{ν}(\vect{a}). & & <td valign=top align=right>^{}<a href="#gmp_vm"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\mat{C} ← \vect{a} {\circ \atop \odot_2} \vect{b} ^{}_{} & & \mat{C}_{\textit{j}}^{\textit{i}} = (\vect{a}_{\textit{i}} \odot \vect{b}_{\textit{j}}). & & <td valign=top align=right>^{}_{}<a href="#gmp_vv"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<a name="s.11"></a>
\par \textbf{S.11 Files}

\begin{tabularx}
<td valign=top>File^{}_{} & & <td valign=top>\tree{Φ}_{\textit{j}}^{\textit{i}} & & A representation of \vect{a} of the form\\
 (\vect{p}_1, \vect{a}_1, \vect{p}_2, \vect{a}_2, ..., \vect{a}_{\textit{ν}(\vect{a})}, \vect{p}_{\textit{ν}(\vect{a})+1}, ∘, \vect{p}_{\textit{ν}(\vect{a})+2}, ..., \vect{p}_{\textit{ν}(\vect{p})}), where \vect{p}_{\textit{h}} is the partition at position \textit{h}, \vect{p}_1 = \vect{p}_{\textit{ν}(\vect{p})} = \textit{λ}, and (\mathbf{α}^1 \wedge \textbf{ω}^1)/\vect{p} \subseteq \mathbf{λ}. & <td valign=top align=right><a href="#file_defn"><img src="APLimg/link.bmp"></a> & \\
<td nowrap valign=top>Position file^{}_{} & & <td valign=top>\textit{π}(\tree{Φ}_{\textit{j}}^{\textit{i}}) ← \textit{h} & & Set file to position \textit{h}. Called \textit{rewind} if \textit{h} = 1, and \textit{wind} if \textit{h} = \textit{ν}(\vect{p}). & <td valign=top align=right><a href="#file_position"><img src="APLimg/link.bmp"></a> & \\
<td colspan=6>Record (from position \textit{h}) & \\
<td valign=top> Forward^{}_{} & & <td valign=top nowrap>_0\tree{Φ}_{\textit{j}}^{\textit{i}} ← \vect{a}, \mathbf{λ}_{\textit{k}} & & \vect{a}_{\textit{h}} ← \vect{a}, \vect{p}_{\textit{h}+1} ← \mathbf{λ}_{\textit{k}}; stop at position \textit{h} + 1. Zero prescript may be elided and \mathbf{λ}_1 may be elided. & <td valign=top align=right><a href="#forward_record"><img src="APLimg/link.bmp"></a> & \\ \\
<td valign=top nowrap> Backward^{}_{} & & <td valign=top nowrap>_1\tree{Φ}_{\textit{j}}^{\textit{i}} ← \vect{a}, \mathbf{λ}_{\textit{k}} & & \vect{a}_{\textit{h}-1} ← \vect{a}, \vect{p}_{\textit{h}-1} ← \mathbf{λ}_{\textit{k}}; stop at position \textit{h} - 1. \mathbf{λ}_1 may be elided. & <td valign=top align=right><a href="#backward_record"><img src="APLimg/link.bmp"></a> & \\
<td colspan=6>Read (from position \textit{h}) & \\
<td valign=top> Forward^{}_{} & & <td valign=top nowrap>\textit{a}, \textit{b} ← _0\tree{Φ}_{\textit{j}}^{\textit{i}} & & \textit{a} ← \vect{a}_{\textit{h}}; \textit{b} ← \vect{p}_{\textit{h}+1}; stop at position \textit{h} + 1. Associated branch is controlled by \vect{p}_{\textit{h}+1}, and \textit{b} may be elided. Zero prescript may be elided. & <td valign=top align=right><a href="#forward_read"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> Backward^{}_{} & & <td valign=top nowrap>\textit{a}, \textit{b} ← _1\tree{Φ}_{\textit{j}}^{\textit{i}} & & \textit{a} ← \vect{a}_{\textit{h}-1}; \textit{b} ← \vect{p}_{\textit{h}-1}; stop at position \textit{h} - 1. Associated branch is controlled by \vect{p}_{\textit{h}-1} and \textit{b} may be elided. & <td valign=top align=right><a href="#backward_read"><img src="APLimg/link.bmp"></a> & \\
<td colspan=6>File array & \\
<td valign=top> Full^{}_{} & & <td valign=top>\tree{Φ}^{}_{} & & Array of files \tree{Φ}_{\textit{j}}^{\textit{i}}, for \textit{i} ϵ \textbf{ι}^1(\textit{μ}(\tree{Φ})), \textit{j} ϵ \textbf{ι}^1(\textit{ν}(\tree{Φ})). & <td valign=top align=right><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> Row^{}_{} & & <td valign=top>\tree{Φ}^{\textit{i}}_{} & & Row of files \tree{Φ}_{\textit{j}}^{\textit{i}}, for \textit{j} ϵ \textbf{ι}^1(\textit{ν}(\tree{Φ})). & <td valign=top align=right><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> Column^{}_{} & & <td valign=top>\tree{Φ}_{\textit{j}} ^{} & & Column of files \tree{Φ}_{\textit{j}}^{\textit{i}}, for \textit{i} ϵ \textbf{ι}^1(\textit{μ}(\tree{Φ})). & <td valign=top align=right><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td colspan=6>Compression & \\
<td valign=top> Row & & \vect{u}/\tree{Φ} & & <td rowspan=2 valign=top>Selection as in corresponding operations on matrices. & <td valign=top align=right><a href="#file_array"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> Column & & \vect{u}/\!/\tree{Φ} & \\
\end{tabularx}

<a name="s.12"></a>
\par \textbf{S.12 Trees}

\begin{tabularx}
<td valign=top nowrap>Path \vect{i}_{}^} & & <td valign=top nowrap>\vect{c} ← \mat{A}^{\vect{i}} & & <td valign=top>\vect{c}_1 is the \vect{i}_1th root of \mat{A}; \vect{c}_{\textit{j}} is the \vect{i}_{\textit{j}}th node of the nodes on level \textit{j} reachable from node \vect{c}_{\textit{j}-1}. & <td valign=top align=right>^{}_{}<a href="#path"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Node \vect{i}_{}^} & & <td valign=top nowrap>\vect{c} ← (\mat{A}^{\vect{i}})_{\textit{ν}(\vect{i})} & & <td valign=top>The final node of path \mat{A}^{\vect{i}}. & <td valign=top align=right>^{}_{}<a href="#node"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>Subtree \vect{i}_} & & <td valign=top nowrap>\mat{C} ← \mat{A}_{\vect{ i}} & & <td valign=top>\mat{C} is the subtree of \mat{A} rooted in node \vect{i}. & <td align=right>_{}<a href="#subtree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Component-by-\\
component} & & <td valign=top nowrap>\mat{C} ← \mat{A} \odot \mat{B}^{} & & <td valign=top>(\mat{C}^{ \vect{i}})_{\textit{ν}(\vect{i})} = (\mat{A}^{ \vect{i}})_{\textit{ν}(\vect{i})} \odot (\mat{B}^{ \vect{i}})_{\textit{ν}(\vect{i})}. & <td valign=top align=right>^{}_{}<a href="#1.23.7"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Path reduction} & & <td valign=top nowrap>\mat{C} ← \odot/\mat{A} & & <td valign=top>Reduction by operator or relation \odot on nodes in left list order. & <td valign=top align=right>^{}_{}<a href="#tree_reduction"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Level reduction} & & <td valign=top nowrap>\mat{C} ← \odot/\!/\mat{A} & & <td valign=top>Reduction by operator or relation \odot on nodes in right list order. & <td valign=top align=right>^{}_{}<a href="#tree_reduction"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\textit{j}-origin \vect{b}-index}_{} & & <td valign=top nowrap>\mat{B} ← \vect{b} \textbf{ι}_{\textit{j}} \mat{A} & & <td valign=top>(\mat{B}^{ \vect{i}})_{\textit{ν}(\vect{i})} = \vect{b} \textbf{ι}_{\textit{j}} ((\mat{A}^{\vect{i}})_{\textit{ν}(\vect{i})}) & <td valign=top align=right>^{}_{}<a href="#tree_iota"><img src="APLimg/link.bmp"></a> & \\
<td valign=top nowrap>\textit{j}-origin mapping_{} & & <td valign=top nowrap>\mat{C} ← \vect{b} \int_{\textit{j}} \mat{A} & & <td valign=top>Rooted subtree \mat{C}_{\textit{i}} is a single null character node if \vect{b}_{\textit{i}} \notin \mathbf{α}^{\textit{j}}(\mathbf{μ}_1(\mat{A})); otherwise \mat{C}_i = \mat{A}_{\vect{b}_{\textit{i}}}, where \mat{A} is treated in a \textit{j}-origin system. & <td valign=top align=right>^{}_{}<a href="#tree_map"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Full right list\\
matrix & & <td valign=top nowrap>\mat{C} ← ]\mat{A} & <td rowspan=2><img align=left src="APLimg/matrixr4.bmp"> & <td valign=top rowspan=2>The rows of the index matrix \overbar{\vect{α}}^2/\mat{C} are the right (left) justified index vectors (with null fill to the common dimension \textit{ν}(\mat{A})) arranged in increasing order; \mat{C}_1 and \mat{C}_2 are the corresponding degree and node vectors of \mat{A}. & <td valign=top align=right rowspan=2>^{}_{}<a href="#1.23.3"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Full left list\\
matrix & & <td valign=top nowrap>\mat{C} ← [\mat{A} & \\
<td valign=top>Right list matrix & & <td valign=top nowrap>\mat{C} ← \mathbf{α}^2/]\mat{A} & <td rowspan=2><img align=left src="APLimg/matrixr2.bmp"> & <td valign=top rowspan=2>The degree and node vector columns of the full right (left) list. & <td valign=top align=right rowspan=2><a href="#1.23.3"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Left list matrix & & <td valign=top nowrap>\mat{C} ← \mathbf{α}^2/[\mat{A} & \\
<td valign=top>Tree compression & & <td valign=top>\mat{C} ← \mat{U}/\mat{A} & & \mat{C} is obtained from \mat{A} by suppressing node \vect{i} if node \vect{i} of \mat{U} is zero and reconnecting so that for each remaining pair of nodes, the one lies in the subtree rooted at the second if and only if it did so in \mat{A}. & <td valign=top align=right><a href="#1.23.6"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Path compression & & <td valign=top>\mat{C} ← \vect{u}/\mat{A} & & \mat{C} is obtained from \mat{A} by suppressing all nodes on level \textit{j} if \vect{u}_{\textit{j}} = 0, and reconnecting as in the compression \mat{U}/\mat{A}. & <td valign=top align=right><a href="#path_compression"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Level compression & & <td valign=top>\mat{C} ← \vect{u}/\!/\mat{A} & & \mat{C} is obtained from \mat{A} by suppressing rooted subtree \mat{A}_{\textit{j}} if \vect{u}_{\textit{j}} = 0. & <td valign=top align=right>^{}<a href="#level_compression"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Level mesh & & <td valign=top nowrap>\mat{C} ← \backslash\backslash\mat{A}, \vect{u}, \mat{B}\backslash\backslash & & \overbar{\vect{u}}/\!/\mat{C} = \mat{A}; \vect{u}/\!/\mat{C} = \mat{B}. & \\
<td valign=top>Level mask & & <td valign=top nowrap>\mat{C} ← /\!/\mat{A}, \vect{u}, \mat{B}/\!/ & & \overbar{\vect{u}}/\!/\mat{C} = \overbar{\vect{u}}/\!/\mat{A}; \vect{u}/\!/\mat{C} = \vect{u}/\!/\mat{B}. & \\
<td valign=top>Path catenation & & <td valign=top nowrap>\mat{C} ← \mat{A} \oplus \mat{B} & & \mat{C} is obtained by connecting roots of \mat{B} to leaves of \mat{A}, allotting successive groups of at most ⌈\mathbf{μ}_1(\mat{B}) ÷ \textit{λ}(\mat{A})⌉ roots of \mat{B} to each successive leaf of \mat{A}. & <td valign=top align=right><a href="#1.23"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Full tree & & <td valign=top nowrap>\mat{W} ← \mat{E} & & Each node of \mat{W} is unity and the structure of \mat{W} is determined by compatibility. & <td valign=top align=right><a href="#full_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> & & <td valign=top nowrap>\mat{W} ← \mat{E}(\vect{k}) & & Each node of \mat{W} is unity; \mat{W} is homogeneous (i.e., all nodes on any level have a common degree) and \mathbf{ν}(\mat{W}) = \vect{k}. & <td valign=top align=right><a href="#full_tree_k"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Zero tree & & <td valign=top nowrap>\mat{W} ← <img src="APLimg/0tree.bmp">\\
 \mat{W} ← 0 & <img src="APLimg/bracketsx12.bmp"> & Each node of \mat{W} is zero and the structure of \mat{W} is determined by compatibility. & <td valign=top align=right><a href="#full_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> & & <td valign=top nowrap>\mat{W} ← <img src="APLimg/0tree.bmp">(\vect{k})} & & Each node of \mat{W} is zero; \mat{W} is homogeneous and \mathbf{ν}(\mat{W}) = \vect{k}. & <td valign=top align=right><a href="#full_tree_k"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Path tree^{} & & <td valign=top nowrap>\mat{W} ← ^{\vect{u}}\mat{E} & & \overbar{\vect{u}}/\mat{W} = 0; \vect{u}/\mat{W} = \mat{E}; structure of \mat{W} determined by compatibility. & <td valign=top align=right><a href="#full_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> & & <td valign=top nowrap>\mat{W} ← ^{\vect{u}}\mat{E}(\vect{k}) & & \overbar{\vect{u}}/\mat{W} = 0; \vect{u}/\mat{W} = \mat{E}; \mat{W} is homogeneous and \mathbf{ν}(\mat{W}) = \vect{k}. & <td valign=top align=right><a href="#full_tree_k"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Level tree_{} & & <td valign=top nowrap>\mat{W} ← _{\vect{u}}\mat{E} & & \overbar{\vect{u}}/\!/\mat{W} = 0; \vect{u}/\!/\mat{W} = \mat{E}; structure of \mat{W} determined by compatibility. & <td valign=top align=right><a href="#full_tree"><img src="APLimg/link.bmp"></a> & \\
<td valign=top> & & <td valign=top nowrap>\mat{W} ← _{\vect{u}}\mat{E}(\vect{k}) & & \overbar{\vect{u}}/\!/\mat{W} = 0; \vect{u}/\!/\mat{W} = \mat{E}; \mat{W} is homogeneous and \mathbf{ν}(\mat{W}) = \vect{k}. & <td valign=top align=right><a href="#full_tree_k"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximization & & <td valign=top nowrap>\mat{W} ← \mat{U} ⌈ \mat{A} & <td rowspan=2><img src="APLimg/matrixr2.bmp"> & <td rowspan=2>\mat{W} = \mat{U} \wedge (\mat{A} = \textit{m}\mat{E}), where \textit{m} is the maximum (minimum) over all nodes of \mat{U}/\mat{A}. & <td valign=top align=right><a href="#tree_max"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Minimization & & <td valign=top nowrap>\mat{W} ← \mat{U} ⌊ \mat{A} & \\
<td valign=top>Maximum path prefix & & <td valign=top nowrap>\mat{W} ← α/\mat{U}} & & \mat{W} is obtained from \mat{U} by zeroing all nodes of every subtree rooted in a zero node. & <td valign=top align=right><a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximum path suffix & & <td valign=top nowrap>\mat{W} ← ω/\mat{U}} & & \mat{W} is obtained from \mat{U} by zeroing every node which contains a zero node in its subtree. & <td valign=top align=right><a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Forward path set selector & & <td valign=top nowrap>\mat{W} ← \textit{σ}/\mat{A}} & & (\mat{W}^{ \vect{i}})_{\textit{ν}(\vect{i})} = 1 if (\mat{A}^{\vect{i}})_{\textit{ν}(\vect{i})} differs from all preceding nodes of path \mat{A}^{\vect{i}}. & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Backward path set selector & & <td valign=top nowrap>\mat{W} ← \textit{τ}/\mat{A}} & & (\mat{W}^{ \vect{i}})_{\textit{ν}(\vect{i})} = 1 if (\mat{A}^{\vect{i}})_{\textit{ν}(\vect{i})} differs from all other nodes in its subtree. & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximum level^{} prefix & & <td valign=top nowrap>\mat{W} ← α/\!/\mat{U}} ^{} & & <td valign=top>\textbf{ϵ}^{\textit{j}}/\mat{W} = α/\textbf{ϵ}^{\textit{j}}/\mat{U} & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Maximum level^{} suffix & & <td valign=top nowrap>\mat{W} ← ω/\!/\mat{U}} ^{} & & <td valign=top>\textbf{ϵ}^{\textit{j}}/\mat{W} = ω/\textbf{ϵ}^{\textit{j}}/\mat{U} & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Forward level^{}\\
 set selector & & <td valign=top nowrap>\mat{W} ← \textit{σ}/\!/\mat{A}} ^{} & & <td valign=top>\textbf{ϵ}^{\textit{j}}/\mat{W} = \textit{σ}/\textbf{ϵ}^{\textit{j}}/\mat{U} & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
<td valign=top>Backward level^{} set selector & & <td valign=top nowrap>\mat{W} ← \textit{τ}/\!/\mat{A}} ^{} & & <td valign=top>\textbf{ϵ}^{\textit{j}}/\mat{W} = \textit{τ}/\textbf{ϵ}^{\textit{j}}/\mat{U} & <td valign=top align=right>^{}<a href="#fig1.24"><img src="APLimg/link.bmp"></a> & \\
\end{tabularx}

<hr>

\par $\small{Originally$ appeared as \textit{A Programming Language}, Wiley, 1962, available as a pdf file from
<a target=_parent href="http:/\!/www.softwarepreservation.org/projects/apl/Books/APROGRAMMING%20LANGUAGE">here</a>.
<script src="apldisplay.js" type="text/javascript"></script>
}

\begin{tabularx}
\small{created: } & \small{2009-10-13 21:35} & \\
\small{updated:} & \small{2019-10-17 15:15} & \\
\end{tabularx}

 & \\\end{tabularx}

</body>
</html>
